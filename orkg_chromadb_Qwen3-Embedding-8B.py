"""
Loads pre-computed Qwen embeddings for ORKG properties from Parquet files into a
NEW ChromaDB vector database collection.

Required Input Files:
- Parquet files containing ORKG property embeddings generated by `orkg_embeddings_Qwen3-Embedding-8B.py`.
  Expected in the directory `orkg_data/embeddings_parquet_qwen`.
  Columns: 'id', 'label', 'embedding'.

Output:
- A local ChromaDB store in `orkg_data/chroma_db_store_qwen`.

Prerequisites:
- sentence-transformers, chromadb, pandas, numpy, torch
"""

import os
import glob
import warnings
import pandas as pd
import numpy as np
import chromadb
import torch
from sentence_transformers import SentenceTransformer

# --- Configuration ---

# Input/Output Paths
PARQUET_FILES_DIR = os.path.join("orkg_data", "embeddings_parquet_qwen")
CHROMA_DB_PATH = os.path.join("orkg_data", "chroma_db_store_qwen")  # NEW PATH
COLLECTION_NAME = "orkg_labels_qwen"  # NEW COLLECTION NAME

# Model Configuration (must match generation script exactly!)
MODEL_NAME = "Qwen/Qwen3-Embedding-8B"

# Database Settings
CHROMA_BATCH_SIZE = 5000  # Chroma handles larger batches well locally

# Suppress runtime warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)


def get_local_model() -> SentenceTransformer:
    """Initializes the local Qwen model for the test query."""
    if torch.backends.mps.is_available():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"

    print(f"Info: Loading model '{MODEL_NAME}' for verification query on {device}...")
    try:
        # Load in float32 for stability
        return SentenceTransformer(MODEL_NAME, trust_remote_code=True, device=device)
    except Exception as e:
        print(f"Error loading model: {e}")
        return None


def main():
    # 1. Setup ChromaDB
    os.makedirs(CHROMA_DB_PATH, exist_ok=True)
    print(f"Info: Using ChromaDB path: {CHROMA_DB_PATH}")

    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

    # Clean start: Delete old collection if exists
    try:
        chroma_client.delete_collection(name=COLLECTION_NAME)
        print(f"Info: Old collection '{COLLECTION_NAME}' deleted.")
    except Exception:
        pass  # Collection didn't exist, that's fine

    # Create new collection
    # Note: We use 'cosine' distance space which is standard for embeddings
    collection = chroma_client.create_collection(name=COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
    print(f"Info: New collection '{COLLECTION_NAME}' created.")

    # 2. Load Parquet Data
    parquet_files = glob.glob(os.path.join(PARQUET_FILES_DIR, "*.parquet"))
    if not parquet_files:
        print(f"Error: No Parquet files found in {PARQUET_FILES_DIR}. Exiting.")
        return
    print(f"Info: Found {len(parquet_files)} Parquet file(s).")

    unique_data_by_id = {}
    total_rows = 0

    for parquet_file in parquet_files:
        print(f"Info: Reading {os.path.basename(parquet_file)}...")
        try:
            df = pd.read_parquet(parquet_file)
            total_rows += len(df)

            for _, row in df.iterrows():
                orkg_id = str(row['id'])
                # Deduplication logic
                if orkg_id not in unique_data_by_id:
                    unique_data_by_id[orkg_id] = {
                        'label': row['label'],
                        'embedding': row['embedding']  # This is already a numpy array from previous script
                    }
        except Exception as e:
            print(f"Error reading {parquet_file}: {e}")
            continue

    print(f"Info: Loaded {len(unique_data_by_id)} unique items (from {total_rows} total rows).")

    # 3. Prepare Batches for ChromaDB
    ids = []
    documents = []
    embeddings = []
    metadatas = []

    for orkg_id, data in unique_data_by_id.items():
        ids.append(orkg_id)
        documents.append(data['label'])

        # Ensure embedding is a standard list of floats for Chroma
        emb = data['embedding']
        if hasattr(emb, 'tolist'):
            emb = emb.tolist()
        embeddings.append(emb)

        metadatas.append({"label": data['label'], "original_id": orkg_id})

    # 4. Insert into ChromaDB
    total_items = len(ids)
    print(f"Info: Inserting {total_items} items into ChromaDB...")

    for i in range(0, total_items, CHROMA_BATCH_SIZE):
        end_idx = min(i + CHROMA_BATCH_SIZE, total_items)
        batch_ids = ids[i:end_idx]
        batch_docs = documents[i:end_idx]
        batch_embs = embeddings[i:end_idx]
        batch_meta = metadatas[i:end_idx]

        try:
            collection.add(
                ids=batch_ids,
                embeddings=batch_embs,
                documents=batch_docs,
                metadatas=batch_meta
            )
            print(f"   Added batch {i} to {end_idx}.")
        except Exception as e:
            print(f"Error adding batch {i}: {e}")
            return

    print("Info: All data loaded successfully.")

    # 5. Verification Query (Test)
    print("\n--- Performing Test Query ---")
    model = get_local_model()

    if model:
        test_query = "research limitation"
        print(f"Query: '{test_query}'")

        # Generate embedding for query
        query_emb = model.encode(test_query, normalize_embeddings=True)
        query_emb_list = query_emb.tolist()  # Chroma needs list

        results = collection.query(
            query_embeddings=[query_emb_list],
            n_results=3,
            include=['documents', 'distances']
        )

        print("Results:")
        for idx, doc in enumerate(results['documents'][0]):
            dist = results['distances'][0][idx]
            print(f"  {idx + 1}. '{doc}' (Distance: {dist:.4f})")

    print("\nInfo: Setup of 'chroma_db_store_qwen' complete.")


if __name__ == "__main__":
    main()