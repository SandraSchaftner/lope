[
    {
        "paper_id": "http://orkg.org/orkg/resource/R138187",
        "research_problem": "Recommender systems for Smart city",
        "orkg_properties": "['research problem', 'has contribution type', 'has been evaluated in the City', 'has been evaluated in the Country', 'belongs to Smart City Dimension', 'helps achieve Smart city Goal', 'Addresses Smart city Action', 'has Application Scope', 'has Data Source', 'has Target users', 'has Recommended items', 'uses Recommendation approach', 'uses Recommendation Method', 'is based on User Preferences Type', 'Is based on Explicit User Preferences', 'Is based on Implicit User Preferences', 'Exploited data', 'has Implementation level']",
        "nechakhin_result": "['Machine Learning Algorithms',\n 'Collaborative Filtering',\n 'Content-based Filtering',\n 'Hybrid Filtering',\n 'Matrix Factorization',\n 'Deep Learning',\n 'Feature Extraction',\n 'Data Mining',\n 'Data Preprocessing',\n 'User-based Filtering',\n 'Item-based Filtering',\n 'Association Rule Mining',\n 'Context-aware Filtering',\n 'Location-based Filtering',\n 'Temporal Filtering',\n 'Evaluation Metrics',\n 'Recommendation Accuracy',\n 'Recommendation Diversity',\n 'Recommendation Serendipity',\n 'Cold Start Problem',\n 'Scalability',\n 'Real-time Recommendation',\n 'Privacy',\n 'Trustworthiness',\n 'Data Fusion',\n 'Contextual Information',\n 'User Profile',\n 'Item Profile',\n 'Contextual Factors',\n 'Information Retrieval',\n 'Semantic Analysis',\n 'Data Integration']",
        "nechakhin_mappings": 5,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "A building permit system for smart cities: A cloud-based framework",
        "abstract": "In this paper we propose a novel, cloud-based framework to support citizens and city officials in the building permit process. The proposed framework is efficient, user-friendly, and transparent with a quick turn-around time for homeowners. Compared to existing permit systems, the proposed smart city permit framework provides a pre-permitting decision workflow, and incorporates adata analyticsand mining module that enables the continuous improvement of both theend user experienceand the permitting and urban planning processes. This is enabled through a data mining-powered permit recommendation engine as well as a data analytics process that allow a gleaning of key insights for real estate development and city planning purposes, by analyzing how users interact with the system depending on their location, time, and type of request. The novelty of the proposed framework lies in the integration of a pre-permit processing front-end with permit processing and data analytics & mining modules, along with utilization of techniques for extracting knowledge from the data generated through the use of the system. The proposed framework is completely cloud-based, such that any city can deploy it with lower initial as well as maintenance costs. We also present a proof-of-concept use case, using real permit data from New York City.\nA smart built environment has become necessary for ensuring social well-being due to uncontrolled population growth and unrestrainable urban sprawl. In this connection, effective land administration is a significant element to actualize sustainable development. Yet existing building permit procedures fail to satisfy the need for current construction demands because of the insufficient transparency and inefficient procedures. Two dimensional (2D) based systems also remain incapable to unambiguously delineate the property ownership related to complex buildings. Keeping up-to-date the three dimensional (3D) urban models is another key for smart cities but this issue has become difficult owing to the rapid changes in the cities. In this sense, this paper first examines thoroughly the current situation in Turkey in terms of the building permit procedures, land administration, and 3D city modeling. Then, the paper detailedly proposes a reformative framework. The framework consists of the use of digital building models for both building permit processes and 3D registration of property ownership, as well as updating the 3D city model databases. The proposed framework is evaluated in terms of its applicability with a discussion of the prospective directions.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139297",
        "research_problem": "Intelligent systems for Smart city",
        "orkg_properties": "['research problem', 'has contribution type', 'has been evaluated in the City', 'has been evaluated in the Country', 'belongs to Smart City Dimension', 'helps achieve Smart city Goal', 'Addresses Smart city Action', 'has Application Scope', 'has Data Source', 'has Target users', 'has Recommended items', 'uses Recommendation approach', 'uses Recommendation Method', 'is based on User Preferences Type', 'Is based on Explicit User Preferences', 'Is based on Implicit User Preferences', 'Exploited data', 'has Implementation level']",
        "nechakhin_result": "['Artificial Intelligence', 'Machine Learning', 'Deep Learning', 'Data Analytics', 'Internet of Things', 'Urban Planning', 'Smart Infrastructure', 'Sustainable Development', 'Energy Efficiency', 'Smart Governance']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "What's going on in my city?: recommender systems and electronic participatory budgeting",
        "abstract": "In this paper, we present electronic participatory budgeting (ePB) as a novel application domain for recommender systems. On public data from the ePB platforms of three major US cities - Cambridge, Miami and New York City-, we evaluate various methods that exploit heterogeneous sources and models of user preferences to provide personalized recommendations of citizen proposals. We show that depending on characteristics of the cities and their participatory processes, particular methods are more effective than others for each city. This result, together with open issues identified in the paper, call for further research in the area."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R146022",
        "research_problem": "Smart city digital transformation",
        "orkg_properties": "['Context', 'research problem', 'has methodology', 'Components ', 'Issue(s) Addressed\\t', 'Technologies Deployed']",
        "nechakhin_result": "['City infrastructure',\n 'Internet of Things (IoT)',\n 'Data analytics',\n 'Cloud computing',\n 'Artificial intelligence (AI)',\n 'Communication networks',\n 'Urban planning',\n 'Energy management',\n 'Transportation systems',\n 'Citizen engagement',\n 'Cybersecurity',\n 'Sustainability']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "A Framework for a Smart City Design: Digital Transformation in the Helsinki Smart City",
        "abstract": "Recently, there has been substantial interest in the concept of a smart city, as it has been a viable solution to the dilemmas created by the urbanization of cities. Digital technologies—such as Internet-of-Things, artificial intelligence, big data, and geospatial technologies—are closely associated with the concept of a smart city. By means of modern digital technologies, cities aim to optimize their performance and services. Further, cities actively endorse modern digital technologies to foster digitalization and the emergence of data-based innovations and a knowledge economy. In this paper, a framework for a smart city design is presented. The framework considers a smart city from the perspective of four dimensions—strategy, technology, governance, and stakeholders. The framework is complemented with sub-dimensions, and the purpose of this framework is to strengthen the governance and sustainability of smart city initiatives. Further, the proposed framework is applied to the Helsinki smart city, the capital of Finland. The objective is to analyse the Helsinki smart city through dimensions presented in the framework and learn how the city of Helsinki governs and implements its smart city initiatives.\nThe foundation for the smart city framework presented in this paper originates from the prior work presented by Hämäläinen and Tyrväinen (2018). The framework was applied to the Helsinki smart city. Data for empirical research were collected by interviewing persons and stakeholders involved in the development of the Helsinki smart city (Table1). The semi-structured interview protocol was employed in interviews, which provided flexibility and the possibility for a deeper understanding of the development of Helsinki. Interviewee 1 represented the Helsinki environmental protection unit and was in charge of Helsinki city’s energy and climate statistical data. Interviewee 2, Deputy CEO, represented the Smart Kalasatama project at Forum Virium Ltd. Interviewees 3 and 4 represented Helsinki Region Infoshare, an organization that releases Helsinki city’s open data. Interviewee 5 was a community manager at Smart Kalasatama project, who was responsible of stakeholder relations. Interviewees 6 and 7 represented residents of the Smart Kalasatama district. All interviews were audio recorded and transcribed after the interviews. Additional data were collected by attending workshops related to smart cities and seminars in Finland, as well as reviewing official Helsinki city reports, documents, and websites. Data were collected during the period May 2017–February 2019.Table 1 Empirical data collectionFull size table\nThe capital of Finland, Helsinki, has over 600,000 inhabitants. The total area of the city is 719 km2, of which almost 70% is sea (502 km2) and 30% is land (217 km2). The population density in Helsinki is almost 3000 inhabitants per km2. Smart Kalasatama is a strategic smart city development district in Helsinki. It is a new residential area, which is expected to provide homes for approximately 25,000 inhabitants by 2040. As a strategic smart city development area, Smart Kalasatama provides facilities for agile smart city pilots with a multi-stakeholder collaboration. The development of Smart Kalasatama is facilitated by Forum Virium Helsinki (FVH) Ltd., an innovation business unit owned by Helsinki city. Further, Helsinki is part of the ‘The Six City Strategy’ project, which delivers smart city pilot projects in fields such as smart mobility"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R138642",
        "research_problem": "Smart city ontology",
        "orkg_properties": "['research problem', 'Linked Ontology', 'Ontology domains']",
        "nechakhin_result": "['Urban planning', 'Internet of Things', 'Data analytics', 'Sustainable development', 'Wireless communication', 'Energy management', 'Citizen participation', 'Traffic management', 'Environmental monitoring', 'Infrastructure management']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Km4City ontology building vs data harvesting and cleaning for smart-city services",
        "abstract": "Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies andknowledge basefor smart city. Smart City ontology is not yet standardized, and a lot ofresearch workis needed to identify models that can easily support thedata reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system fordata ingestionand reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing abig datavolume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and thebig data architecturefor the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R151413",
        "research_problem": "digital twin",
        "orkg_properties": "['research problem', 'has form', 'has functionality', 'has miscellaneous qualitative']",
        "nechakhin_result": "['Technology', 'Simulation', 'Data', 'Virtualization', 'Internet of Things', 'Cyber-Physical Systems', 'Modeling', 'Machine Learning', 'Artificial Intelligence', 'Analytics', 'Predictive Maintenance']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Consistency check to synchronize the Digital Twin of manufacturing automation based on anchor points",
        "abstract": "Increasing product variety and the shortening of product lifecycles require a fast and inexpensive reconfiguration of existing manufacturing automation systems. To face this challenge one solution is a Digital Twin, which can be used to reduce the complexity and time of reconfiguration by early detection of design or process sequence errors of the system with a cross-domain simulation. For engineering the Digital Twin and systemically synchronizing the data of mechatronic components in the interdisciplinary engineering models of a Digital Twin during the life cycle of manufacturing automation systems, this paper presents a concept for the engineering of a Digital Twin based on model integration in a PLM IT-Platform and an Anchor-Point method to systematically detect variances of the mechatronic data structure between the digital models and the physical system. The data of a mechatronic component from interdisciplinary domains, developed by the corresponding engineering tools are referred to as anchor points. This paper analyses domain-specific challenges in automation software-code to develop an assistance system for rule-based consistency check and for synchronizing the engineering models of the Digital Twin of the manufacturing automation system based on the Anchor-Point method.\nDigital Twin is one of the promising digital technologies being developed at present to support digital transformation and decision making in multiple industries. While the concept of a Digital Twin is nearly 20 years old, it continues to evolve as it expands to new industries and use cases. This has resulted in a continually increasing variety of definitions that threatens to dilute the concept and lead to ineffective implementations of the technology. There is a need for a consolidated and generalized definition, with clearly established characteristics to distinguish what constitutes a Digital Twin and what does not. This paper reviews 46 Digital Twin definitions given in the literature over the past ten years to propose a generalized definition that encompasses the breadth of options available and provides a detailed characterization which includes criteria to distinguish the Digital Twin from other digital technologies. Next, a process and considerations for the implementation of Digital Twins is presented through a case study. Digital Twin future needs and opportunities are also outlined.\nVarious kinds of engineering software and digitalized equipment are widely applied through the lifecycle of industrial products. As a result, massive data of different types are being produced. However, these data are hysteretic and isolated from each other, leading to low efficiency and low utilization of these valuable data. Simulation based on theoretical and static model has been a conventional and powerful tool for the verification, validation, and optimization of a system in its early planning stage, but no attention is paid to the simulation application during system run-time. With the development of new-generation information and digitalization technologies, more data can be collected, and it is time to find a way for the deep application of all these data. As a result, the concept of digital twin has aroused much concern and is developing rapidly. Dispute and discussions around concepts, paradigms, frameworks, applications, and technologies of digital twin are on the rise both in academic and industrial communities. After a complete search of several databases "
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139853",
        "research_problem": "What is the tole of conservation of cultural heritage in the progres towards UN Development Goals?",
        "orkg_properties": "['method', 'research problem', 'has subject domain', 'Material', 'Process', 'has conclusion']",
        "nechakhin_result": "['conservation of cultural heritage', 'UN Development Goals', 'role', 'progress']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "SMART CITIES AND HERITAGE CONSERVATION: DEVELOPING A SMARTHERITAGE AGENDA FOR SUSTAINABLE INCLUSIVE COMMUNITIES",
        "abstract": "This paper discusses the potential of current advancements in Information Communication Technologies (ICT) for cultural heritage preservation, valorization and management within contemporary cities. The paper highlights the potential of virtual environments to assess the impacts of heritage policies on urban development. It does so by discussing the implications of virtual globes and crowdsourcing to support the participatory valuation and management of cultural heritage assets. To this purpose, a review of available valuation techniques is here presented together with a discussion on how these techniques might be coupled with ICT tools to promote inclusive governance."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139927",
        "research_problem": "developing an energy efficiency solution for a historicaal building",
        "orkg_properties": "['method', 'research problem', 'study location (country)', 'has subject domain', 'Material', 'has smart city instance']",
        "nechakhin_result": "['building type', 'energy consumption', 'historical preservation', 'energy efficiency technology', 'building materials', 'architectural design', 'heating and cooling systems', 'energy saving strategies', 'urban environment', 'building codes and regulations']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Smart Cities and Historical Heritage",
        "abstract": "The theme of smart grids will connote in the immediate future the production and distribution of electricity, integrating effectively and in a sustainable way energy deriving from large power stations with that distributed and supplied by renewable sources. In programmes of urban redevelopment, however, the historical city has not yet been subject to significant experimentation, also due to the specific safeguard on this kind of Heritage. This reflection opens up interesting new perspectives of research and operations, which could significantly contribute to the pursuit of the aims of the Smart City. This is the main goal of the research here presented and focused on the binomial renovation of a historical complex/enhancement and upgrading of its energy efficiency.PDFDownloads\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139975",
        "research_problem": "What are the reasons for the weak substantiation of cultural heritage in smart city strategies?",
        "orkg_properties": "['method', 'uses framework', 'research problem', 'has subject domain', 'has smart city instance', 'Data', 'Process', 'has objective']",
        "nechakhin_result": "['Cultural heritage', 'Smart city strategies', 'Weak substantiation', 'Reasons']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "CULTURAL HERITAGE IN SMART CITY ENVIRONMENTS: THE UPDATE",
        "abstract": ".In 2017 we published a seminal research study in the International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences about how smart city tools, solutions and applications underpinned historical and cultural heritage of cities at that time (Angelidou et al. 2017). We now return to investigate the progress that has been made during the past three years, and specifically whether the weak substantiation of cultural heritage in smart city strategies that we observed in 2017 has been improved. The newest literature suggests that smart cities should capitalize on local strengths and give prominence to local culture and traditions and provides a handful of solutions to this end. However, a more thorough examination of what has been actually implemented reveals a (still) rather immature approach. The smart city cases that were selected for the purposes of this research include Tarragona (Spain), Budapest (Hungary) and Karlsruhe (Germany). For each one we collected information regarding the overarching structure of the initiative, the positioning of cultural heritage and the inclusion of heritage-related smart city applications. We then performed a comparative analysis based on a simplified version of the Digital Strategy Canvas. Our findings suggest that a rich cultural heritage and a broader strategic focus on touristic branding and promotion are key ingredients of smart city development in this domain; this is a commonality of all the investigated cities. Moreover, three different strategy architectures emerge, representing the different interplays among the smart city, cultural heritage and sustainable urban development. We conclude that a new generation of smart city initiatives is emerging, in which cultural heritage is of increasing importance. This generation tends to associate cultural heritage with social and cultural values, liveability and sustainable urban development.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140112",
        "research_problem": "to explain the notion of the city as a laboratory for innovation",
        "orkg_properties": "['research problem', 'Data', 'Process', 'Has finding', 'has outcome', 'has objective']",
        "nechakhin_result": "['urban studies', 'innovation', 'city planning', 'urban development', 'smart cities', 'urban design', 'technology', 'urbanization', 'sustainability', 'social innovation']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Smart cities of the future",
        "abstract": "Here we sketch the rudiments of what constitutes a smart city which we define as a city in which ICT is merged with traditional infrastructures, coordinated and integrated using new digital technologies. We first sketch our vision defining seven goals which concern: developing a new understanding of urban problems; effective and feasible ways to coordinate urban technologies; models and methods for using urban data across spatial and temporal scales; developing new technologies for communication and dissemination; developing new forms of urban governance and organisation; defining critical problems relating to cities, transport, and energy; and identifying risk, uncertainty, and hazards in the smart city. To this, we add six research challenges: to relate the infrastructure of smart cities to their operational functioning and planning through management, control and optimisation; to explore the notion of the city as a laboratory for innovation; to provide portfolios of urban simulation which inform future designs; to develop technologies that ensure equity, fairness and realise a better quality of city life; to develop technologies that ensure informed participation and create shared knowledge for democratic city governance; and to ensure greater and more effective mobility and access to opportunities for urban populations. We begin by defining the state of the art, explaining the science of smart cities. We define six scenarios based on new cities badging themselves as smart, older cities regenerating themselves as smart, the development of science parks, tech cities, and technopoles focused on high technologies, the development of urban services using contemporary ICT, the use of ICT to develop new urban intelligence functions, and the development of online and mobile forms of participation. Seven project areas are then proposed: Integrated Databases for the Smart City, Sensing, Networking and the Impact of New Social Media, Modelling Network Performance, Mobility and Travel Behaviour, Modelling Urban Land Use, Transport and Economic Interactions, Modelling Urban Transactional Activities in Labour and Housing Markets, Decision Support as Urban Intelligence, Participatory Governance and Planning Structures for the Smart City. Finally we anticipate the paradigm shifts that will occur in this research and define a series of key demonstrators which we believe are important to progressing a science of smart cities."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R74705",
        "research_problem": "Contributions of smart cities technologies to cultural heritage documentation and services ",
        "orkg_properties": "['approach', 'research problem']",
        "nechakhin_result": "['Smart cities technologies',\n 'Cultural heritage documentation',\n 'Cultural heritage services']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "Smart Cities and Cultural Heritage – A Review of Developments and Future Opportunities",
        "abstract": "This paper reviews a cross-section of international developments in smart cities and their implications for the cultural heritage sector. A main focus of the paper is an assessment of selected case studies in the cultural heritage sector exploring the use of smart platforms and visualisation technologies. The results highlight a particular set of current challenges, as well as providing scope for identifying future opportunities in developing smart cultural heritage services."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140051",
        "research_problem": "The research draws on Mouffe’s concept of agonistic relations to explore the diversifying ideals, rhetoric, and practices of hackathon organization.",
        "orkg_properties": "['method', 'research problem', 'has methodology', 'has subject domain', 'Material', 'Has finding']",
        "nechakhin_result": "['Concept of agonistic relations',\n'Exploring diversifying ideals',\n'Rhetoric of hackathon organization',\n'Practices of hackathon organization']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Hackathons and the Practices and Possibilities of Participation",
        "abstract": "Smart city developments have been subjected to technocratic envisioning and neoliberal urban developments. However, there have been attempts to reclaim the right to the city through organizing civic initiatives to widen the access to the making of future technologies and cities. This chapter draws on Mouffe’s concept of agonistic relations to explore the diversifying ideals, rhetoric, and practices of hackathon organization to consider how they might cooperate with or contest one another and provide alternative means to technology and city making. The chapter analyzes different ways of organizing hackathons and discusses the opportunities for participants with diverse social backgrounds, knowledges and technical competences to join and work together. By examining the conflictual positions, articulations, and arrangements to widen participation, the chapter suggests that more open, inclusive, and collaborative city-making events might be possible. Further work is needed to examine conflictual hackathon participation practices and other civic initiatives to pursue a more egalitarian smart city.\nSmart city developments have been subjected to technocratic envisioning and neoliberal urban developments. However, there have been attempts to reclaim the right to the city through organizing civic initiatives to widen the access to the making of future technologies and cities. This chapter draws on Mouffe’s concept of agonistic relations to explore the diversifying ideals, rhetoric, and practices of hackathon organization to consider how they might cooperate with or contest one another and provide alternative means to technology and city making. The chapter analyzes different ways of organizing hackathons and discusses the opportunities for participants with diverse social backgrounds, knowledges and technical competences to join and work together. By examining the conflictual positions, articulations, and arrangements to widen participation, the chapter suggests that more open, inclusive, and collaborative city-making events might be possible. Further work is needed to examine conflictual hackathon participation practices and other civic initiatives to pursue a more egalitarian smart city."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140059",
        "research_problem": "aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons",
        "orkg_properties": "['result', 'research problem', 'has size', 'Material', 'Data', 'Method', 'has dubject domain', 'Has finding']",
        "nechakhin_result": "['startup factors', 'decision factors', 'open data hackathons', 'developer participation']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Open data hackathons: an innovative strategy to enhance entrepreneurial intention",
        "abstract": "PurposeIn terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services. Hackathons encourage the development of new applications using open data and the creation of startups based on these applications. Researchers focus on factors that affect nascent entrepreneurs’ decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet. This paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons.Design/methodology/approachIn total, 70 papers were examined and analyzed using a three-phased literature review methodology, which was suggested byWebster and Watson (2002).These surveys investigated several factors that affect a nascent entrepreneur to create a startup.FindingsEventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors.Originality/valueThe paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons.\nFindingsEventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors.\nPurposeIn terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services. Hackathons encourage the development of new applications using open data and the creation of startups based on these applications. Researchers focus on factors that affect nascent entrepreneurs’ decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet. This paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139736",
        "research_problem": "How to present contested heritage in a digital archive?",
        "orkg_properties": "['method', 'Country of study', 'has  start of period', 'has end of period', 'research problem', 'Institution', 'has subject domain', 'has stakeholder']",
        "nechakhin_result": "['heritage', 'contested heritage', 'digital archive', 'presentation', 'archival presentation', 'heritage preservation', 'digital preservation', 'archival materials', 'cultural heritage', 'cultural preservation', 'digital technologies', 'heritage representation', 'heritage documentation', 'archival practices']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Public History and Contested Heritage: Archival Memories of the Bombing of Italy",
        "abstract": "This article presents a case study of a collaborative public history project between participants in two countries, the United Kingdom and Italy. Its subject matter is the bombing war in Europe, 1939-1945, which is remembered and commemorated in very different ways in these two countries: the sensitivities involved thus constitute not only a case of public history conducted at the national level but also one involving contested heritage. An account of the ways in which public history has developed in the UK and Italy is presented. This is followed by an explanation of how the bombing war has been remembered in each country. In the UK, veterans of RAF Bomber Command have long felt a sense of neglect, largely because the deliberate targeting of civilians has not fitted comfortably into the dominant victor narrative. In Italy, recollections of being bombed have remained profoundly dissonant within the received liberation discourse. The International Bomber Command Centre Digital Archive (or Archive) is then described as a case study that employs a public history approach, focusing on various aspects of its inclusive ethos, intended to preserve multiple perspectives. The Italian component of the project is highlighted, problematising the digitisation of contested heritage within the broader context of twentieth-century history. Reflections on the use of digital archiving practices and working in partnership are offered, as well as a brief account of user analytics of the Archive through its first eighteen months online."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139784",
        "research_problem": "How to integrate participatory approaches in hertage diplomacy?",
        "orkg_properties": "['Country of study', 'research problem', 'has subject domain', 'has stakeholder']",
        "nechakhin_result": "['participatory approaches', 'heritage diplomacy']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "The Management Of Heritage In Contested Cross-Border Contexts: Emerging Research On The Island Of Ireland",
        "abstract": "DescriptionThis paper introduces the recently begun REINVENT research project focused on the management of heritage in the cross-border cultural landscape of Derry~Londonderry. The importance of facilitating dialogue over cultural heritage to the maintenance of ‘thin’ borders in contested crossborder contexts is underlined in the paper, as is the relatively favourable strategic policy context for progressing ‘heritage diplomacy’ on the island of Ireland. However, it is argued that more inclusive and participatory approaches to the management of heritage are required to assist in the mediation of contestation, particularly accommodating a greater diversity of ‘non-expert’ opinion, in addition to helping identify value conflicts and dissonance. The application of digital technologies in the form of Public Participation Geographic Information Systems (PPGIS) is proposed, and this is briefly discussed in relation to some of the expected benefits and methodological challenges that must be addressed in the REINVENT project. The paper concludes by emphasising the importance of dialogue and knowledge exchange between academia and heritage policymakers/practitioners."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139788",
        "research_problem": "How walking methods can better illuminate post‐conflict space?",
        "orkg_properties": "['has countries', 'research problem', 'Institution', 'has subject domain', 'Data', 'Material', 'discusses method', 'has stakeholder']",
        "nechakhin_result": "['walking methods', 'post‐conflict space', 'illumination']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Troubling places: Walking the “troubling remnants” of post‐conflict space",
        "abstract": "This paper explores the productive potential of walking methods in post-conflict space, with particular emphasis on Northern Ireland. We argue that walking methods are especially well suited to studying post-conflict spatial arrangements, yet remain underutilised for a variety of reasons. Specifically, we argue that walking methods can “trouble” dominant productions of post-conflict space, revealing its storied depth, multi-temporality, and the alternative narratives of the past that frequently remain hidden in places touched by violence.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139800",
        "research_problem": "What characterises contested heritage?",
        "orkg_properties": "['method', 'research problem', 'Has endpoint', 'has subject domain', 'has sources', 'has stakeholder']",
        "nechakhin_result": "['Historical significance',\n 'Cultural importance',\n 'Social relevance',\n 'Differing perspectives',\n 'Historical narratives',\n 'Political implications',\n 'Public perception',\n 'Ownership claims',\n 'Legal disputes',\n 'Restoration and preservation efforts',\n 'Collaborative processes',\n 'Ethics and values',\n 'Memory and identity',\n 'Representation and portrayal']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "A systematic review of literature on contested heritage",
        "abstract": "ABSTRACTContested heritage has increasingly been studied by scholars over the last two decades in multiple disciplines, however, there is still limited knowledge about what contested heritage is and how it is realized in society. Therefore, the purpose of this paper is to produce a systematic literature review on this topic to provide a holistic understanding of contested heritage, and delineate its current state, trends and gaps. Methodologically, four electronic databases were searched, and 102 journal articles published before 2020 were extracted. A content analysis of each article was then conducted to identify key themes and variables for classification. Findings show that while its research often lacks theoretical underpinnings, contested heritage is marked by its diversity and complexity as it becomes a global issue for both tourism and urbanization. By presenting a holistic understanding of contested heritage, this review offers an extensive investigation of the topic area to help move literature pertaining contested heritage forward.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139810",
        "research_problem": "How to define digital heritage interpretation?",
        "orkg_properties": "['result', 'research problem', 'has subject domain', 'has stakeholder', 'Material', 'Data']",
        "nechakhin_result": "['Digital technology', 'Cultural heritage', 'Interpretation techniques', 'Digital preservation', 'Museum studies', 'Information technology', 'Virtual reality', 'Archaeology', 'Art history']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Digital heritage interpretation: a conceptual framework",
        "abstract": "ABSTRACT‘Heritage Interpretation’ has always been considered as an effective learning, communication and management tool that increases visitors’ awareness of and empathy to heritage sites or artefacts. Yet the definition of ‘digital heritage interpretation’ is still wide and so far, no significant method and objective are evident within the domain of ‘digital heritage’ theory and discourse. Considering ‘digital heritage interpretation’ as a process rather than as a tool to present or communicate with end-users, this paper presents a critical application of a theoretical construct ascertained from multiple disciplines and explicates four objectives for a comprehensive interpretive process. A conceptual model is proposed and further developed into a conceptual framework with fifteen considerations. This framework is then implemented and tested on an online platform to assess its impact on end-users’ interpretation level. We believe the presented interpretive framework (PrEDiC) will help heritage professionals and media designers to develop interpretive heritage project.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139820",
        "research_problem": "How to connect the scholarly modes of communication and stakeholder-led participatory cultures in difficult heritage?",
        "orkg_properties": "['research problem', 'has subject domain', 'has stakeholder', 'materal', 'has communication channel']",
        "nechakhin_result": "['scholarly modes of communication', 'stakeholder-led participatory cultures', 'difficult heritage']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Digital Media, Participatory Culture, and Difficult Heritage: Online Remediation and the Trans-Atlantic Slave Trade",
        "abstract": "A diverse and changing array of digital media have been used to present heritage online. While websites have been created for online heritage outreach for nearly two decades, social media is employed increasingly to complement and in some cases replace the use of websites. These same social media are used by stakeholders as a form of participatory culture, to create communities and to discuss heritage independently of narratives offered by official institutions such as museums, memorials, and universities. With difficult or “dark” heritage—places of memory centering on deaths, disasters, and atrocities—these online representations and conversations can be deeply contested. Examining the websites and social media of difficult heritage, with an emphasis on the trans-Atlantic slave trade provides insights into the efficacy of online resources provided by official institutions, as well as the unofficial, participatory communities of stakeholders who use social media for collective memories."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139670",
        "research_problem": "How to create and deploy smart replicas in interactive musem exhibitions?",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality']",
        "nechakhin_result": "['interactive museum exhibitions', 'smart replicas', 'creation process', 'deployment process', 'replica design', 'interactivity', 'technology integration', 'visitor experience', 'augmented reality', 'user interface', 'user engagement', 'interaction design', 'sensor technology', 'data analytics', 'storage and retrieval', 'accessibility', 'security', 'sustainability']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Using Tangible Smart Replicas as Controls for an Interactive Museum Exhibition",
        "abstract": "This paper presents the design, creation and use of tangible smart replicas in a large-scale museum exhibition. We describe the design rationale for the replicas, the process used in their creation, as well as the implementation and deployment of these replicas in a live museum exhibition. Deployment of the exhibition resulted in over 14000 visitors interacting with the system during the 6 months that the exhibition was open. Based on log data, interviews and observations, we examine the reaction to these smart replicas from the point of view of the museum curators and also of the museum's visitors and reflect on the fulfillment of our expectations."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139674",
        "research_problem": "What types of information architecture support smart exhibits in museums?",
        "orkg_properties": "['method', 'research problem', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality', 'Has number of user interactions']",
        "nechakhin_result": "['information architecture', 'smart exhibits', 'museums']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "exhiSTORY: Smart exhibits that tell their own stories",
        "abstract": "Museum exhibitions are designed to tell a story; this story is woven by curators and in its context a particular aspect of each exhibit, fitting to the message that the story is intended to convey, is highlighted. Adding new exhibits to the story requires curators to identify for each exhibit its aspects that fit to the message of the story and position the exhibit at the right place in the story thread. The availability of rich semantic information for exhibits, allows for exploiting the wealth of meanings that museum exhibits express, enabling the automated or semi-automated generation of practically countless stories that can be told. Personalization algorithms can then be employed to choose from these stories the ones most suitable for each individual user, based on the semantics of the stories and information within the user profile. In this work we examine how opportunities arising from technological advances in the fields of IoT and semantics can be used to develop smart, self-organizing exhibits that cooperate with each other and provide visitors with comprehensible, rich, diverse, personalized and highly stimulating experiences. These notions are included in the design of a system named exhiSTORY, which also exploits previously ignored information and identifies previously unseen semantic links. We present the architecture of the system and discuss its application potential.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139681",
        "research_problem": "How does the physical affordance of tangible interaction affect the communication of built heritage information?",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality', 'Has number of user interactions']",
        "nechakhin_result": "['physical affordance', 'tangible interaction', 'communication', 'built heritage information']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Communicating Built Heritage Information Using Tangible Interaction Approach",
        "abstract": "Built heritage objects possess multiple types of information, varying from simple, factual aspects to more complex qualitative information and values, such as the architectural qualities, the construction techniques, or symbolic meanings of monuments. This qualitative information is relatively difficult to communicate using the conventional ways like museum labels or audio guides. Nonetheless, tangible interaction is a promising paradigm for communicating tacit information, its qualities have been demonstrated in a wide range of applications in different realms. Therefore, this study investigates how tangible interaction can enable the communication of qualitative information of built heritage to lay visitors. The main objectives of this study are communicating tacit and architectural qualities of built heritage in a physical form, investigating the effect of tangible interaction on social interaction among heritage visitors, and enhancing visitors' in-situ experience of built heritage or 1:1 replicas. Our early findings indicate the capability of tangible interaction for engaging museum visitors to accomplish additional endeavors, and facilitating their understanding of cultural values and architectural qualities of built heritage."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139691",
        "research_problem": "What is the value of personalised tangible data souvenirs as a bridge between the physical, personal museum experience? ",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality']",
        "nechakhin_result": "['Personalisation', 'Tangible data souvenirs', 'Physical museum experience', 'Personal museum experience']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Tangible data souvenirs as a bridge between a physical museum visit and online digital experience",
        "abstract": "This paper presents the design, implementation, use and evaluation of a tangible data souvenir for an interactive museum exhibition. We define a data souvenir as the materialisation of the personal visiting experience: a data souvenir is dynamically created on the basis of data recorded throughout the visit and therefore captures and represents the experience as lived. The souvenir provides visitors with a memento of their visit and acts as a gateway to further online content. A step further is to enable visitors to contribute, in other words the data souvenir can become a means to collect visitor-generated content. We discuss the rationale behind the use of a data souvenir, the design process and resulting artefacts, and the implementation of both the data souvenir and online content system. Finally, we examine the installation of the data souvenirs as part of a long-lasting exhibition: the use of this souvenir by visitors has been logged over 7 months and issues around the gathering of user-generated content in such a way are discussed.\nThe design brief was to create a souvenir that fit with the theme of the exhibition, while being easily produced in the thousands with minimal maintenance and also offering layers of information to the visitor. This included a representation of some aspects of their visit, the connection between the displays within the exhibition and the city of The Hague and access to an online post-visit experience where the visitors could find curated content as well as visitor-generated content, such as personal and family memories.A number of concepts for the data souvenir were created (Fig.7) in order to explore the design space and to look at issues such as ease of production, cost and customisation. These concepts included generative postcards, an overlay for a street map, and a set of travel or identification papers. Each of these connected in some way to the exhibition, whether through the use of place (the street map), the type of object (the travel papers), or the content used to create them (the postcard).Fig. 7Different concepts explore at the creative phase: a set of foldable tickets (top left); a written postcard with a personalised stamp (top right); different personalised stamps (bottom left); and a layered and annotated city map (bottom right)Full size imageThe postcard concept was selected based on considerations such as the number of expected visitors (about 20,000), production process and cost. Industrial printers and paper used to print tickets for events were considered; such a technology is very fast and reliably prints thousands of cards before any maintenance for ink or paper is needed. In addition, the card background can be pre-printed on both sides with a high-quality customised image in colour (Fig.8), while patterns in black can be generated and printed dynamically so as to implement the personalisation component (Fig.9).Fig. 8The standard pre-printed postcard front and back. Thefronthas predisposed areas for the printing that is done dynamically after the analysis of the logs (Fig.9); thebackshows a stylised map of The Hague with the locations represented in the exhibition indicated. Thenumberson this map also match the ones on the stampsFull size imageFig. 9The personalised souvenir summarises the highlights of the visit. The postcard on theleftshows the visitor received the English narratives, followed the story of the German and spent the most time at locations 1, 3 and 9; the postca"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139698",
        "research_problem": "How to build a holistic approach for planning of multimedia, virtual, and mixed reality applications based on the concept of “augmented” and multisensory experience, innovative tangible user interfaces, and storytelling techniques?",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality', 'Has number of user interactions']",
        "nechakhin_result": "['Multimedia applications', 'Virtual reality applications', 'Mixed reality applications', 'Augmented experience', 'Multisensory experience', 'Innovative tangible user interfaces', 'Storytelling techniques']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Accessibility, Natural User Interfaces and Interactions in Museums: The IntARSI Project",
        "abstract": "In a museum context, people have specific needs in terms of physical, cognitive, and social accessibility that cannot be ignored. Therefore, we need to find a way to make art and culture accessible to them through the aid of Universal Design principles, advanced technologies, and suitable interfaces and contents. Integration of such factors is a priority of the Museums General Direction of the Italian Ministry of Cultural Heritage, within the wider strategy of museum exploitation. In accordance with this issue, the IntARSI project, publicly funded, consists of a pre-evaluation and a report of technical specifications for a new concept of museology applied to the new Museum of Civilization in Rome (MuCIV). It relates to planning of multimedia, virtual, and mixed reality applications based on the concept of “augmented” and multisensory experience, innovative tangible user interfaces, and storytelling techniques. An inclusive approach is applied, taking into account the needs and attitudes of a wide audience with different ages, cultural interests, skills, and expectations, as well as cognitive and physical abilities."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139703",
        "research_problem": "How to bridge the gap between the physical and the digital by means of technology in museums?",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality', 'Has number of user interactions']",
        "nechakhin_result": "['technology', 'museums', 'digital', 'physical', 'bridge', 'gap']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "\"\n            Mobiles for museum visit should be abolished\n            \": a comparison of smart replicas, smart cards, and phones",
        "abstract": "A comparative evaluation of smart replicas, phone app and smart cards looked at the personal preferences of visitors and the appeal of mobiles in museum exhibitions. As part of an exhibition evaluation, 76 participants used all three interactions modes and gave their opinions in a questionnaire. The result shows that Phone and Replica are equally liked but the Phone is the most disliked interaction mode. Preference for the phone is due to its mobility as opposed to a listen in place interaction; but the phone takes the attention away from the exhibition and isolates from the group. Visitors expect museums to provide the phones as opposed to apps for \"bring your own\"."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R161811",
        "research_problem": "X-ray laser advances and applications",
        "orkg_properties": "['research problem', 'Paper type', 'has system qualities']",
        "nechakhin_result": "['X-ray laser technology', 'X-ray laser physics', 'X-ray laser applications', 'X-ray laser development', 'X-ray laser experiments', 'X-ray laser materials', 'X-ray laser interactions', 'X-ray laser spectroscopy', 'X-ray laser diagnostics', 'X-ray laser imaging', 'X-ray laser sources', 'X-ray laser optics', 'X-ray laser simulation', 'X-ray laser theory', 'X-ray laser design']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Beam optics of exploding foil plasma x‐ray lasers",
        "abstract": "In soft x‐ray lasers, amplification is achieved as the x rays propagate down a long narrow plasma column. Refraction, due to electron density gradients, tends to direct the x‐rays out of high density regions. This can have the undesirable effect of shortening the distance that the x ray stay within the plasma, thereby limiting the amount of amplification. The exploding foil design lessens refraction, but does not eliminate it. In this paper, a quantitative analysis of propagation and amplification in an exploding foil x‐ray laser is presented. The density and gain profiles within the plasma are modeled in an approximate manner, which enables considerable analytic progress. It is found that refraction introduces a loss term to the laser amplification. The beam pattern from a parabolic gain profile laser has a dominant peak on the x‐ray laser axis. The pattern from a quartic gain profile having a dip on‐axis can produce a profile with off‐axis peaks, in better agreement with recent experimental data."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R108960",
        "research_problem": "Biodiversity inventories with DNA based-tools",
        "orkg_properties": "['research problem', 'Biogeographical region', 'Locus (genetics)', 'DNA sequencing method', 'Phylum (Biology)', 'higher number estimated species (Method)', 'No. of estimated species (Method)', 'lower number estimated species (Method)', 'study location (country)', 'Order (Taxonomy - biology)', 'Class (Taxonomy - biology)', 'studied taxonomic group (Biology)']",
        "nechakhin_result": "['Taxonomic', 'Genetic', 'Spatial', 'Temporal', 'Methodological']",
        "nechakhin_mappings": 4,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Use of species delimitation approaches to tackle the cryptic diversity of an assemblage of high Andean butterflies (Lepidoptera: Papilionoidea)",
        "abstract": "Cryptic biological diversity has generated ambiguity in taxonomic and evolutionary studies. Single-locus methods and other approaches for species delimitation are useful for addressing this challenge, enabling the practical processing of large numbers of samples for identification and inventory purposes. This study analyzed an assemblage of high Andean butterflies using DNA barcoding and compared the identifications based on the current morphological taxonomy with three methods of species delimitation (automatic barcode gap discovery, generalized mixed Yule coalescent model, and Poisson tree processes). Sixteen potential cryptic species were recognized using these three methods, representing a net richness increase of 11.3% in the assemblage. A well-studied taxon of the genusVanessa, which has a wide geographical distribution, appeared with the potential cryptic species that had a higher genetic differentiation at the local level than at the continental level. The analyses were useful for identifying the potential cryptic species inPedaliodesandForsterinariacomplexes, which also show differentiation along altitudinal and latitudinal gradients. This genetic assessment of an entire assemblage of high Andean butterflies (Papilionoidea) provides baseline information for future research in a region characterized by high rates of endemism and population isolation.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R137480",
        "research_problem": "Polymeric nanoparticles for cancer treatment",
        "orkg_properties": "['Polymer', 'keywords', 'research problem', 'Animal model', 'location of study', 'Journal', 'Biodegredable or inorganic polymer', 'Nanoparticle visualisation', 'Uses drug', 'Organisations', 'Nanoparticles preparation method', 'Surfactant']",
        "nechakhin_result": "['Polymer type',\n 'Nanoparticle size and shape',\n 'Surface charge',\n 'Drug loading capacity',\n 'Drug release kinetics',\n 'Targeting ligands',\n 'Biocompatibility',\n 'In vitro characterization',\n 'In vivo studies',\n 'Toxicity',\n 'Therapeutic efficacy',\n 'Long-term stability']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "PLGA Nanoparticles Stabilized with Cationic Surfactant: Safety Studies and Application in Oral Delivery of Paclitaxel to Treat Chemical-Induced Breast Cancer in Rat",
        "abstract": "PurposeThis study was carried out to formulate poly(lactide-co-glycolide) (PLGA) nanoparticles using a quaternary ammonium salt didodecyl dimethylammonium bromide (DMAB) and checking their utility to deliver paclitaxel by oral route.MethodsParticles were prepared by emulsion solvent diffusion evaporation method. DMAB and particles stabilized with it were evaluated by MTT and LDH cytotoxicity assays. Paclitaxel was encapsulated in these nanoparticles and evaluated in a chemical carcinogenesis model in Sprague Dawley rats.ResultsMTT and LDH assays showed the surfactant to be safe toin vitrocell cultures at concentrations <33 μM. PLGA nanoparticles prepared using this stabilizer were also found to be non-toxic to cell lines for the duration of the study. When administered orally to rats bearing chemically induced breast cancer, nanoparticles were equally effective/better than intravenous paclitaxel in cremophor EL at 50% lower dose.ConclusionsThis study proves the safety and utility of DMAB in stabilizing preformed polymers like PLGA resulting in nanoparticles. This preliminary data provides a proof of concept of enabling oral chemotherapy by efficacy enhancement for paclitaxel.\nPLGA is a biodegradable and biocompatible polymer, and products based on this polymer are already approved by the United States Food & Drug Administrations (US FDA) for human use. In 1999, the US FDA approved a PLGA microsphere formulation, Nutropin Depot, as a once-a-month alternative to daily injections of human growth hormone. To increase the physical stability of nanoparticles, surfactants or stabilizers are used. Reports on the positive surface charge of DMAB (25) provided the incentive to aid the delivery of paclitaxel, since it is expected to ensure better interaction with the negatively charged cell membrane. This can result in increased retention time at the cell surface, thus increasing the chances of particle uptake. DMAB is capable of producing small and highly stable nanoparticles at 1%w/vconcentration (15). Due to the charged surface, the particle agglomeration is impeded.The particle preparation process was studied using variables like surfactant concentration, phase ratio, and homogenizer speed to understand the influence of these parameters on defining the particle characteristics. The smallest particle size with 1% surfactant might be due to better stabilization of the nanoglobules by a more comprehensive presence of the stabilizer at the interface of the two phases. Increase in specific surface area increases the surface free energy, and the decreasing particle size with increasing speed of the homogenizer reflects the transfer of higher amount of energy to the colloidal system. Similarly, a direct correlation was seen between particle size and ratio of organic-to-aqueous phase with 3:10 ratio producing particles around 70 nm (TableIII). This can be due to the surfactant effectively lining the interface between the globules and the aqueous phase when the internal phase volume is lower. In none of the parameters, we have seen a saturation effect implying that it is possible to increase or decrease the particle size beyond the obtained values, but the effect might not hold linear and would plateau outside a range. In our experience, the foremost criteria to be set is the desired particle size, and the particle preparation exercise should be carried out to then satisfy other product characteristics like drug-loading and residual surfactant concentra"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R137522",
        "research_problem": "Cervical cancer",
        "orkg_properties": "['Polymer', 'keywords', 'research problem', 'has cell line', 'location of study', 'Journal', 'Biodegredable or inorganic polymer', 'Nanoparticle visualisation', 'has  drug release studies', 'Uses drug', 'Organisations', 'Nanoparticles preparation method', 'Surfactant']",
        "nechakhin_result": "['Cancer type', 'Body part affected', 'Cancer stage', 'Treatment methods', 'Risk factors', 'Genetic factors', 'Epidemiology', 'Prevention strategies', 'Screening methods', 'Symptoms', 'Diagnosis techniques', 'Prognosis', 'Public health initiatives', 'Clinical trials']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 1,
        "nechakhin_deviation": 3,
        "title": "Paclitaxel-loaded poly(D,L-lactide-co-glycolide) nanoparticles for radiotherapy in hypoxic human tumor cells in vitro",
        "abstract": "Radioresistant hypoxic cells may contribute to the failure of radiation therapy in controlling certain tumors. Some studies have suggested the radiosensitizing effect of paclitaxel. The poly(D,L-lactide-co-glycolide)(PLGA) nanoparticles containing paclitaxel were prepared by o/w emulsification-solvent evaporation method. The physicochemical characteristics of the nanoparticles (i.e. encapsulation efficiency, particle size distribution, morphology, in vitro release) were studied. The morphology of the two human tumor cell lines: a carcinoma cervicis (HeLa) and a hepatoma (HepG2), treated with paclitaxel-loaded nanoparticles was photomicrographed. Flow cytometry was used to quantify the number of the tumor cells held in the G2/M phase of the cell cycle. The cellular uptake of nanoparticles was evaluated by transmission electronic microscopy. Cell viability was determined by the ability of single cell to form colonies in vitro. The prepared nanoparticles were spherical in shape with size between 200nm and 800nm. The encapsulation efficiency was 85.5％. The release behaviour of paclitaxel from the nanoparticles exhibited a biphasic pattern characterised by a fast initial release during the first 24 h, followed by a slower and continuous release. Co-culture of the two tumor cell lines with paclitaxel-loaded nanoparticles demonstrated that the cell morphology was changed and the released paclitaxel retained its bioactivity to block cells in the G2/M phase. The cellular uptake of nanoparticles was observed. The free paclitaxel and paclitaxel-loaded nanoparticles effectively sensitized hypoxic HeLa and HepG2 cells to radiation. Under this experimental condition, the radiosensitization of paclitaxel-loaded nanoparticles was more significant than that of free paclitaxel."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R138001",
        "research_problem": "Lung cancer",
        "orkg_properties": "['Polymer', 'keywords', 'research problem', 'has cell line', 'location of study', 'Journal', 'Cytotoxicity assay', 'Biodegredable or inorganic polymer', 'Nanoparticle visualisation', 'has  drug release studies', 'Uses drug', 'Organisations', 'Nanoparticles preparation method', 'Surfactant']",
        "nechakhin_result": "['Cancer type', 'Pathogenesis', 'Risk factors', 'Symptoms', 'Diagnosis', 'Treatment', 'Prevention', 'Epidemiology']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Paclitaxel-loaded PLGA nanoparticles: preparation, physicochemical characterization and in vitro anti-tumoral activity",
        "abstract": "The main objective of this study was to develop a polymeric drug delivery system for paclitaxel, intended to be intravenously administered, capable of improving the therapeutic index of the drug and devoid of the adverse effects of Cremophor®EL. To achieve this goal paclitaxel (Ptx)-loaded poly(lactic-co-glycolic acid) (PLGA) nanoparticles (Ptx-PLGA-Nps) were prepared by the interfacial deposition method. The influence of different experimental parameters on the incorporation efficiency of paclitaxel in the nanoparticles was evaluated. Our results demonstrate that the incorporation efficiency of paclitaxel in nanoparticles was mostly affected by the method of preparation of the organic phase and also by the organic phase/aqueous phase ratio. Our data indicate that the methodology of preparation allowed the formation of spherical nanometric (<200 nm), homogeneous and negatively charged particles which are suitable for intravenous administration. The release behaviour of paclitaxel from the developed Nps exhibited a biphasic pattern characterised by an initial fast release during the first 24 h, followed by a slower and continuous release. The in vitro anti-tumoral activity of Ptx-PLGA-Nps developed in this work was assessed using a human small cell lung cancer cell line (NCI-H69 SCLC) and compared to the in vitro anti-tumoral activity of the commercial formulation Taxol®. The influence of Cremophor®EL on cell viability was also investigated. Exposure of NCI-H69 cells to 25 μg/ml Taxol®resulted in a steep decrease in cell viability. Our results demonstrate that incorporation of Ptx in nanoparticles strongly enhances the cytotoxic effect of the drug as compared to Taxol®, this effect being more relevant for prolonged incubation times."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R138014",
        "research_problem": "Breast cancer",
        "orkg_properties": "['Polymer', 'keywords', 'research problem', 'has cell line', 'location of study', 'Journal', 'Cytotoxicity assay', 'Biodegredable or inorganic polymer', 'Nanoparticle visualisation', 'has  drug release studies', 'Uses drug', 'Organisations', 'Nanoparticles preparation method', 'Surfactant']",
        "nechakhin_result": "['Cancer type', 'Stage of breast cancer', 'Genetic markers', 'Tumor size', 'Lymph node involvement', 'Hormone receptor status', 'Age of patient', 'Treatment options', 'Survival rate', 'Risk factors', 'Family history of breast cancer', 'Screening methods']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Nanoparticles of lipid monolayer shell and biodegradable polymer core for controlled release of paclitaxel: Effects of surfactants on particles size, characteristics and in vitro performance",
        "abstract": "This work developed a system of nanoparticles of lipid monolayer shell and biodegradable polymer core for controlled release of anticancer drugs with paclitaxel as a model drug, in which the emphasis was given to the effects of the surfactant type and the optimization of the emulsifier amount used in the single emulsion solvent evaporation/extraction process for the nanoparticle preparation on the particle size, characters andin vitroperformance. The drug loaded nanoparticles were characterized by laser light scattering (LLS) for size and size distribution, field-emission scanning electron microscopy (FESEM) for surface morphology, X-ray photoelectron spectroscopy (XPS) for surface chemistry, zetasizer for surface charge, and high performance liquid chromatography (HPLC) for drug encapsulation efficiency andin vitrodrug release kinetics. MCF-7 breast cancer cells were employed to evaluate the cellular uptake and cytotoxicity. It was found that phospholipids of short chains such as 1,2-dilauroylphosphatidylocholine (DLPC) have great advantages over the traditional emulsifier poly(vinyl alcohol) (PVA), which is used most often in the literature, in preparation of nanoparticles of biodegradable polymers such as poly(d,l-lactide-co-glycolide) (PLGA) for desired particle size, character andin vitrocellular uptake and cytotoxicity. After incubation with MCF-7 cells at 0.250mg/ml NP concentration, the coumarin-6 loaded PLGA NPs of DLPC shell showed more effective cellular uptake versus those of PVA shell. The analysis of IC50, i.e. the drug concentration at which 50% of the cells are killed, demonstrated that our DLPC shell PLGA core NP formulation of paclitaxel could be 5.88-, 5.72-, 7.27-fold effective than the commercial formulation Taxol®after 24, 48, 72h treatment, respectively.\nLipid–polymer hybrid nanoparticles (LPNs) are core–shell nanoparticle structures comprising polymer cores and lipid/lipid–PEG shells, which exhibit complementary characteristics of both polymeric nanoparticles and liposomes, particularly in terms of their physical stability and biocompatibility. Significantly, the LPNs have recently been demonstrated to exhibit superiorin vivocellular delivery efficacy compared to that obtained from polymeric nanoparticles and liposomes. Since their inception, the LPNs have advanced significantly in terms of their preparation strategy and scope of applications. Their preparation strategy has undergone a shift from the conceptually simple two-step method, involving preformed polymeric nanoparticles and lipid vesicles, to the more principally complex, yet easier to perform, one-step method, relying on simultaneous self-assembly of the lipid and polymer, which has resulted in better products and higher production throughput. The scope of LPNs’ applications has also been extended beyond single drug delivery for anticancer therapy, to include combinatorial and active targeted drug deliveries, and deliveries of genetic materials, vaccines, and diagnostic imaging agents. This review details the current state of development for the LPNs preparation and applications from which we identify future research works needed to bring the LPNs closer to its clinical realization."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R142246",
        "research_problem": "COVID-19 Vaccine",
        "orkg_properties": "['research problem', 'Organisations', 'Vaccine Name', 'Delivery Vehicle', 'Delivery Route']",
        "nechakhin_result": "['Vaccine type',\n'Vaccine effectiveness',\n'Vaccine development process',\n'Clinical trials',\n'Side effects',\n'Vaccine distribution',\n'Vaccine administration',\n'Vaccine safety',\n'Vaccine efficacy',\n'Vaccine ingredients',\n'Vaccine production',\n'Vaccine storage',\n'Vaccine supply chain',\n'Vaccine hesitancy',\n'Vaccine accessibility',\n'Vaccine impact on different populations',\n'Vaccine research and development']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Safety and Immunogenicity of Two RNA-Based Covid-19 Vaccine Candidates",
        "abstract": "BackgroundSevere acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infections and the resulting disease, coronavirus disease 2019 (Covid-19), have spread to millions of persons worldwide. Multiple vaccine candidates are under development, but no vaccine is currently available. Interim safety and immunogenicity data about the vaccine candidate BNT162b1 in younger adults have been reported previously from trials in Germany and the United States.MethodsIn an ongoing, placebo-controlled, observer-blinded, dose-escalation, phase 1 trial conducted in the United States, we randomly assigned healthy adults 18 to 55 years of age and those 65 to 85 years of age to receive either placebo or one of two lipid nanoparticle–formulated, nucleoside-modified RNA vaccine candidates: BNT162b1, which encodes a secreted trimerized SARS-CoV-2 receptor–binding domain; or BNT162b2, which encodes a membrane-anchored SARS-CoV-2 full-length spike, stabilized in the prefusion conformation. The primary outcome was safety (e.g., local and systemic reactions and adverse events); immunogenicity was a secondary outcome. Trial groups were defined according to vaccine candidate, age of the participants, and vaccine dose level (10 μg, 20 μg, 30 μg, and 100 μg). In all groups but one, participants received two doses, with a 21-day interval between doses; in one group (100 μg of BNT162b1), participants received one dose.ResultsA total of 195 participants underwent randomization. In each of 13 groups of 15 participants, 12 participants received vaccine and 3 received placebo. BNT162b2 was associated with a lower incidence and severity of systemic reactions than BNT162b1, particularly in older adults. In both younger and older adults, the two vaccine candidates elicited similar dose-dependent SARS-CoV-2–neutralizing geometric mean titers, which were similar to or higher than the geometric mean titer of a panel of SARS-CoV-2 convalescent serum samples.ConclusionsThe safety and immunogenicity data from this U.S. phase 1 trial of two vaccine candidates in younger and older adults, added to earlier interim safety and immunogenicity data regarding BNT162b1 in younger adults from trials in Germany and the United States, support the selection of BNT162b2 for advancement to a pivotal phase 2–3 safety and efficacy evaluation. (Funded by BioNTech and Pfizer; ClinicalTrials.gov number,NCT04368728.)\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R142566",
        "research_problem": "Nanocrystals for cancer treatment",
        "orkg_properties": "['research problem', 'Uses drug', 'Nanoparticles preparation method', 'Surfactant', 'Delivery Route']",
        "nechakhin_result": "['nanocrystal size', 'nanocrystal composition', 'nanocrystal surface chemistry', 'nanocrystal stability', 'drug loading efficiency', 'controlled drug release', 'biocompatibility', 'in vivo toxicity', 'cellular uptake', 'effectiveness against different types of cancer']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Formulation and antitumor activity evaluation of nanocrystalline suspensions of poorly soluble anticancer drugs",
        "abstract": "Purpose. Determine if wet milling technology could be used to formulate water insoluble antitumor agents as stabilized nanocrystalline drug suspensions that retain biological effectiveness following intravenous injection.Methods. The versatility of the approach is demonstrated by evaluation of four poorly water soluble chemotherapeutic agents that exhibit diverse chemistries and mechanisms of action. The compounds selected were: piposulfan (alkylating agent), etoposide (topoisomerase II inhibitor), camptothecin (topoisomerase I inhibitor) and paclitaxel (antimitotic agent). The agents were wet milled as a 2% w/v solids suspension containing 1 % w/v surfactant stabilizer using a low energy ball mill. The size , physical stability and efficacy of the nanocrystalline suspensions were evaluated.Results. The data show the feasibility of formulating poorly water soluble anticancer agents as physically stable aqueous nanocrystalline suspensions. The suspensions are physically stable and efficacious following intravenous injection.Conclusions. Wet milling technology is a feasible approach for formulating poorly water soluble chemotherapeutic agents that may offer a number of advantages over a more classical approach.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144151",
        "research_problem": "PLGA-based nanoparticles as drug carriers",
        "orkg_properties": "['Advantages', 'Composition', 'research problem', 'Nanoparticles preparation method', 'Cargo', 'Type of nanocarrier']",
        "nechakhin_result": "['Polymer composition', 'Particle size', 'Surface charge', 'Drug-loading capacity', 'Drug release profile', 'Targeting ligands', 'Biocompatibility', 'Stability', 'In vitro drug release kinetics', 'In vivo biodistribution', 'Toxicity']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "PLGA-based nanoparticles: An overview of biomedical applications",
        "abstract": "Poly(lactic-co-glycolic acid) (PLGA) is one of the most successfully developed biodegradable polymers. Among the different polymers developed to formulate polymeric nanoparticles, PLGA has attracted considerable attention due to its attractive properties: (i) biodegradability and biocompatibility, (ii) FDA and European Medicine Agency approval in drug delivery systems for parenteral administration, (iii) well described formulations and methods of production adapted to various types of drugse.g.hydrophilic or hydrophobic small molecules or macromolecules, (iv) protection of drug from degradation, (v) possibility of sustained release, (vi) possibility to modify surface properties to provide stealthness and/or better interaction with biological materials and (vii) possibility to target nanoparticles to specific organs or cells. This review presents why PLGA has been chosen to design nanoparticles as drug delivery systems in various biomedical applications such as vaccination, cancer, inflammation and other diseases. This review focuses on the understanding of specific characteristics exploited by PLGA-based nanoparticles to target a specific organ or tissue or specific cells.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144256",
        "research_problem": "Nanoemulsions: formation, properties and applications",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Cargo', 'Type of nanocarrier']",
        "nechakhin_result": "['Nanoemulsion formation techniques', 'Properties of nanoemulsions', 'Applications of nanoemulsions', 'Nanoemulsion stability', 'Nanoemulsion characterization', 'Surfactants and emulsifiers used in nanoemulsions', 'Effects of formulation parameters on nanoemulsion properties', 'Methodologies for evaluating nanoemulsion performance']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Nanoemulsions: formation, properties and applications",
        "abstract": "Nanoemulsions are kinetically stable liquid-in-liquid dispersions with droplet sizes on the order of 100 nm. Their small size leads to useful properties such as high surface area per unit volume, robust stability, optically transparent appearance, and tunable rheology. Nanoemulsions are finding application in diverse areas such as drug delivery, food, cosmetics, pharmaceuticals, and material synthesis. Additionally, they serve as model systems to understand nanoscale colloidal dispersions. High and low energy methods are used to prepare nanoemulsions, including high pressure homogenization, ultrasonication, phase inversion temperature and emulsion inversion point, as well as recently developed approaches such as bubble bursting method. In this review article, we summarize the major methods to prepare nanoemulsions, theories to predict droplet size, physical conditions and chemical additives which affect droplet stability, and recent applications."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144268",
        "research_problem": "Lipid micelles as drug carriers",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Cargo', 'Type of nanocarrier']",
        "nechakhin_result": "['Lipids composition',\n 'Micelle size and shape',\n 'Drug loading capacity',\n 'Drug release profile',\n 'Biocompatibility',\n 'In vivo stability',\n 'Targeted drug delivery',\n 'Cellular uptake mechanism']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "PEG–lipid micelles as drug carriers: physiochemical attributes, formulation principles and biological implication",
        "abstract": "PEG–lipid micelles, primarily conjugates of polyethylene glycol (PEG) and distearyl phosphatidylethanolamine (DSPE) or PEG–DSPE, have emerged as promising drug-delivery carriers to address the shortcomings associated with new molecular entities with suboptimal biopharmaceutical attributes. The flexibility in PEG–DSPE design coupled with the simplicity of physical drug entrapment have distinguished PEG–lipid micelles as versatile and effective drug carriers for cancer therapy. They were shown to overcome several limitations of poorly soluble drugs such as non-specific biodistribution and targeting, lack of water solubility and poor oral bioavailability. Therefore, considerable efforts have been made to exploit the full potential of these delivery systems; to entrap poorly soluble drugs and target pathological sites both passively through the enhanced permeability and retention (EPR) effect and actively by linking the terminal PEG groups with targeting ligands, which were shown to increase delivery efficiency and tissue specificity. This article reviews the current state of PEG–lipid micelles as delivery carriers for poorly soluble drugs, their biological implications and recent developments in exploring their active targeting potential. In addition, this review sheds light on the physical properties of PEG–lipid micelles and their relevance to the inherent advantages and applications of PEG–lipid micelles for drug delivery.KeywordsActive targetingcancercytotoxicitydrug deliverydrug resistancemicellesnanotechnologyPEG–lipidpoorly solublesustained releaseDeclaration of interestThe authors report no conflicts of interest.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144328",
        "research_problem": "Liposomes as drug carriers",
        "orkg_properties": "['Composition', 'research problem', 'Nanoparticles preparation method', 'Type of nanocarrier']",
        "nechakhin_result": "['Drug delivery systems', 'Lipid-based drug delivery', 'Nanotechnology', 'Biomaterials', 'Pharmaceutical science', 'Pharmacokinetics', 'Formulation development', 'Drug encapsulation', 'Biocompatibility', 'Drug release kinetics', 'Targeted drug delivery', 'Stability of liposomes']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Preparation, Biodistribution and Neurotoxicity of Liposomal Cisplatin following Convection Enhanced Delivery in Normal and F98 Glioma Bearing Rats",
        "abstract": "The purpose of this study was to evaluate two novel liposomal formulations of cisplatin as potential therapeutic agents for treatment of the F98 rat glioma. The first was a commercially produced agent, Lipoplatin™, which currently is in a Phase III clinical trial for treatment of non-small cell lung cancer (NSCLC). The second, produced in our laboratory, was based on the ability of cisplatin to form coordination complexes with lipid cholesteryl hemisuccinate (CHEMS). Thein vitrotumoricidal activity of the former previously has been described in detail by other investigators. The CHEMS liposomal formulation had a Pt loading efficiency of 25% and showed more potentin vitrocytotoxicity against F98 glioma cells than free cisplatin at 24 h.In vivoCHEMS liposomes showed high retention at 24 h after intracerebral (i.c.) convection enhanced delivery (CED) to F98 glioma bearing rats. Neurotoxicologic studies were carried out in non-tumor bearing Fischer rats following i.c. CED of Lipoplatin™ or CHEMS liposomes or their “hollow” counterparts. Unexpectedly, Lipoplatin™ was highly neurotoxic when given i.c. by CED and resulted in death immediately following or within a few days after administration. Similarly “hollow” Lipoplatin™ liposomes showed similar neurotoxicity indicating that this was due to the liposomes themselves rather than the cisplatin. This was particularly surprising since Lipoplatin™ has been well tolerated when administered intravenously. In contrast, CHEMS liposomes and their “hollow” counterparts were clinically well tolerated. However, a variety of dose dependent neuropathologic changes from none to severe were seen at either 10 or 14 d following their administration. These findings suggest that further refinements in the design and formulation of cisplatin containing liposomes will be required before they can be administered i.c. by CED for the treatment of brain tumors and that a formulation that may be safe when given systemically may be highly neurotoxic when administered directly into the brain."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144336",
        "research_problem": "Solid lipid nanoparticles as drug carriers",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Type of nanocarrier']",
        "nechakhin_result": "['Nanoparticle composition', 'Drug encapsulation method', 'Drug release kinetics', 'Particle size', 'Surface charge', 'Biocompatibility', 'Drug loading capacity', 'Drug stability', 'In vitro drug release profiles', 'In vivo performance', 'Targeting ability', 'Drug efficacy', 'Toxicity', 'Biodegradability']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Solid Lipid Nanoparticles: Emerging Colloidal Nano Drug Delivery Systems",
        "abstract": "Solid lipid nanoparticles (SLNs) are nanocarriers developed as substitute colloidal drug delivery systems parallel to liposomes, lipid emulsions, polymeric nanoparticles, and so forth. Owing to their unique size dependent properties and ability to incorporate drugs, SLNs present an opportunity to build up new therapeutic prototypes for drug delivery and targeting. SLNs hold great potential for attaining the goal of targeted and controlled drug delivery, which currently draws the interest of researchers worldwide. The present review sheds light on different aspects of SLNs including fabrication and characterization techniques, formulation variables, routes of administration, surface modifications, toxicity, and biomedical applications."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144341",
        "research_problem": "Lipid nanocapsules as drug carriers",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Type of nanocarrier']",
        "nechakhin_result": "['Lipid composition', 'Nanocapsule size', 'Drug encapsulation efficiency', 'Surface charge', 'Drug release kinetics', 'Biocompatibility', 'Targeting ligands', 'Stability', 'Overcoming biological barriers', 'Drug loading capacity', 'Administration route', 'In vivo biodistribution', 'Toxicity', 'Pharmacokinetics', 'Therapeutic efficacy']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Lipid nanocapsules: A new platform for nanomedicine",
        "abstract": "Nanomedicine, an emerging new field created by the fusion of nanotechnology and medicine, is one of the most promising pathways for the development of effective targeted therapies with oncology being the earlier and the most notable beneficiary to date. Indeed, drug-loaded nanoparticles provide an ideal solution to overcome the low selectivity of the anticancer drugs towards the cancer cells in regards to normal cells and the induced severe side-effects, thanks to their passive and/or active targeting to cancer tissues. Liposome-based systems encapsulating drugs are already used in some cancer therapies (e.g. Myocet, Daunoxome, Doxil). But liposomes have some important drawbacks: they have a low capacity to encapsulate lipophilic drugs (even though it exists), they are manufactured through processes involving organic solvents, and they are leaky, unstable in biological fluids and more generally in aqueous solutions for being commercialized as such. We have developed new nano-cargos, the lipid nanocapsules, with sizes below the endothelium fenestration (ϕ<100nm), that solve these disadvantages. They are prepared according to a solvent-free process and they are stable for at least one year in suspension ready for injection, which should reduce considerably the cost and convenience for treatment. Moreover, these new nano-cargos have the ability to encapsulate efficiently lipophilic drugs, offering a pharmaceutical solution for their intravenous administration.The lipid nanocapsules (LNCs) have been prepared according to an original method based on a phase-inversion temperature process recently developed and patented. Their structure is a hybrid between polymeric nanocapsules and liposomes because of their oily core which is surrounded by a tensioactive rigid membrane. They have a lipoprotein-like structure. Their size can be adjusted below 100nm with a narrow distribution. Importantly, these properties confer great stability to the structure (physical stability>18 months). Blank or drug-loaded LNCs can be prepared, with or without PEG (polyethyleneglycol)ylation that is a key parameter that affects the vascular residence time of the nano-cargos. Other hydrophilic tails can also be grafted. Different anticancer drugs (paclitaxel, docetaxel, etoposide, hydroxytamoxifen, doxorubicin, etc.) have been encapsulated. They all are released according to a sustained pattern. Preclinical studies on cell cultures and animal models of tumors have been performed, showing promising results.\nNanomedicine, an emerging new field created by the fusion of nanotechnology and medicine, is one of the most promising pathways for the development of effective targeted therapies with oncology being the earlier and the most notable beneficiary to date. Indeed, drug-loaded nanoparticles provide an ideal solution to overcome the low selectivity of the anticancer drugs towards the cancer cells in regards to normal cells and the induced severe side-effects, thanks to their passive and/or active targeting to cancer tissues. Liposome-based systems encapsulating drugs are already used in some cancer therapies (e.g. Myocet, Daunoxome, Doxil). But liposomes have some important drawbacks: they have a low capacity to encapsulate lipophilic drugs (even though it exists), they are manufactured through processes involving organic solvents, and they are leaky, unstable in biological fluids and more generally in aqueous solutions for being commercialized as such. We have developed n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144346",
        "research_problem": "Nano-lipid carrier for drug delivery",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Type of nanocarrier']",
        "nechakhin_result": "['nanotechnology',\n 'lipid carrier',\n 'drug delivery',\n 'nanoparticles',\n 'nanomedicine',\n 'drug release',\n 'targeted therapy',\n 'encapsulation',\n 'biocompatibility',\n 'pharmacokinetics',\n 'toxicity',\n 'efficiency']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Recent strategies and advances in the fabrication of nano lipid carriers and their application towards brain targeting",
        "abstract": "In last two decades, the lipidnanocarriershave been extensively investigated for theirdrug targetingefficiency towards the critical areas of the human body like CNS, cardiac region, tumor cells,etc.Owing to the flexibility andbiocompatibility, the lipid-based nanocarriers, includingnanoemulsion,liposomes, SLN, NLCetc.have gained much attention among various other nanocarrier systems for brain targeting of bioactives. Across different lipid nanocarriers, NLC remains to be the safest, stable, biocompatible and cost-effective drug carrier system with high encapsulation efficiency. Drug delivery to the brain always remains a challenging issue for scientists due to the complex structure and various barrier mechanisms surrounding the brain. The application of a suitable nanocarrier system and the use of any alternative route of drug administration like nose-to-brain drug delivery could overcome the hurdle and improves the therapeutic efficiency of CNS acting drugs thereof. NLC, a second-generation lipid nanocarrier, upsurges the drug permeation across the BBB due to its unique structural properties. The biocompatible lipid matrix and nano-size make it an ideal drug carrier for brain targeting. It offers many advantages over other drug carrier systems, including ease of manufacturing and scale-up to industrial level, higher drug targeting, high drug loading, control drug release, compatibility with a wide range of drug substances, non-toxic and non-irritant behavior. This review highlights recent progresses towards the development of NLC for brain targeting of bioactives with particular reference to its surface modifications, formulations aspects,pharmacokineticbehavior and efficacy towards the treatment of variousneurological disorderslike AD, PD, schizophrenia, epilepsy, brain cancer,CNS infection(viral and fungal),multiple sclerosis,cerebral ischemia, andcerebral malaria. This work describes in detail the role and application of NLC, along with its different fabrication techniques and associated limitations. Specific emphasis is given to compile a summary and graphical data on the area explored by scientists and researchers worldwide towards the treatment of neurological disorders with or without NLC. The article also highlights a brief insight into two prime approaches for brain targeting, including drug delivery across BBB and direct nose-to-brain drug delivery along with the current global status of specific neurological disorders."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144353",
        "research_problem": "Extracellular vesicles as drug carrier",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Cargo', 'Type of nanocarrier']",
        "nechakhin_result": "['Extracellular vesicles', 'Drug delivery', 'Nanoparticles', 'Biological membranes', 'Targeted drug delivery', 'Therapeutic cargo', 'Cellular communication', 'Drug release kinetics', 'Biocompatibility', 'Drug encapsulation', 'In vivo studies', 'Cellular uptake', 'Drug loading efficiency']",
        "nechakhin_mappings": 3,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Exosome-based nanocarriers as bio-inspired and versatile vehicles for drug delivery: recent advances and challenges",
        "abstract": "Recent decades have witnessed the fast and impressive development of nanocarriers as a drug delivery system. Considering the safety, delivery efficiency and stability of nanocarriers, there are many obstacles in accomplishing successful clinical translation of these nanocarrier-based drug delivery systems. The gap has urged drug delivery scientists to develop innovative nanocarriers with high compatibility, stability and longer circulation time. Exosomes are nanometer-sized, lipid-bilayer-enclosed extracellular vesicles secreted by many types of cells. Exosomes serving as versatile drug vehicles have attracted increasing attention due to their inherent ability of shuttling proteins, lipids and genes among cells and their natural affinity to target cells. Attractive features of exosomes, such as nanoscopic size, low immunogenicity, high biocompatibility, encapsulation of various cargoes and the ability to overcome biological barriers, distinguish them from other nanocarriers. To date, exosome-based nanocarriers delivering small molecule drugs as well as bioactive macromolecules have been developed for the treatment of many prevalent and obstinate diseases including cancer, CNS disorders and some other degenerative diseases. Exosome-based nanocarriers have a huge prospect in overcoming many hindrances encountered in drug and gene delivery. This review highlights the advances as well as challenges of exosome-based nanocarriers as drug vehicles. Special focus has been placed on the advantages of exosomes in delivering various cargoes and in treating obstinate diseases, aiming to offer new insights for exploring exosomes in the field of drug delivery."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139645",
        "research_problem": "Search for the sequestration and preservation of organic material on Mars",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Mars', 'Sequestration', 'Preservation', 'Organic material']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Clay minerals in delta deposits and organic preservation potential on Mars",
        "abstract": "Clay-rich sedimentary deposits are often sites of organic matter preservation1,2, and have therefore been sought in Mars exploration3. However, regional deposits of hydrous minerals, including phyllosilicates and sulphates4,5, are not typically associated with valley networks and layered sediments that provide geomorphic evidence of surface water transport on early Mars6,7,8. The Compact Reconnaissance Imaging Spectrometer for Mars (CRISM)9has recently identified phyllosilicates10within three lake basins with fans or deltas that indicate sustained sediment deposition: Eberswalde crater7,11,12, Holden crater12,13and Jezero crater14. Here we use high-resolution data from the Mars Reconnaissance Orbiter (MRO) to identify clay-rich fluvial–lacustrine sediments within Jezero crater, which has a diameter of 45 km. The crater is an open lake basin on Mars with sedimentary deposits of hydrous minerals sourced from a smectite-rich catchment in the Nili Fossae region. We find that the two deltas and the lowest observed stratigraphic layer within the crater host iron–magnesium smectite clay. Jezero crater holds sediments that record multiple episodes of aqueous activity on early Mars. We suggest that this depositional setting and the smectite mineralogy make these deltaic deposits well suited for the sequestration and preservation of organic material."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139649",
        "research_problem": "Search for hydrated silicate minerals on Mars using orbital remote sensing",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Mars', 'hydrated silicate minerals', 'orbital remote sensing']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Hydrated silicate minerals on Mars observed by the Mars Reconnaissance Orbiter CRISM instrument",
        "abstract": "Phyllosilicates, a class of hydrous mineral first definitively identified on Mars by the OMEGA (Observatoire pour la Mineralogie, L’Eau, les Glaces et l’Activitié) instrument1,2, preserve a record of the interaction of water with rocks on Mars. Global mapping showed that phyllosilicates are widespread but are apparently restricted to ancient terrains and a relatively narrow range of mineralogy (Fe/Mg and Al smectite clays). This was interpreted to indicate that phyllosilicate formation occurred during the Noachian (the earliest geological era of Mars), and that the conditions necessary for phyllosilicate formation (moderate to high pH and high water activity3) were specific to surface environments during the earliest era of Mars’s history4. Here we report results from the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM)4of phyllosilicate-rich regions. We expand the diversity of phyllosilicate mineralogy with the identification of kaolinite, chlorite and illite or muscovite, and a new class of hydrated silicate (hydrated silica). We observe diverse Fe/Mg-OH phyllosilicates and find that smectites such as nontronite and saponite are the most common, but chlorites are also present in some locations. Stratigraphic relationships in the Nili Fossae region show olivine-rich materials overlying phyllosilicate-bearing units, indicating the cessation of aqueous alteration before emplacement of the olivine-bearing unit. Hundreds of detections of Fe/Mg phyllosilicate in rims, ejecta and central peaks of craters in the southern highland Noachian cratered terrain indicate excavation of altered crust from depth. We also find phyllosilicate in sedimentary deposits clearly laid by water. These results point to a rich diversity of Noachian environments conducive to habitability.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139653",
        "research_problem": "To estimate mineralogical diversity in Mawrth Vallis region on Mars",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Mawrth Vallis region', 'Mars', 'mineralogical diversity']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Phyllosilicate Diversity and Past Aqueous Activity Revealed at Mawrth Vallis, Mars",
        "abstract": "Observations by the Mars Reconnaissance Orbiter/Compact Reconnaissance Imaging Spectrometer for Mars in the Mawrth Vallis region show several phyllosilicate species, indicating a wide range of past aqueous activity. Iron/magnesium (Fe/Mg)–smectite is observed in light-toned outcrops that probably formed via aqueous alteration of basalt of the ancient cratered terrain. This unit is overlain by rocks rich in hydrated silica, montmorillonite, and kaolinite that may have formed via subsequent leaching of Fe and Mg through extended aqueous events or a change in aqueous chemistry. A spectral feature attributed to an Fe2+phase is present in many locations in the Mawrth Vallis region at the transition from Fe/Mg-smectite to aluminum/silicon (Al/Si)–rich units. Fe2+-bearing materials in terrestrial sediments are typically associated with microorganisms or changes in pH or cations and could be explained here by hydrothermal activity. The stratigraphy of Fe/Mg-smectite overlain by a ferrous phase, hydrated silica, and then Al-phyllosilicates implies a complex aqueous history.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139657",
        "research_problem": "Search for carbonates on Mars despite the acidic weathering",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['carbonates', 'Mars', 'acidic weathering']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Orbital Identification of Carbonate-Bearing Rocks on Mars",
        "abstract": "Geochemical models for Mars predict carbonate formation during aqueous alteration. Carbonate-bearing rocks had not previously been detected on Mars' surface, but Mars Reconnaissance Orbiter mapping reveals a regional rock layer with near-infrared spectral characteristics that are consistent with the presence of magnesium carbonate in the Nili Fossae region. The carbonate is closely associated with both phyllosilicate-bearing and olivine-rich rock units and probably formed during the Noachian or early Hesperian era from the alteration of olivine by either hydrothermal fluids or near-surface water. The presence of carbonate as well as accompanying clays suggests that waters were neutral to alkaline at the time of its formation and that acidic weathering, proposed to be characteristic of Hesperian Mars, did not destroy these carbonates and thus did not dominate all aqueous environments.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139661",
        "research_problem": "To determine the mineralogy of Gale crater on Mars",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Mineral composition', 'Gale crater', 'Mars', 'Martian mineralogy', 'Remote sensing', 'Geological composition', 'Chemical analysis', 'Spectroscopy', 'Mineral identification']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Mineralogy of the MSL Curiosity landing site in Gale crater as observed by MRO/CRISM",
        "abstract": "Orbital data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) and High Resolution Imaging Science Experiment instruments on the Mars Reconnaissance Orbiter (MRO) provide a synoptic view of compositional stratigraphy on the floor of Gale crater surrounding the area where the Mars Science Laboratory (MSL) Curiosity landed. Fractured, light-toned material exhibits a 2.2 µm absorption consistent with enrichment in hydroxylated silica. This material may be distal sediment from the Peace Vallis fan, with cement and fracture fill containing the silica. This unit is overlain by more basaltic material, which has 1 µm and 2 µm absorptions due to pyroxene that are typical of Martian basaltic materials. Both materials are partially obscured by aeolian dust and basaltic sand. Dunes to the southeast exhibit differences in mafic mineral signatures, with barchan dunes enhanced in olivine relative to pyroxene-containing longitudinal dunes. This compositional difference may be related to aeolian grain sorting."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139664",
        "research_problem": "Estimation of seasonal impact on Martian gully formation from spectroscopic studies",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Martian gully formation', 'spectroscopic studies', 'estimation', 'seasonal impact']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "New insights into gully formation on Mars: Constraints from composition as seen by MRO/CRISM",
        "abstract": "Over 100 Martian gully sites were analyzed using orbital data collected by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) and High Resolution Imaging Science Experiment on the Mars Reconnaissance Orbiter (MRO). Most gullies are spectrally indistinct from their surroundings, due to mantling by dust. Where spectral information on gully sediments was obtained, a variety of mineralogies were identified. Their relationship to the source rock suggests that gully-forming processes transported underlying material downslope. There is no evidence for specific compositions being more likely to be associated with gullies or with the formation of hydrated minerals in situ as a result of recent liquid water activity. Seasonal CO2and H2O frosts were observed in gullies at middle to high latitudes, consistent with seasonal frost-driven processes playing important roles in the evolution of gullies. Our results do not clearly indicate a role for long-lived liquid water in gully formation and evolution."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139667",
        "research_problem": "Analysis of spectroscopic data for Mud volcanism on Mars",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['spectroscopic technique used', 'type of mud volcanism', 'analysis parameters', 'features observed in the spectral data', 'location on Mars', 'time period of data collection', 'research methodology']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Insights into Mars mud volcanism using visible and near-infrared spectroscopy",
        "abstract": "Mudvolcanism(MV) has been a proposed formation mechanism for positive-relieflandformsin thelowland, equatorial, andhighland regionsof Mars. While visible and near-infrared (VNIR) spectroscopy has been used in a few cases to argue for the presence of MV on the surface of Mars, data from the CompactReconnaissanceImaging Spectrometer for Mars (CRISM) remain underutilized. We conducted a global examination of proposed Mars MV features using CRISM VNIR data. We observe variable hydration states and place constraints on the composition of these features from orbit. We do not confidently identifyphyllosilicates, carbonates, or sulfates associated with suggested Martianmud volcanoes. However, specific structures in Valles Marineris exhibitVNIR signaturesconsistent with unaltered hydrated glass of a volcanic origin and high-Capyroxene. CRISM visible data from MV features reveal consistent nanophaseferric oxidesignatures on a global scale, although these signatures are not unique to Mars MV materials. Limitations in specific mineral detection are likely due to the fine grain size and/or textural characteristics of putative MV features. While we do not argue in favor of a specific proposed MV site in the context of future robotic or human missions, the insights of this study could be used as a guide for Mars surface exploration.\nMud volcanism has been proposed to explain the formation of morphologically diverse edifices across the surface of Mars. Previous global compositional analysis of surface features on Mars, such as pitted cones and knobs, interpreted as mud volcanoes using Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) targeted data revealed consistent water hydration signatures associated with these landforms (Dapremont and Wray, 2021b). However, no definitive identifications of specific minerals known to comprise mud volcanism materials on Earth (e.g., phyllosilicates, carbonates, sulfates) were identified. Here, we take advantage of an alternative methodology involving a quantitative color band ratio technique using blue-green (BG), red (RED), and infrared (IR) reflectance data sourced from the High Resolution Imaging Science Experiment (HiRISE) camera to determine if any additional mineralogical information about proposed Martian mud volcanoes could be acquired. Across Mars in the three distinct study regions of Valles Marineris, Terra Sirenum, and Arabia Terra, proposed mud materials consistently plot with ferric laboratory and Mars surface reference minerals. In Terra Sirenum, HiRISE color data point clusters acquired from mounds attributed to mud volcanism plot overtop the Fe-smectite reference mineral nontronite. We show that HiRISE color data can be used as a complementary mineralogical investigation tool applied to the study of surface materials on Mars and demonstrate the value of HiRISE color ratios for comparative planetology.\nHighlights•Proposed mud volcanism materials on Mars exhibit variable hydration states.•Valles Marineris mud features exhibit unaltered hydrated glass and HCP signatures.•Consistent nanophase ferric oxide signatures are not unique to Mars mud volcanoes•Mars mud volcanism mineral detection limitations include grain size and/or texture.\nHighlights•Proposed mud volcanism materials on Mars exhibit variable hydration states.•Valles Marineris mud features exhibit unaltered hydrated glass and HCP signatures.•Consistent nanophase ferric oxide signatures are not unique to Mars mud volcanoes•Mars mud v"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140690",
        "research_problem": "Integration of hyperspectral and multispectral thermal imagery for geological mapping at Cuprite, Nevada",
        "orkg_properties": "['Data used', 'research problem', 'Analysis', 'Processing']",
        "nechakhin_result": "['Imagery type (hyperspectral, multispectral thermal)',\n 'Geological mapping',\n 'Location (Cuprite, Nevada)']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Integrating visible, near-infrared and short-wave infrared hyperspectral and multispectral thermal imagery for geological mapping at Cuprite, Nevada",
        "abstract": "This study investigated the potential value of integrating hyperspectral visible, near-infrared, and short-wave infrared imagery with multispectral thermal data for geological mapping. Two coregistered aerial data sets of Cuprite, Nevada were used: Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) hyperspectral data, and MODIS/ASTER Airborne Simulator (MASTER) multispectral thermal data. Four classification methods were each applied to AVIRIS, MASTER, and a combined set. Confusion matrices were used to assess the classification accuracy. The assessment showed, in terms of kappa coefficient, that most classification methods applied to the combined data achieved a marked improvement compared to the results using either AVIRIS or MASTER thermal infrared (TIR) data alone. Spectral angle mapper (SAM) showed the best overall classification performance. Minimum distance classification had the second best accuracy, followed by spectral feature fitting (SFF) and maximum likelihood classification. The results of the study showed that SFF applied to the combination of AVIRIS with MASTER TIR data are especially valuable for identification of silicified alteration and quartzite, both of which exhibit distinctive features in the TIR region. SAM showed some advantages over SFF in dealing with multispectral TIR data, obtaining higher accuracy in discriminating low albedo volcanic rocks and limestone which do not have unique, distinguishing features in the TIR region.\nThe spectral angle mapper (SAM), as a spectral matching method, has been widely used in lithological type identification and mapping using hyperspectral data. The SAM quantifies the spectral similarity between an image pixel spectrum and a reference spectrum with known components. In most existing studies a mean reflectance spectrum has been used as the reference spectrum for a specific lithological class. However, this conventional use of SAM does not take into account the spectral variability, which is an inherent property of many rocks and is further magnified in remote sensing data acquisition process. In this study, two methods of determining reference spectra used in SAM are proposed for the improved lithological mapping. In first method the mean of spectral derivatives was combined with the mean of original spectra, i.e., the mean spectrum and the mean spectral derivative were jointly used in SAM classification, to improve the class separability. The second method is the use of multiple reference spectra in SAM to accommodate the spectral variability. The proposed methods were evaluated in lithological mapping using EO-1 Hyperion hyperspectral data of two arid areas. The spectral variability and separability of the rock types under investigation were also examined and compared using spectral data alone and using both spectral data and first derivatives. The experimental results indicated that spectral variability significantly affected the identification of lithological classes with the conventional SAM method using a mean reference spectrum. The proposed methods achieved significant improvement in the accuracy of lithological mapping, outperforming the conventional use of SAM with a mean spectrum as the reference spectrum, and the matching filtering, a widely used spectral mapping method.\nThis study investigated the potential value of integrating hyperspectral visible, near-infrared, and short-wave infrared imagery with multispectral thermal data for geological mapping. Two c"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140694",
        "research_problem": "Mineralogy of Cuprite hills in Nevada of Unitd states from airborne and spaceborne imageries",
        "orkg_properties": "['research problem', 'Analysis', 'Processing']",
        "nechakhin_result": "['Mineral composition', 'Cuprite hills', 'Nevada', 'United States', 'Airborne imagery', 'Spaceborne imagery']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 3,
        "title": "Comparison of airborne hyperspectral data and eo-1 hyperion for mineral mapping",
        "abstract": ":Airborne hyperspectral data have been available to researchers since the early 1980s and their use for geologic applications is well documented. The launch of the National Aeronautics and Space Administration Earth Observing 1 Hyperion sensor in November 2000 marked the establishment of a test bed for spaceborne hyperspectral capabilities. Hyperion covers the 0.4-2.5-/spl mu/m range with 242 spectral bands at approximately 10-nm spectral resolution and 30-m spatial resolution. Analytical Imaging and Geophysics LLC and the Commonwealth Scientific and Industrial Research Organisation have been involved in efforts to evaluate, validate, and demonstrate Hyperions's utility for geologic mapping in a variety of sites in the United States and around the world. Initial results over several sites with established ground truth and years of airborne hyperspectral data show that Hyperion data from the shortwave infrared spectrometer can be used to produce useful geologic (mineralogic) information. Minerals mapped include carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite. Hyperion data collected under optimum conditions (summer season, bright targets, well-exposed geology) indicate that Hyperion data meet prelaunch specifications and allow subtle distinctions such as determining the difference between calcite and dolomite and mapping solid solution differences in micas caused by substitution in octahedral molecular sites. Comparison of airborne hyperspectral data [from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)] to the Hyperion data establishes that Hyperion provides similar basic mineralogic information, with the principal limitation being limited mapping of fine spectral detail under less-than-optimum acquisition conditions (winter season, dark targets) based on lower signal-to-noise ratios. Case histories demonstrate the analysis methodologies and level of information available from the Hyperion data...Show More"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140698",
        "research_problem": "Application of ASTER data for hydrothermal geological mapping at Cuprite, Nevada",
        "orkg_properties": "['Data used', 'research problem', 'Analysis', 'Processing']",
        "nechakhin_result": "['Remote sensing data sources',\n 'ASTER data',\n 'Hydrothermal geological mapping',\n 'Cuprite, Nevada',\n 'Geographical location',\n 'Geological features',\n 'Mineral exploration',\n 'Spectral bands',\n 'Spectral resolution',\n 'Spatial resolution',\n 'Temporal resolution',\n 'Data preprocessing techniques',\n 'Classification algorithms',\n 'Image enhancement techniques']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Mapping Hydrothermally Altered Rocks at Cuprite, Nevada, Using the Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), a New Satellite-Imaging System",
        "abstract": "The Advanced Spaceborne Thermal Emission and Reflection  Radiometer (ASTER) is a 14-band multispectral instrument on board the Earth  Observing System (EOS), TERRA. The three bands between 0.52 and 0.86μm and  the six bands from 1.60 and 2.43μm, which have 15- and 30-m spatial  resolution, respectively, were selected primarily for making remote  mineralogical determinations.The Cuprite, Nevada, mining district comprises two  hydrothermal alteration centers where Tertiary volcanic rocks have been  hydrothermally altered mainly to bleached silicified rocks and opalized  rocks, with a marginal zone of limonitic argillized rocks. Country rocks are  mainly Cambrian phyllitic siltstone and limestone.Evaluation of an ASTER image of the Cuprite district  shows that spectral reflectance differences in the nine bands in the 0.52 to  2.43μm region provide a basis for identifying and mapping mineralogical  components which characterize the main hydrothermal alteration zones: opal  is the spectrally dominant mineral in the silicified zone; whereas, alunite  and kaolinite are dominant in the opalized zone. In addition, the  distribution of unaltered country rocks was mapped because of the presence  of spectrally dominant muscovite in the siltstone and calcite in limestone,  and the tuffaceous rocks and playa deposits were distinguishable due to  their relatively flat spectra and weak absorption features at 2.33 and 2.20μm, respectively.An Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)  image of the study area was processed using a similar methodology used with  the ASTER data. Comparison of the ASTER and AVIRIS results shows that the  results are generally similar, but the higher spectral resolution of AVIRIS  (224 bands) permits identification of more individual minerals, including  certain polymorphs. However, ASTER has recorded images of more than 90  percent of the Earth’s  land surface with less than 20 percent cloud cover, and these data are  available at nominal or no cost. Landsat TM images have a similar spatial  resolution to ASTER images, but TM has fewer bands, which limits its  usefulness for making mineral determinations.\nArticle Navigation"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140706",
        "research_problem": "Lithologic discrimination and mapping at Nevada using ASTER multispectral data",
        "orkg_properties": "['Data used', 'research problem', 'Analysis']",
        "nechakhin_result": "['Lithologic composition',\n 'Mapping techniques',\n 'Nevada region',\n 'ASTER multispectral data']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "Spectral indices for lithologic discrimination and mapping by using the ASTER SWIR bands",
        "abstract": "The Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) is a research facility instrument launched on NASA's Terra spacecraft in December 1999. Spectral indices, a kind of orthogonal transformation in the five-dimensional space formed by the five ASTER short-wave-infrared (SWIR) bands, were proposed for discrimination and mapping of surface rock types. These include Alunite Index, Kaolinite Index, Calcite Index, and Montmorillonite Index, and can be calculated by linear combination of reflectance values of the five SWIR bands. The transform coefficients were determined so as to direct transform axes to the average spectral pattern of the typical minerals. The spectral indices were applied to the simulated ASTER dataset of Cuprite, Nevada, USA after converting its digital numbers to surface reflectance. The resultant spectral index images were useful for lithologic mapping and were easy to interpret geologically. An advantage of this method is that we can use the pre-determined transform coefficients, as long as image data are converted to surface reflectance.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140710",
        "research_problem": "Mineral mapping at Nevada using diagnostic absorption feature in SWIR wavelength range",
        "orkg_properties": "['Data used', 'research problem', 'Analysis', 'Processing', 'reference']",
        "nechakhin_result": "['geographic location', 'mineral mapping technique', 'SWIR wavelength range', 'absorption features', 'Nevada']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Simple mineral mapping algorithm based on multitype spectral diagnostic absorption features: a case study at Cuprite, Nevada",
        "abstract": "Hyperspectral remote sensing has been widely used in mineral identification using the particularly useful short-wave infrared (SWIR) wavelengths (1.0 to2.5μm2.5μm). Current mineral mapping methods are easily limited by the sensor’s radiometric sensitivity and atmospheric effects. Therefore, a simple mineral mapping algorithm (SMMA) based on the combined application with multitype diagnostic SWIR absorption features for hyperspectral data is proposed. A total of nine absorption features are calculated, respectively, from the airborne visible/infrared imaging spectrometer data, the Hyperion hyperspectral data, and the ground reference spectra data collected from the United States Geological Survey (USGS) spectral library. Based on spectral analysis and statistics, a mineral mapping decision-tree model for the Cuprite mining district in Nevada, USA, is constructed. Then, the SMMA algorithm is used to perform mineral mapping experiments. The mineral map from the USGS (USGS map) in the Cuprite area is selected for validation purposes. Results showed that the SMMA algorithm is able to identify most minerals with high coincidence with USGS map results. Compared with Hyperion data (overall accuracy=74.54%), AVIRIS data showed overall better mineral mapping results (overall accuracy=94.82%) due to low signal-to-noise ratio and high spatial resolution."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140808",
        "research_problem": "Hyperion data for mineralogcal analysis of Lonar crater in India",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Software', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Hyperion data', 'mineralogical analysis', 'Lonar crater', 'India']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Exploring the Mineralogy at Lonar Crater with Hyperspectral Remote Sensing",
        "abstract": "The earth crust is made up of variety of minerals. These minerals are having very significant applications in our day today life. The various studies, characterizing physical, chemical, electrical, structural properties, have been carried out on the Lonar crater for studying mineralogy, surface morphology and geology but has not been done by remote sensing technology. So, the proposed work focused on exploring the mineralogy at the Lonar crater by using high resolution hyperspectral imageries. The spectral reflectance of minerals was characterized by using FieldSpec4 spectroradiometer. The minerals at Lonar crater were explored by performing preprocessing and spectral analysis. The techniques used in the work are Spectral Angle Mapper and Spectral Feature Fitting. The results of the work marked the presence of pigeonite and augite at Lonar crater which indicates that this crater is the result of extrusive volcanic activity. Also, the presence of augite underneath basaltic igneous rocks as the rock type of Lonar crater. The salinity of the Lonar lake is proved by the presence of mirabilite and salt. Thus, the important results of this work are presence of minerals quartz, actinol, jarosite, pigeonite, augite, albite, mirabilite and scolecite. The significance of these minerals related to the crater is discussed here. It also validated the existence of these minerals which were identified through the previous geological, chemical, physical, electrical and magnetic studies and the minerals pyrite, chlorite, richter, illite, limonite, allanite, amphibolite and margarite were also identified."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140812",
        "research_problem": "Hyperion data for characterization of hematite ore mineral classes",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Hyperion data', 'characterization', 'hematite ore', 'mineral classes']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Characterization and mapping of hematite ore mineral classes using hyperspectral remote sensing technique: a case study from Bailadila iron ore mining region",
        "abstract": "The study demonstrates a methodology for mapping various hematite ore classes based on their reflectance and absorption spectra, using Hyperion satellite imagery. Substantial validation is carried out, using the spectral feature fitting technique, with the field spectra measured over the Bailadila hill range in Chhattisgarh State in India. The results of the study showed a good correlation between the concentration of iron oxide with the depth of the near-infrared absorption feature (R2= 0.843) and the width of the near-infrared absorption feature (R2= 0.812) through different empirical models, with a root-mean-square error (RMSE) between < 0.317 and < 0.409. The overall accuracy of the study is 88.2% with a Kappa coefficient value of 0.81. Geochemical analysis and X-ray fluorescence (XRF) of field ore samples are performed to ensure different classes of hematite ore minerals. Results showed a high content of Fe > 60 wt% in most of the hematite ore samples, except banded hematite quartzite (BHQ) (< 47 wt%).\nOpen AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visithttp://creativecommons.org/licenses/by/4.0/.Reprints and permissions\nThis research work illustrates the practicability of using Hyperion imagery for mapping various types of hematite ores in hilly and inaccessible locations. The finest processed image spectra assisted in identifying the prominent absorption features (750–1100 nm) related to iron oxide minerals. Matching between image and field spectra through spectral feature fitting technique showed the best correlation. Prominent spectral absorption features of iron ore at a particular wavelength, location, and geochemical analysis of iron ore samples showed a good association. The depth and width of the absorption features of spectra are increasing with the increasing amount of iron content. The depth of absorption is increasing significantly, even a minute increase in the concentration of iron content in the sample. SFF is a robust method to compare the similarity between field and image spectra of Hyperion data. Further, the classified map of Hyperion data resulted from the spectral angle mapper algorithm. RMSE values show the exceptional matching between image and field spectra which leads to an increase in the classification accuracy. The confusion matrix and the value of the Kappa coefficient show the accuracy of the classified output.\nThe study demonstrates a methodology for mapping various hematite ore classes based on their reflectance and absorption spectra, using Hyperion satellite imagery. Substantial validation is carried out, using the spectral feature fitting technique, with the field spectra measured over the Bailadila hill range in Chhattisgarh State in India. The results of t"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140815",
        "research_problem": "Hyperion data for for hydrothermal alteration and base metal minerals",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Software', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Data Source', 'Hyperion Satellite', 'Hydrothermal Alteration', 'Base Metal Minerals']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Mapping Regolith and Gossan for Mineral Exploration in the Eastern Kumaon Himalaya, India using hyperion data and object oriented image classification",
        "abstract": "Crystalline in the KumaonHimalaya, India near Askot area is a prominent site of the base metal mineralization and gossanised surface. This area is hosted by the sulphides and sulphates of Cu, Pb, Zn and Au and Ag mineralization with the altered rocks like sericite chlorite schist,gneissetc. Due to the deep weathering this area is also a good illustration site of the gossanised outcrop.Regolithmapping through the multispectral remotely sensed data using different parametric and nonparametric classification algorithm has been used for many years. In recent years, object oriented classification for classification of object rather than pixel has gained a good success within the geospatial community. On the other hand, space borne hyperspectralremote sensinghas gained a great success in identification of the minerals from space. The narrow contiguous bands of this hyperspectral remote sensing data can provide more information of the chemical content of the different minerals.In this study EO1 (Earth Observation) Hyperion hyperspectral sensor data has been evaluated for regolith andgossanmapping using the object orientedimage classificationtechnique. The efficacy of the hyperion data is evaluated in and around the Askot base metalmineral depositsforhydrothermal alterationand base metal minerals. Three sites were selected by regolith mapping using object oriented image classification method and were evaluated by the aid of hyperspectral data for alteration minerals. During the field verification it was found that the mineralization within these sites was associated with the dolomite, gneiss and schists. Stronghydrothermal activityand shearing were dominated at these sites. Minerals like carbonates, sulphates and sulphides of copper, lead, zinc and silver, magnetite along with the altered minerals like, mica, chlorite, talc etc. were found at the target sites. eCognition™ and ENVI® software’s were used for the object based classification and for the hyperspectral data processing, respectively. It was concluded from this study that object oriented classification is a suitable classification algorithm for regolith and gossan mapping as it uses the spatial, spectral and textural information of the remote sensing dataset. Target areas suggested by the object oriented classification method were examined by the hyperspectral data analysis. Results of the hyperspectral analysis were in good confirmation of the minerals found in the field as well as in the lab. Though the signal to noise ratio of the scene was low but still hyperion data was able to highlight themineralogyof the Askot and nearby areas. It was also concluded by this study that the spectral analysis was very helpful for identification of the gossan and limonite over the conventionalpetrographyanalysis.\nAirborne particulate matter (PM) has been a major threat to air quality and public health in major cities in China for more than a decade. Green space has been deemed to be effective in mitigating PM pollution; however, few studies have examined its effectiveness at the neighborhood scale. In this study, the authors probe the contributions from different landscape components in the green space (i.e., tree, grass), as well as the spatial scale of planning on fine PM (PM2.5) concentrations in urban neighborhoods. PM2.5data including 37 samples from five megacities were collected from the National Environmental Monitoring Centre in China. Results showed that, neighborhood green space greatly "
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140819",
        "research_problem": "Limestone mineral identification using Hyperion imagery",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Software', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Mineralogy', 'Limestone', 'Hyperion imagery', 'Identification']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Enhancement of limestone mineral identification using Hyperion imagery: a case study from Tirunelveli District, Tamil Nadu, South India",
        "abstract": "Hyperspectral remote sensing consolidates imaging and spectroscopy in a solitary system which frequently comprises big datasets and necessitates the novel processing methods. In the present study, Cheranmadevi Block of Tirunelveli District in Tamil Nadu is selected to extract the abundant limestone mineral. Hyperion is one of the freely available hyperspectral imagery containing 242 spectral bands with 10-nm intervals in the wavelength between 400 and 2500 nm. The main objectives of the present research work are to enhance the imagery visualization, end member extraction, and classification, and estimate the abundant limestone quantity by removing the striping error in Hyperion imagery. The scanning electron microscope with energy-dispersive X-ray spectroscopy analysis is performed to identify the chemical composition of limestone mineral. The spectral reflectance of limestone is characterized using analytical spectral devices like a field spectroradiometer. Limestone has deep absorption in the short-wave infrared region (1900–2500 nm) around 2320–2340 nm due to their calcite composition (CaCO3). The feature extraction in Hyperion data is performed using various preprocessing steps like bad bands removal, vertical strip removal, and radiance and reflectance creation. To improve the classification accuracy, vertical strip removal process is performed using a local destriping algorithm. The absolute reflectance is achieved by the atmospheric correction module using Fast Line-of-sight Atmospheric Analysis of Hypercubes. The acquired reflectance image spectra are compared with the spectral libraries of USGS, JPL, and field spectra. Destriping enhances qualities of Hyperion data interims of the spectral profile, radiance, reflectance, and data reduction methods. The present research work focused on the local destriping algorithm to increase the quality and quantity of limestone deposit extraction.\nAdam E, Mutanga O, Rugege D (2010) Multispectral and hyperspectral remote sensing for identification and mapping of wetland vegetation: a review. Wetl Ecol Manag 18(3):281–296"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140823",
        "research_problem": "Mapping the spatial distribution of altered minerals in rocks and soils in the Gadag Schist Belt (GSB) using Hyperion data",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Spatial distribution', 'Altered minerals', 'Rocks', 'Soils', 'Gadag Schist Belt', 'Hyperion data']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Spatial distribution of altered minerals in the Gadag Schist Belt (GSB) of Karnataka, Southern India using hyperspectral remote sensing data",
        "abstract": "Spatial distribution of altered minerals in rocks and soils in the Gadag Schist Belt (GSB) is carried out using Hyperion data of March 2013. The entire spectral range is processed with emphasis on VNIR (0.4–1.0 μm) and SWIR regions (2.0–2.4 μm). Processing methodology includes Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes correction, minimum noise fraction transformation, spectral feature fitting (SFF) and spectral angle mapper (SAM) in conjunction with spectra collected, using an analytical spectral device spectroradiometer. A total of 155 bands were analysed to identify and map the major altered minerals by studying the absorption bands between the 0.4–1.0-μm and 2.0–2.3-μm wavelength regions. The most important and diagnostic spectral absorption features occur at 0.6–0.7 μm, 0.86 and at 0.9 μm in the VNIR region due to charge transfer of crystal field effect in the transition elements, whereas absorption near 2.1, 2.2, 2.25 and 2.33 μm in the SWIR region is related to the bending and stretching of the bonds in hydrous minerals (Al-OH, Fe-OH and Mg-OH), particularly in clay minerals. SAM and SFF techniques are implemented to identify the minerals present. A score of 0.33–1 was assigned for both SAM and SFF, where a value of 1 indicates the exact mineral type. However, endmember spectra were compared with United States Geological Survey and John Hopkins University spectral libraries for minerals and soils. Five minerals, i.e. kaolinite-5, kaolinite-2, muscovite, haematite, kaosmec and one soil, i.e. greyish brown loam have been identified. Greyish brown loam and kaosmec have been mapped as the major weathering/altered products present in soils and rocks of the GSB. This was followed by haematite and kaolinite. The SAM classifier was then applied on a Hyperion image to produce a mineral map. The dominant lithology of the area included greywacke, argillite and granite gneiss."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140827",
        "research_problem": "Sub-pixel mineral investigation using Hyperion data in Rajasthan, India",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Software', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Mineral type', 'Spectral reflectance', 'Hyperion data', 'Sub-pixel analysis', 'Rajasthan, India']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Sub-pixel mineral mapping using EO-1 Hyperion hyperspectral data",
        "abstract": ".This study describes the utility of Earth Observation (EO)-1 Hyperion data for sub-pixel mineral investigation using Mixture Tuned Target Constrained Interference Minimized Filter (MTTCIMF) algorithm in hostile mountainous terrain of Rajsamand district of Rajasthan, which hosts economic mineralization such as lead, zinc, and copper etc. The study encompasses pre-processing, data reduction, Pixel Purity Index (PPI) and endmember extraction from reflectance image of surface minerals such as illite, montmorillonite, phlogopite, dolomite and chlorite. These endmembers were then assessed with USGS mineral spectral library and lab spectra of rock samples collected from field for spectral inspection. Subsequently, MTTCIMF algorithm was implemented on processed image to obtain mineral distribution map of each detected mineral. A virtual verification method has been adopted to evaluate the classified image, which uses directly image information to evaluate the result and confirm the overall accuracy and kappa coefficient of 68 % and 0.6 respectively. The sub-pixel level mineral information with reasonable accuracy could be a valuable guide to geological and exploration community for expensive ground and/or lab experiments to discover economic deposits. Thus, the study demonstrates the feasibility of Hyperion data for sub-pixel mineral mapping using MTTCIMF algorithm with cost and time effective approach.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143607",
        "research_problem": "Applications of Hyperspectral remote sensing for mineral exploartion",
        "orkg_properties": "['research problem', 'Preprocessing', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['Hyperspectral remote sensing techniques', 'Mineral exploration', 'Spectral signatures', 'Mineral mapping', 'Geological features', 'Spectral libraries', 'Image classification', 'Data preprocessing', 'Spectral unmixing', 'Feature extraction', 'Machine learning algorithms', 'Image fusion', 'Data fusion', 'Data interpretation', 'Accuracy assessment']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Multi- and hyperspectral geologic remote sensing: A review",
        "abstract": "Geologists have used remote sensing data since the advent of the technology for regional mapping, structural interpretation and to aid in prospecting for ores and hydrocarbons. This paper provides a review of multispectral and hyperspectral remote sensing data, products and applications in geology. During the early days of Landsat Multispectral scanner and Thematic Mapper, geologists developed band ratio techniques and selective principal component analysis to produce iron oxide and hydroxyl images that could be related to hydrothermal alteration. The advent of the Advanced Spaceborne Thermal Emission and Reflectance Radiometer (ASTER) with six channels in the shortwave infrared and five channels in the thermal region allowed to produce qualitative surface mineral maps of clay minerals (kaolinite, illite), sulfate minerals (alunite), carbonate minerals (calcite, dolomite), iron oxides (hematite, goethite), and silica (quartz) which allowed to map alteration facies (propylitic, argillic etc.). The step toward quantitative and validated (subpixel) surface mineralogic mapping was made with the advent of high spectral resolution hyperspectral remote sensing. This led to a wealth of techniques to match image pixel spectra to library and field spectra and to unravel mixed pixel spectra to pure endmember spectra to derive subpixel surface compositional information. These products have found their way to the mining industry and are to a lesser extent taken up by the oil and gas sector. The main threat for geologic remote sensing lies in the lack of (satellite) data continuity. There is however a unique opportunity to develop standardized protocols leading to validated and reproducible products from satellite remote sensing for the geology community. By focusing on geologic mapping products such as mineral and lithologic maps,geochemistry, P-T paths, fluid pathways etc. the geologic remote sensing community can bridge the gap with the geosciences community. Increasingly workflows should be multidisciplinary and remote sensing data should be integrated with field observations and subsurface geophysical data to monitor and understand geologic processes.Highlights► Multispectral RS allows geologists a qualitative assessment of surface composition. ► Hyperspectralremote sensingallows mapping surfacemineralogy. ► Data integration (hyperspectral,geophysics, geochemistry) reveals proxies to P-T trajectories and fluid pathways in alteration systems. ► Standards and protocols are lacking in geologic remote sensing. ► Data continuity is essential in long-term monitoring ofgeological processes.\nKnowledge of tree species distribution is important worldwide for sustainable forest management and resource evaluation. The accuracy and information content of species maps produced using remote sensing images vary with scale, sensor (optical, microwave, LiDAR), classification algorithm, verification design and natural conditions like tree age, forest structure and density. Imaging spectroscopy reduces the inaccuracies making use of the detailed spectral response. However, the scale effect still has a strong influence and cannot be neglected. This study aims to bridge the knowledge gap in understanding the scale effect in imaging spectroscopy when moving from 4 to 30 m pixel size for tree species mapping, keeping in mind that most current and future hyperspectral satellite based sensors work with spatial resolution around 30 m or more.Two airborne (HyMAP) and one s"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143755",
        "research_problem": "Vegetation stress through Hyperspectral remote sensing",
        "orkg_properties": "['research problem', 'Study Area', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['Vegetation type', 'Vegetation growth stage', 'Hyperspectral sensor', 'Spectral bands', 'Spectral resolution', 'Feature extraction methods', 'Classification algorithms', 'Vegetation indices', 'Temporal resolution', 'Spatial resolution', 'Data preprocessing steps']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Hyper-spectral remote sensing to monitor vegetation stress",
        "abstract": "Background, aim, and scopeVegetation stress diagnoses based on plant sampling and physiochemical analysis using traditional methods are commonly time-consuming, destructive and expensive. The measurement of field spectral reflectance is one basis of airborne or spaceborne remote sensing monitoring.Materials and methodsIn this study, paddy plants were grown in the barrels evenly filled with 10.0 kg soil that was mixed respectively with 0, 2.5 × 207.2 and 5.0 × 207.2 mg Pb per 1,000 g soil. Rice canopy spectra were gathered by mobile hyper-spectral radiometer (ASD FieldSpec Pro FR, USA). Meanwhile, canopy leaves in the field-of-view (FOV) of spectroradiometer were collected and then prepared in the laboratory, (1) for chlorophyll measurement by Model 721 spectrophotometer, and (2) for Pb determination by atomic absorption spectrophotometer (SpectraAA-220FS).Results and discussionCanopy spectral reflectance in the region of visible-to-near-infrared light (VNIR) increased, because ascended Pb concentration caused the decrease of canopy chlorophyll content. In the agro-ecosystem, however, heavy metal contamination is presented typically as mixture and their interactions strongly affect actually occurring effects. Normalized spectral absorption depth (Dn), and shifting distance (DS) of red edge position (REPs) revealed the differences in Pb concentration for canopy leaves, especially at the early tillering stage. Due to insufficient biomass of rice plants, the 30th day was not reliable enough for the selection of crucial growth stages. Some special sensitive bands might be omitted at the same time because of limited sample sets.ConclusionsOur initial experiments are still too few in the amounts of both metals and plants neither to build accurate prediction models nor to discuss the transformation from ground to air/spaceborne remote sensing. However, we are pleased to communicate that ground remote sensing measurements would provide reliable information for the estimation of Pb concentration in rice plants at the early tillering stage when proper features (such as DS andDn) of reflectance spectra are applied.Recommendations and perspectivesHyper-spectral remote sensing is a potential and promising technology for monitoring environmental stresses on agricultural vegetation. Further ground remote sensing experiments are necessary to evaluate the possibility of hyper-spectral reflectance spectroscopy in monitoring different kinds of metals’ stress on various plants.\nREPs and DSs towards longer or shorter wavelength are summarized in Table1. It shows that REPs of Pb2 paddy rice plants shifted towards shorter wavelength (blue light) by 22 nm on the 30th day, but DSs were equal to zero when paddy rice plants are on the 50th and 65th day with more and more exuberant vegetation, and then REPs of Pb2 shifted towards longer wavelength (red light) by 6 nm and 12 nm. In contrast, DSs of Pb1 were much less and not obvious.Table 1 REPs and DS (nm) in 1st derivative spectra in all growth stagesFull size tableThe beginning and the terminal points were determined at 530 and 740 nm, respectively. Pb contamination also made the depth of absorption at 680 nm different between paddy rice plants of Pb0, Pb1 and Pb2. On the 50th day (Fig.1a),Dncurve of Pb0 enveloped those of Pb1 and Pb2 from 530 to 740 nm, while on the 65th day (Fig.1b) all threeDncurves went consistently, and then on the 80th day (Fig.1c). TheDncurve of Pb0 was enveloped by both of Pb1 and Pb2 fro"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143763",
        "research_problem": "Spectral library for remote sensing of urban materials",
        "orkg_properties": "['research problem', 'Study Area', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['spectral resolution',\n 'spatial resolution',\n 'spectral range',\n 'material composition',\n 'urban environment',\n 'remote sensing techniques',\n 'data acquisition',\n 'data preprocessing',\n 'data interpretation',\n 'classification algorithms',\n 'data fusion',\n 'sample size',\n 'sensor characteristics',\n 'atmospheric correction']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Development and utilization of urban spectral library for remote sensing of urban environment",
        "abstract": null
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143778",
        "research_problem": "Effects of mining areas on vegetation through Hyperspectral remote sensing",
        "orkg_properties": "['research problem', 'Study Area', 'Preprocessing', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['mining areas',\n 'vegetation',\n 'effects',\n 'Hyperspectral remote sensing']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 3,
        "title": "Application of hyperspectral remote sensing for environment monitoring in mining areas",
        "abstract": "Environmental problems caused by extraction of minerals have long been a focus on environmental earth sciences. Vegetation growing conditions are an indirect indicator of the environmental problem in mining areas. A growing number of studies in recent years made substantial efforts to better utilize remote sensing for dynamic monitoring of vegetation growth conditions and the environment in mining areas. In this article, airborne and satellite hypersectral remote sensing data—HyMap and Hyperion images are used in the Mount Lyell mining area in Australia and Dexing copper mining area in China, respectively. Based on the analyses of biogeochemical effect of dominant minerals, the vegetation spectrum and vegetation indices, two hyperspectral indices: vegetation inferiority index (VII) and water absorption disrelated index (WDI) are employed to monitor the environment in the mining area. Experimental results indicate that VII can effectively distinguish the stressed and unstressed vegetation growth situation in mining areas. The sensitivity of VII to the vegetation growth condition is shown to be superior to the traditional vegetation index—NDVI. The other index, WDI, is capable of informing whether the target vegetation is affected by a certain mineral. It is an important index that can effectively distinguish the hematite areas that are covered with sparse vegetation. The successful applications of VII and WDI show that hyperspectral remote sensing provides a good method to effectively monitor and evaluate the vegetation and its ecological environment in mining areas."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143827",
        "research_problem": "Application of Hyperspectral remote sensing to Agriculture",
        "orkg_properties": "['research problem', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['Hyperspectral remote sensing technology', 'Agriculture', 'Crop monitoring', 'Plant health assessment', 'Yield prediction', 'Water stress detection', 'Disease detection', 'Crop classification', 'Crop mapping', 'Nutrient management', 'Precision agriculture', 'Data analysis techniques']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "A short survey of hyperspectral remote sensing applications in agriculture",
        "abstract": ":Hyperspectral sensors are devices that acquire images over hundreds of spectral bands, thereby enabling the extraction of spectral signatures for objects or materials observed. Hyperspectral remote sensing has been used over a wide range of applications, such as agriculture, forestry, geology, ecological monitoring and disaster monitoring. In this paper, the specific application of hyperspectral remote sensing to agriculture is examined. The technological development of agricultural methods is of critical importance as the world's population is anticipated to continuously rise much beyond the current number of 7 billion. One area upon which hyperspectral sensing can yield considerable impact is that of precision agriculture - the use of observations to optimize the use of resources and management of farming practices. For example, hyperspectral image processing is used in the monitoring of plant diseases, insect pests and invasive plant species; the estimation of crop yield; and the fine classification of crop distributions. This paper also presents a detailed overview of hyperspectral data processing techniques and suggestions for advancing the agricultural applications of hyperspectral technologies in Turkey."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143846",
        "research_problem": "Use of hyperspectral remote sensing technology to geological applications",
        "orkg_properties": "['research problem', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['Hyperspectral remote sensing technology', \n 'Geological applications']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "A review on the geological applications of hyperspectral remote sensing technology",
        "abstract": ":Based on the progress of hyperspectral data acquisition, information processing and geological requirements, the current status and trends of hyperspectral remote sensing...Show MoreMetadata"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143971",
        "research_problem": "Polarized raman scattering for calcium alumino silicate glasses",
        "orkg_properties": "['Techniques', 'research problem', 'Minerals in consideration', 'Instruments', 'Raman laser used']",
        "nechakhin_result": "['Raman spectroscopy', 'Polarization', 'Calcium alumino silicate glasses']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Infrared and Raman spectra of calcium alumino–silicate glasses",
        "abstract": "Calcium alumino–silicate glasses were investigated by infrared reflectivity and polarized Raman scattering. Vibrational modes were assigned to different types of atomic motion in the glass network. The nature of the low frequency Raman peak or Boson Peak is discussed in terms of existing theories. Kramers–Kronig analysis was performed on IR reflectivity data to extract longitudinal and transverse optic phonon frequencies.\nThe purpose of this review is to provide non-specialists with a basic understanding of the information micro-Raman Spectroscopy (μRS) may yield when this characterization tool is applied to nanomaterials, a generic term for describing nano-sized crystals and bulk homogeneous materials with a structural disorder at the nanoscale – typically nanoceramics, nanocomposites, glassy materials and relaxor ferroelectrics. The selected materials include advanced and ancient ceramics, semiconductors and polymers developed in the form of dots, wires, films, fibres or composites for applications in the energy, electronic and aeronautics–aerospace industries. The text is divided into five sections:•Section 1 is a general introduction.•Section 2 outlines the principles of conventional μRS.•Section 3 introduces the main effects for nanomaterials, with special emphasis on two models that connect Raman spectra features to “grain size”, namely the Phonon Confinement Model (PCM) and the Elastic Sphere Model (ESM).•Section 4 presents the experimental versatility of μRS applied to nanomaterials (phase identification, phase transition monitoring, grain size determination, defect concentration assessment, etc.).•Section 5 deals with the micro-mechanical aspects of μRS (“Raman extensometry”). Special emphasis is placed on the relationship between the stress-related coefficientsSɛ/σand the macroscopic response of the materials to the applied stress.\nCalcium alumino–silicate glasses were investigated by infrared reflectivity and polarized Raman scattering. Vibrational modes were assigned to different types of atomic motion in the glass network. The nature of the low frequency Raman peak or Boson Peak is discussed in terms of existing theories. Kramers–Kronig analysis was performed on IR reflectivity data to extract longitudinal and transverse optic phonon frequencies.\nThe structure and properties of glasses and melts in the CaO–Al2O3–SiO2(CAS) system play an important role in earth and material sciences. Aluminum has a crucial role in this ternary system, and its environment is still questioned. In this paper, we present new results using Raman spectroscopy and27Al Nuclear Magnetic Resonance on CAS glasses obtained by classic and rapid quenching methods. We propose an Al/Si tetrahedral distribution in the glass network in different Qnspecies. In this system, we show that Al and Si are mainly in Q4species along the joinR= CaO/Al2O3= 1, and in depolymerized Q2and Q3units at high CaO content for other joins (R= 1.57 and 3). Five- ([5]Al) and six-fold ([6]Al) coordinated aluminum can be detected in the peraluminous glasses (R< 1) in agreement with the deficit of charge compensators, Ca2+ions, near Al atoms. Unexpectedly, between 5% and 8% of[5]Al is also observed for percalcic glasses (R> 1), except for glasses with low silica and high CaO content. The presence of[5]Al is related to viscous flow mechanisms while, in highly depolymerized glasses, the absence of[5]Al may indicate different mechanisms for melts to flow. This systematic study on the CAS"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143983",
        "research_problem": "Advantage of integrated spectral analysis for Actinolite mineral",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['Spectral resolution', 'Spectral range', 'Spectral sensitivity', 'Spectral enhancement techniques', 'Data preprocessing', 'Data acquisition', 'Signal-to-noise ratio', 'Dimensionality reduction', 'Feature selection', 'Pattern recognition algorithms', 'Classification algorithms']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "EPR, optical, infrared and Raman spectral studies of Actinolite mineral",
        "abstract": "Electron paramagnetic resonance (EPR), optical, infrared and Raman spectral studies have been performed on a natural Actinolite mineral. The room temperature EPR spectrum reveals the presence of Mn2+and Fe3+ions giving rise to two resonance signals atg=2.0 and 4.3, respectively. The resonance signal atg=2.0 exhibits a six line hyperfine structure characteristic of Mn2+ions. EPR spectra have been studied at different temperatures from 123 to 433K. The number of spins (N) participating in the resonance atg=2.0 has been calculated at different temperatures. A linear relationship is observed between logNand 1/Tin accordance with Boltzmann law and the activation energy was calculated. The paramagnetic susceptibility (χ) has been calculated at different temperatures and is found to be increasing with decreasing temperature as expected from Curie’s law. From the graph of 1/χversusT, the Curie constant and Curie temperature have been evaluated. The optical absorption spectrum exhibits bands characteristic of Fe2+and Fe3+ions. The crystal field parameterDqand the Racah parametersBandChave been evaluated from the optical absorption spectrum. The infrared spectral studies reveal the formation of Fe3+OH complexes due to the presence of higher amount of iron in this mineral. The Raman spectrum exhibits bands characteristic of SiOSi stretching and MgOH translation modes.\nThis paper presents a new aqueous precipitation method to prepare silicon-substituted hydroxyapatites Ca10(PO4)6−y(SiO4)y(OH)2−y(VOH)y(SiHAs) and details the characterization of powders with varying Si content up toy= 1.25 mol molSiHA−1. X-ray diffraction, transmission electron microscopy, solid-state nuclear magnetic resonance and Fourier transform infrared spectroscopy were used to accurately characterize samples calcined at 400 °C for 2 h and 1000 °C for 15 h. This method allows the synthesis of monophasic SiHAs with controlled stoichiometry. The theoretical maximum limit of incorporation of Si into the hexagonal apatitic structure isy< 1.5. This limit depends on the OH content in the channel, which is a function of the Si content, temperature and atmosphere of calcination. These results, particularly those from infrared spectroscopy, raise serious reservations about the phase purity of previously prepared and biologically evaluated SiHA powders, pellets and scaffolds in the literature."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143998",
        "research_problem": "Advantage of integrated spectral analysis for clinochlore mineral",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['spectral analysis technique',\n 'integrated approach',\n 'clinochlore mineral',\n 'advantages']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Optical absorption, EPR, infrared and Raman spectral studies of clinochlore mineral",
        "abstract": "Optical absorption, EPR, Infrared and Raman spectral studies have been carried out on natural clinochlore mineral. The optical absorption spectrum exhibits bands characteristic of Fe2+and Fe3+ions. A band observed in the NIR region is attributed to an intervalence charge transfer (Fe2+–Fe3+) band. The room temperature EPR spectrum of single crystal of clinochlore mineral reveals the dominance of Fe3+ion exhibiting resonance signals atg=2.66; 3.68 and 4.31 besides one isotropic resonance signal atg=2.0. The EPR studies have been carried out for a polycrystalline sample in the temperature range from 103 to 443K and for a single crystal of clinochlore mineral in the temperature range 123–297K. The number of spins (N) participating in resonance atg=4.3 signal of the single crystal of clinochlore mineral has been calculated at different temperatures. The paramagnetic susceptibility (χ) is calculated from the EPR data at different temperatures for single crystal of clinochlore mineral. The Curie constant and Curie temperature values are evaluated from 1/χversusTgraph. The infrared spectral studies reveal the formation of Fe3+–OH complexes due to the presence of higher amount of iron in this mineral. The Raman spectrum exhibits bands characteristic of Si–O–Si stretching and Si–O bending modes.\nThe efficacy and the interface interactions of fluoride on laterite were investigated using batch methods; under various ionic strengths, pH, fluoride loading and diverse spectroscopy along with surface complexation modeling. The laterite used in this study was rich in iron (40%) and aluminum (30%). Proton binding sites were characterized by potentiometric titrations yielding pHZPCaround pH 8.72. Adsorption of fluoride on laterite is strongly pH dependent showing a maximum adsorption at pH <5, though not affected by the electrolyte concentration. Experimental data were quantified with a 2pKgeneralized diffused layer model considering two different surface binding sites for both protons and anions, using reaction stoichiometries. Surface complexation modeling showed that both Fe and Al sites of the laterite surface contributes to fluoride adsorption via inner-sphere complexation forming monodentate mononuclear interaction with laterite. Fluoride adsorption followed the Freundlich isotherm, indicating multi-site complexation on the laterite surface. FT-IR spectroscopic data provides an evidence for increased hydrogen bonding, indicated by the broadening of the OH stretch features around 3300 cm−1.\nLizardites in Yuanjiang laterite ore were characterized using X-ray diffraction (XRD), X-ray energy dispersive spectroscopy (EDS) and Fourier transform infrared spectroscopy (FTIR). Their leaching behaviour in sulphuric acid was investigated. XRD patterns show that there are two different lizardite polytypes in Yuanjiang laterite ore: lizardite-1T and lizardite-1M. The crystal order of the lizardites between soil and rock samples was different, which was demonstrated by XRD and FTIR spectrum analyses. The obvious change of the crystal cell volume was observed in the lizardite-1T of soil sample, with higher iron content, whereas the crystal cell varied little for the lizardite-1T of rock sample, with higher aluminum content. FTIR spectrum, EDS analyses and the leaching experiments show substitution of octahedral nickel, iron and aluminum, as well as tetrahedral aluminum. Iron occurred as ferrous and ferric iron in the lizardite-1T of rock sample, mostly as ferrous"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144015",
        "research_problem": "Raman spectral analysis for humite minerals",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['Raman spectroscopy', 'Spectral analysis', 'Humite minerals', 'Mineralogy', 'Chemical composition', 'Crystal structure', 'Raman spectrum', 'Spectral features', 'Peak positions', 'Peak intensities', 'Peak widths', 'Vibrational modes', 'Spectral database', 'Raman peaks classification', 'Peak assignments']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "A Raman spectroscopic study of humite minerals",
        "abstract": "Raman spectroscopy has been used to study the structure of the humite mineral group ((A2SiO4)n–A(OH, F)2wherenrepresents the number of olivine and brucite layers in the structure and is 1, 2, 3 or 4 and A2+is Mg, Mn, Fe or some mix of these cations). The humite group of minerals forms a morphotropic series with the minerals olivine and brucite. The members of the humite group contain layers of the olivine structure that alternate with layers of the brucite-like sheets. The minerals are characterized by a complex set of bands in the 800–1000 cm−1region attributed to the stretching vibrations of the olivine (SiO4)4−units. The number of bands in this region is influenced by the number of olivine layers. Characteristic bending modes of the (SiO4)4−units are observed in the 500–650 cm−1region. The brucite sheets are characterized by the OH stretching vibrations in the 3475–3625 cm−1wavenumber region. The position of the OH stretching vibrations is determined by the strength of the hydrogen bond formed between the brucite-like OH units and the olivine silica layer. The number of olivine sheets and not the chemical composition determines the strength of the hydrogen bonds. Copyright © 2006 John Wiley & Sons, Ltd."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144024",
        "research_problem": "Raman spectral analysis for borosilicate mineral ferroaxinite",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['Raman spectroscopy', 'minerals', 'borosilicate minerals', 'ferroaxinite', 'spectral analysis']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Raman spectroscopy of the borosilicate mineral ferroaxinite",
        "abstract": "Raman spectroscopy, complemented by infrared spectroscopy, has been used to characterise the ferroaxinite minerals of the theoretical formula Ca2Fe2+Al2BSi4O15(OH), a ferrous aluminium borosilicate. The Raman spectra are complex but are subdivided into sections on the basis of the vibrating units. The Raman spectra are interpreted in terms of the addition of borate and silicate spectra. Three characteristic bands of ferroaxinite are observed at 1082, 1056 and 1025 cm−1and are attributed to BO4stretching vibrations. Bands at 1003, 991, 980 and 963 cm−1are assigned to SiO4stretching vibrations. Bands are found in these positions for each of the ferroaxinites studied. No Raman bands were found above 1100 cm−1showing that ferroaxinites contain only tetrahedral boron. The hydroxyl stretching region of ferroaxinites is characterised by a single Raman band between 3368 and 3376 cm−1, the position of which is sample-dependent. Bands for ferroaxinite at 678, 643, 618, 609, 588, 572, 546 cm−1may be attributed to the ν4bending modes and the three bands at 484, 444 and 428 cm−1may be attributed to the ν2bending modes of the (SiO4)2−. Copyright © 2006 John Wiley & Sons, Ltd."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144034",
        "research_problem": "Raman spectral analysis for joaquinite minerals",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['Mineral composition and characteristics', 'Raman spectral profile', 'Spectral peak positions and intensities', 'Crystal structure', 'Chemical formula', 'Sampling method', 'Spectrometer type and settings', 'Data pre-processing techniques', 'Spectral databases used for comparison', 'Analytical techniques and methodologies used', 'Similarities in spectral features', 'Geological or mineralogical context']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Raman spectroscopy of the joaquinite minerals",
        "abstract": "Selected joaquinite minerals have been studied by Raman spectroscopy. The minerals are categorised into two groups depending upon whether bands occur in the 3250 to 3450 cm−1region and in the 3450 to 3600 cm−1region, or in the latter region only. The first set of bands is attributed to water stretching vibrations and the second set to OH stretching bands. In the literature, X-ray diffraction could not identify the presence of OH units in the structure of joaquinite. Raman spectroscopy proves that the joaquinite mineral group contains OH units in their structure, and in some cases both water and OH units. A series of bands at 1123, 1062, 1031, 971, 912 and 892 cm−1are assigned to SiO stretching vibrations. Bands above 1000 cm−1are attributable to the νasmodes of the (SiO4)4−and (Si2O7)6−units. Bands that are observed at 738, around 700, 682 and around 668, 621 and 602 cm−1are attributed toOSiO bending modes. The patterns do not appear to match the published infrared spectral patterns of either (SiO4)4−or (Si2O7)6−units. The reason is attributed to the actual formulation of the joaquinite mineral, in which significant amounts of Ti or Nb and Fe are found. Copyright © 2007 John Wiley & Sons, Ltd."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139324",
        "research_problem": "Chemical sensors",
        "orkg_properties": "['Analyte', 'research problem', 'Sensing material', 'Sensing environment', 'Architecture']",
        "nechakhin_result": "['Chemical properties', 'Detection method', 'Sensing material', 'Analytes', 'Sensitivity', 'Selectivity', 'Response time', 'Stability', 'Cost', 'Fabrication method']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Graphene Nanomesh As Highly Sensitive Chemiresistor Gas Sensor",
        "abstract": "Click to copy section linkSection link copied!High Resolution ImageDownload MS PowerPoint SlideGraphene is a one atom thick carbon allotrope with all surface atoms that has attracted significant attention as a promising material as the conduction channel of a field-effect transistor and chemical field-effect transistor sensors. However, the zero bandgap of semimetal graphene still limits its application for these devices. In this work, ethanol-chemical vapor deposition (CVD) of a grown p-type semiconducting large-area monolayer graphene film was patterned into a nanomesh by the combination of nanosphere lithography and reactive ion etching and evaluated as a field-effect transistor and chemiresistor gas sensors. The resulting neck-width of the synthesized nanomesh was about ∼20 nm and was comprised of the gap between polystyrene (PS) spheres that was formed during the reactive ion etching (RIE) process. The neck-width and the periodicities of the graphene nanomesh (GNM) could be easily controlled depending on the duration/power of the RIE and the size of the PS nanospheres. The fabricated GNM transistor device exhibited promising electronic properties featuring a high drive current and anION/IOFFratio of about 6, significantly higher than its film counterpart. Similarly, when applied as a chemiresistor gas sensor at room temperature, the graphene nanomesh sensor showed excellent sensitivity toward NO2and NH3, significantly higher than their film counterparts. The ethanol-based graphene nanomesh sensors exhibited sensitivities of about 4.32%/ppm in NO2and 0.71%/ppm in NH3with limits of detection of 15 and 160 ppb, respectively. Our demonstrated studies on controlling the neck width of the nanomesh would lead to further improvement of graphene-based transistors and sensors.This publication is licensed under the terms of your\n                            institutional subscription.Request reuse permissions."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R141661",
        "research_problem": "Nanothermometer",
        "orkg_properties": "['precursors', 'research problem', 'Method of nanomaterial synthesis', 'Nanomaterial', 'Has a unit']",
        "nechakhin_result": "['Nanotechnology', 'Temperature measurement', 'Thermometers', 'Nanostructures', 'Materials science', 'Nanoparticles']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "Fluorescent N-Doped Carbon Dots as in Vitro and in Vivo Nanothermometer",
        "abstract": "Click to copy section linkSection link copied!High Resolution ImageDownload MS PowerPoint SlideThe fluorescent N-doped carbon dots (N-CDs) obtained from C3N4emit strong blue fluorescence, which is stable with different ionic strengths and time. The fluorescence intensity of N-CDs decreases with the temperature increasing, while it can recover to the initial one with the temperature decreasing. It is an accurate linear response of fluorescence intensity to temperature, which may be attributed to the synergistic effect of abundant oxygen-containing functional groups and hydrogen bonds. Further experiments also demonstrate that N-CDs can serve as effectivein vitroandin vivofluorescence-based nanothermometer.This publication is licensed under the terms of your\n                            institutional subscription.Request reuse permissions."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R195342",
        "research_problem": "Ontology integration",
        "orkg_properties": "['Implemented technologies', 'research problem']",
        "nechakhin_result": "['domain',\n 'ontology',\n 'integration approach',\n 'integration method',\n 'semantic web',\n 'knowledge representation',\n 'mapping techniques',\n 'ontology mapping',\n 'ontology alignment',\n 'ontology merging',\n 'ontology mediation',\n 'ontology mediation',\n 'ontology interoperability',\n 'ontology versioning',\n 'ontology evolution',\n 'ontology similarity',\n 'ontology matching',\n 'ontology heterogeneity']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Constructing Cooking Ontology for Live Streams",
        "abstract": "Constructing Cooking Ontology for Live StreamsShanlin ChangSan-Yih HwangYu-Chen YangAffiliation:National Sun Yat-sen University, TWCloseAffiliation:National Sun Yat-sen University, TWCloseAffiliation:National Sun Yat-sen University, TWCloseChapter from the book: Australasian Conference on Information Systems,  . 2018.Australasian Conference on Information Systems 2018.Return to BookDownloadPDFWe build a cooking domain knowledge by using an ontology schema that reflects natural language processing and enhances ontology instances with semantic query. Our research helps audiences to better understand live streaming, especially when they just switch to a show. The practical contribution of our research is to use cooking ontology, so we may map clips of cooking live stream video and instructions of recipes. The architecture of our study presents three sections: ontology construction, ontology enhancement, and mapping cooking video to cooking ontology. Also, our preliminary evaluations consist of three hierarchies—nodes, ordered-pairs, and 3-tuples—that we use to referee (1) ontology enhancement performance for our first experiment evaluation and (2) the accuracy ratio of mapping between video clips and cooking ontology for our second experiment evaluation. Our results indicate that ontology enhancement is effective and heightens accuracy ratios on matching pairs with cooking ontology and video clips.Disciplines:ComputingInformation technology"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R195387",
        "research_problem": "Inspection of interrelationship between foods and biomarkers",
        "orkg_properties": "['description', 'research problem', 'Implemented technologies', 'type of software']",
        "nechakhin_result": "['Foods',\n 'Biomarkers',\n 'Interrelationship',\n 'Nutrition',\n 'Diet',\n 'Metabolism',\n 'Health',\n 'Biochemical markers',\n 'Food analysis',\n 'Dietary patterns']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "FOBI: an ontology to represent food intake data and associate it with metabolomic data",
        "abstract": "Nutrition research can be conducted by using two complementary approaches: (i) traditional self-reporting methods or (ii) via metabolomics techniques to analyze food intake biomarkers in biofluids. However, the complexity and heterogeneity of these two very different types of data often hinder their analysis and integration. To manage this challenge, we have developed a novel ontology that describes food and their associated metabolite entities in a hierarchical way. This ontology uses a formal naming system, category definitions, properties and relations between both types of data. The ontology presented is called FOBI (Food-Biomarker Ontology) and it is composed of two interconnected sub-ontologies. One is a ’Food Ontology’ consisting of raw foods and ‘multi-component foods’ while the second is a ‘Biomarker Ontology’ containing food intake biomarkers classified by their chemical classes. These two sub-ontologies are conceptually independent but interconnected by different properties. This allows data and information regarding foods and food biomarkers to be visualized in a bidirectional way, going from metabolomics to nutritional data or vice versa. Potential applications of this ontology include the annotation of foods and biomarkers using a well-defined and consistent nomenclature, the standardized reporting of metabolomics workflows (e.g. metabolite identification, experimental design) or the application of different enrichment analysis approaches to analyze nutrimetabolomic data.Availability: FOBI is freely available in both OWL (Web Ontology Language) and OBO (Open Biomedical Ontologies) formats at the project’s Github repository (https://github.com/pcastellanoescuder/FoodBiomarkerOntology) and FOBI visualization tool is available inhttps://polcastellano.shinyapps.io/FOBI_Visualization_Tool/."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R195718",
        "research_problem": "Food recommendation",
        "orkg_properties": "['description', 'research problem', 'Implemented technologies', 'type of software']",
        "nechakhin_result": "['Cuisine', 'Ingredients', 'Dietary restrictions', 'Flavors', 'Nutritional value', 'Cooking methods', 'Meal type', 'Food preferences']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "NAct: The Nutrition &amp; Activity Ontology for Healthy Living",
        "abstract": "This paper presents the NAct (Nutrition & Activity) Ontology, designed to drive personalised nutritional and physical activity recommendations and effectively support healthy living, through a reasoning-based AI decision support system. NAct coalesces nutritional, medical, behavioural and lifestyle indicators with potential dietary and physical activity directives. The paper presents the first version of the ontology, including its co-design and engineering methodology, along with usage examples in supporting healthy nutritional and physical activity choices. Lastly, the plan for future improvements and extensions is discussed."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R198240",
        "research_problem": "Monitoring people behavior",
        "orkg_properties": "['description', 'research problem', 'name', 'purpose']",
        "nechakhin_result": "['Surveillance techniques',\n 'Data collection methods',\n 'Analysis techniques',\n 'Ethical considerations',\n 'Privacy concerns',\n 'Context of behavior (e.g., online, physical)',\n 'Demographic factors',\n 'Temporal factors',\n 'Spatial factors',\n 'Cultural factors',\n 'Psychological factors',\n 'Societal factors']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "HeLiS: An Ontology for Supporting Healthy Lifestyles",
        "abstract": "The use of knowledge resources in the digital health domain is a trending activity significantly grown in the last decade. In this paper, we presentHeLiS: an ontology aiming to provide in tandem a representation of both the food and physical activity domains and the definition of concepts enabling the monitoring of users’ actions and of their unhealthy behaviors. We describe the construction process, the plan for its maintenance, and how this ontology has been used into a real-world system with a focus on “Key to Health”: a project for promoting healthy lifestyles on workplaces.\nTheHeLiSontology is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0Footnote14and it can be downloaded from thePerKAppproject website at the link reported at the beginning of the paper. The rational behind the CC BY-NC-SA 4.0 is that the Trentino Healthcare Department, that funds the project in which theHeLiSontology has been developed, was not in favor of releasing this ontology for business purposes. Hence, they force the adoption of this type of license for releasing the ontology. TheHeLiSontology can be downloaded in two different modalities: (i) the conceptual model only, where the user can download a light version of the ontology that does not contain any individual, or (ii) the full package, where the ontology is populated with all the individuals we have already modeled. TheHeLiSontology is constantly updated due to the project activities using the ontology as core component.The ontology is available also as web service. Detailed instructions are provided on the ontology website. Briefly, the service exposes a set ofinformativemethods enabling the access to a JSON representation of the individuals included into the ontology.The reusability aspect of theHeLiSontology can be seen from three perspectives. First, theHeLiSontology contains structured supervised knowledge about the food and physical activity domains. Provided information has a high granularity and the consistency of the modeled data have been validated by domain experts. Hence, the ontology represents a valuable artifact for the digital health domain. Second, the ontology model represents a relevant resource of medical knowledge. In particular, the ontology contains a set of rules, modeling good practices, related to what a person should eat and which physical activities a person should do for maintaining a good health, and the relationships between food’s allergies or intolerances and specific food categories. This kind of knowledge is presented, for the first time, in a structured way and it can be reused in several third-party applications for different purposes. Third, theHeLiSontology enables the construction of health-based (but not limited to this domain) applications exploiting the whole content of the ontology as well as the sole conceptual model.\nThe topic area of theHeLiSontology is timely for researchers and developers working on digital health applications (including mobile) who are now exploring the use of Semantic Web technology in the form of knowledge graphs and rules to analyze the nutrition intake over time, activities, and their association with health risks and symptoms related to chronic diseases.TheHeLiSontology can be integrated within applications going beyond the mere access to the resource for informative purposes. Indeed, theMonitoringbranch of the ontology can be populated to properly respond to the needs of the solutions integrating the"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R201361",
        "research_problem": "Constraint Question Answering over Food Knowledge Graph",
        "orkg_properties": "['method', 'description', 'research problem', 'Domain', 'question answering type of system', 'Question Answering category', 'name']",
        "nechakhin_result": "['Food domain', 'Knowledge graph', 'Constraint', 'Question answering']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph",
        "abstract": "Food recommendation has become an important means to help guide users to adopt healthy dietary habits. Previous works on food recommendation either i) fail to consider users' explicit requirements, ii) ignore crucial health factors (e.g., allergies and nutrition needs), or iii) do not utilize the rich food knowledge for recommending healthy recipes. To address these limitations, we propose a novel problem formulation for food recommendation, modeling this task as constrained question answering over a large-scale food knowledge base/graph (KBQA). Besides the requirements from the user query, personalized requirements from the user's dietary preferences and health guidelines are handled in a unified way as additional constraints to the QA system. To validate this idea, we create a QA style dataset for personalized food recommendation based on a large-scale food knowledge graph and health guidelines. Furthermore, we propose a KBQA-based personalized food recommendation framework which is equipped with novel techniques for handling negations and numerical comparisons in the queries. Experimental results on the benchmark show that our approach significantly outperforms non-personalized counterparts (average 59.7% absolute improvement across various evaluation metrics), and is able to recommend more relevant and healthier recipes."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R217480",
        "research_problem": "Question Answering ",
        "orkg_properties": "['method', 'description', 'data source', 'research problem', 'Implemented technologies', 'Domain', 'Question Answering category']",
        "nechakhin_result": "['Natural language processing', 'Information retrieval', 'Text comprehension', 'Machine learning', 'Relevance ranking']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Food safety Knowledge Graph and Question Answering System",
        "abstract": "The issue of food safety in recent years has always been the focus of public opinion. Every time there are unqualified foods, it will cause widespread panic and rumor spread, which has a great impact on social stability. Therefore, this paper crawled the data of unqualified foods officially released in recent years from the network, and designed the extraction algorithm of food general entities, food domain entities and relationships between entities for these data. The extracted entity pairs were stored in the gStore database. In order to solve the problem of association of knowledge in knowledge graph, this paper also designed the food safety ontology which organized the concepts, classifications and relationships about food production and food inspection. Finally, this paper also built an intelligent question answering system by means of gStore's http service to help person grasp the unqualified food information through natural language."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R218619",
        "research_problem": "Question answering dataset",
        "orkg_properties": "['method', 'data source', 'research problem', 'Domain', 'Type of knowledge source', 'Question Type', 'language']",
        "nechakhin_result": "['Text similarity', 'Natural language processing', 'Information retrieval', 'Machine learning', 'Semantic analysis', 'Document classification']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation",
        "abstract": "The proliferation of recipes and other food information on the Web presents an opportunity for discovering and organizing diet-related knowledge into a knowledge graph. Currently, there are several ontologies related to food, but they are specialized in specific domains, e.g., from an agricultural, production, or specific health condition point-of-view. There is a lack of a unified knowledge graph that is oriented towards consumers who want to eat healthily, and who need an integrated food suggestion service that encompasses food and recipes that they encounter on a day-to-day basis, along with the provenance of the information they receive. Our resource contribution is a software toolkit that can be used to create a unified food knowledge graph that links the various silos related to food while preserving the provenance information. We describe the construction process of our knowledge graph, the plan for its maintenance, and how this knowledge graph has been utilized in several applications. These applications include a SPARQL-based service that lets a user determine what recipe to make based on ingredients at hand while taking constraints such as allergies into account, as well as a cognitive agent that can perform natural language question answering on the knowledge graph.Resource Website:https://foodkg.github.io\nAmerican Diabetes Association: 4. lifestyle management: standards of medical care in diabetes—2018. Diab. Care40(Suppl. 1), S33–S43 (2017)"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R220967",
        "research_problem": "Question Generation",
        "orkg_properties": "['method', 'data source', 'research problem', 'Domain', 'Type of knowledge source']",
        "nechakhin_result": "['problem statement', 'research question', 'objectives', 'data collection methods', 'data analysis techniques', 'findings', 'limitations', 'recommendations', 'future research directions']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Exploiting a Large-scale Knowledge Graph for Question Generation in Food Preference Interview Systems",
        "abstract": "This paper presents a dialogue system that acquires user's food preference through a conversation. First, we proposed a method for selecting relevant topics and generating questions based on Freebase, a large-scale knowledge graph. To select relevant topics, using the Wikipedia corpus, we created a topic-embedding model that represents the correlation among topics. For missing entities in Freebase, knowledge completion was applied using knowledge graph embedding. We incorporated these functions into a dialogue system and conducted a user study. The results reveal that the proposed dialogue system more efficiently elicited words related to food and common nouns, and these words were highly correlated in a word embedding space."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R218619",
        "research_problem": "Question Answering using SPARQL",
        "orkg_properties": "['description', 'research problem', 'Domain', 'question answering task', 'question answering components', 'Question Answering category']",
        "nechakhin_result": "['Natural Language Processing', 'Information Retrieval', 'Question Answering Systems', 'Semantic Web', 'SPARQL', 'Linked Data', 'Semantic Queries']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation",
        "abstract": "The proliferation of recipes and other food information on the Web presents an opportunity for discovering and organizing diet-related knowledge into a knowledge graph. Currently, there are several ontologies related to food, but they are specialized in specific domains, e.g., from an agricultural, production, or specific health condition point-of-view. There is a lack of a unified knowledge graph that is oriented towards consumers who want to eat healthily, and who need an integrated food suggestion service that encompasses food and recipes that they encounter on a day-to-day basis, along with the provenance of the information they receive. Our resource contribution is a software toolkit that can be used to create a unified food knowledge graph that links the various silos related to food while preserving the provenance information. We describe the construction process of our knowledge graph, the plan for its maintenance, and how this knowledge graph has been utilized in several applications. These applications include a SPARQL-based service that lets a user determine what recipe to make based on ingredients at hand while taking constraints such as allergies into account, as well as a cognitive agent that can perform natural language question answering on the knowledge graph.Resource Website:https://foodkg.github.io\nAmerican Diabetes Association: 4. lifestyle management: standards of medical care in diabetes—2018. Diab. Care40(Suppl. 1), S33–S43 (2017)"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R193153",
        "research_problem": "motion synthesis",
        "orkg_properties": "['dataset', 'research problem', 'Models']",
        "nechakhin_result": "['motion capture data', 'motion analysis', 'animation', 'motion planning', 'human movement', 'kinematics', 'dynamics', 'humanoid robots', 'machine learning', 'deep learning', 'computer vision', 'biomechanics', 'cognitive science']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Bayesian Adversarial Human Motion Synthesis",
        "abstract": ":We propose a generative probabilistic model for human motion synthesis. Our model has a hierarchy of three layers. At the bottom layer, we utilize Hidden semi-Markov Model (HSMM), which explicitly models the spatial pose, temporal transition and speed variations in motion sequences. At the middle layer, HSMM parameters are treated as random variables which are allowed to vary across data instances in order to capture large intra- and inter-class variations. At the top layer, hyperparameters define the prior distributions of parameters, preventing the model from overfitting. By explicitly capturing the distribution of the data and parameters, our model has a more compact parameterization compared to GAN-based generative models. We formulate the data synthesis as an adversarial Bayesian inference problem, in which the distributions of generator and discriminator parameters are obtained for data synthesis. We evaluate our method through a variety of metrics, where we show advantage than other competing methods with better fidelity and diversity. We further evaluate the synthesis quality as a data augmentation method for recognition task. Finally, we demonstrate the benefit of our fully probabilistic approach in data restoration task.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189630",
        "research_problem": "3D Rotation Estimation",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "['Computer Vision', 'Pose Estimation', 'Geometry', 'Transformations', 'Feature Extraction']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "A Fast Inverse Kinematics Algorithm for Joint Animation",
        "abstract": "The cyclic coordinate descent(CCD) is a well-known algorithm used for inverse kinematics solutions in multi-joint chains. CCD algorithm can be easily implemented, but it can take a series of iterations before converging to a solution and also generate improper joint rotations. This paper presents a novel Target Triangle algorithm that can fast decides orientation and angle of joint rotation, and eliminates problems associated with improper and large angle rotations. Experimental results are presented to show the performance benefits of the proposed algorithm over CCD methods."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189637",
        "research_problem": "Motion Capture",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "['Human movement',\n 'Pose estimation',\n 'Kinematics',\n '3D tracking',\n 'Skeleton tracking',\n 'Markerless motion capture',\n 'Optical motion capture',\n 'Inertial motion capture',\n 'Biomechanics',\n 'Animation',\n 'Computer vision',\n 'Pattern recognition',\n 'Machine learning',\n 'Data-driven modeling',\n 'Human-computer interaction']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Human upper-body inverse kinematics for increased embodiment in consumer-grade virtual reality",
        "abstract": "Having a virtual body can increase embodiment in virtual reality (VR) applications. However, comsumer-grade VR falls short of delivering sufficient sensory information for full-body motion capture. Consequently, most current VR applications do not even show arms, although they are often in the field of view. We address this shortcoming with a novel human upper-body inverse kinematics algorithm specifically targeted at tracking from head and hand sensors only. We present heuristics for elbow positioning depending on the shoulder-to-hand distance and for avoiding reaching unnatural joint limits. Our results show that our method increases the accuracy compared to general inverse kinematics applied to human arms with the same tracking input. In a user study, participants preferred our method over displaying disembodied hands without arms, but also over a more expensive motion capture system. In particular, our study shows that virtual arms animated with our inverse kinematics system can be used for applications involving heavy arm movement. We demonstrate that our method can not only be used to increase embodiment, but can also support interaction involving arms or shoulders, such as holding up a shield."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189640",
        "research_problem": "Orientation",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "[\"Research area\",\n \"Research question\",\n \"Methodology\",\n \"Data used\",\n \"Key findings\",\n \"Publication year\",\n \"Authors\",\n \"Citations\",\n \"Keywords\",\n \"Related work\",\n \"Funding sources\"]",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Real-Time Inverse Kinematics Techniques for Anthropomorphic Limbs",
        "abstract": "In this paper we develop a set of inverse kinematics algorithms suitable for an anthropomorphic arm or leg. We use a combination of analytical and numerical methods to solve generalized inverse kinematics problems including position, orientation, and aiming constraints. Our combination of analytical and numerical methods results in faster and more reliable algorithms than conventional inverse Jacobian and optimization-based techniques. Additionally, unlike conventional numerical algorithms, our methods allow the user to interactively explore all possible solutions using an intuitive set of parameters that define the redundancy of the system."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189646",
        "research_problem": "motion reconstruction",
        "orkg_properties": "['method', 'paper:published_in', 'research problem', 'deals with']",
        "nechakhin_result": "['motion capture data', 'kinematics', 'temporal dynamics', 'animation', 'trajectory', 'pose estimation', '3D reconstruction', 'motion tracking', 'human movement', 'camera calibration']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Learnt inverse kinematics for animation synthesis",
        "abstract": "Existing work on animation synthesis can be roughly split into two approaches, those that combine segments of motion-capture data, and those that perform inverse kinematics. In this paper, we present a method for performing animation synthesis of an articulated object (e.g. human body and a dog) from a minimal set of body joint positions, following the approach of inverse kinematics. We tackle this problem from a learning perspective. Firstly, we address the need for knowledge on the physical constraints of the articulated body, so as to avoid the generation of a physically impossible poses. A common solution is to heuristically specify the kinematic constraints for the skeleton model. In this paper however, the physical constraints of the articulated body are represented using a hierarchical cluster model learnt from a motion capture database. Additionally, we shall show that the learnt model automatically captures the correlation between different joints through simultaneous modelling of their angles. We then show how this model can be utilised to perform inverse kinematics in a simple and efficient manner. Crucially, we describe how IK is carried out from a minimal set of end-effector positions. Following this, we show how this “learnt inverse kinematics” framework can be used to perform animation syntheses on different types of articulated structures. To this end, the results presented include the retargeting of a flat surface walking animation to various uneven terrains to demonstrate the synthesis of a full human body motion from the positions of only the hands, feet and torso. Additionally, we show how the same method can be applied to the animation synthesis of a dog using only its feet and torso positions."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189649",
        "research_problem": "Aiming",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "['Research problem',\n 'Hypotheses',\n 'Methodology',\n 'Data collection',\n 'Data analysis',\n 'Findings',\n 'Discussion',\n 'Conclusion',\n 'Limitations',\n 'Implications',\n 'Future research recommendations',\n 'References']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Analytical inverse kinematics with body posture control",
        "abstract": "This paper presents a novel whole-body analytical inverse kinematics (IK) method integrating collision avoidance and customizable body control for animating reaching tasks in real-time. Whole-body control is achieved with the interpolation of pre-designed key body postures, which are organized as a function of the direction to the goal to be reached. Arm postures are computed by the analytical IK solution for human-like arms and legs, extended with a new simple search method for achieving postures avoiding joint limits and collisions. In addition, a new IK resolution is presented that directly solves for joints parameterized in the swing-and-twist decomposition. The overall method is simple to implement, fast, and accurate, and therefore suitable for interactive applications controlling the hands of characters. The source code of the IK implementation is provided. Copyright © 2007 John Wiley & Sons, Ltd."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189652",
        "research_problem": "2D Human Pose Estimation",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "['Computer Vision', 'Deep Learning', 'Image Processing', 'Pose Estimation', 'Human Body', 'Human Detection', 'Skeleton Tracking', 'Joint Localization', 'Keypoint Detection', 'Convolutional Neural Networks', 'Feature Extraction', 'Data Augmentation']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Style-based inverse kinematics",
        "abstract": "This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in real-time. Training the model on different input data leads to different styles of IK. The model is represented as a probability distribution over the space of all possible poses. This means that our IK system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles.Our style-based IK can replace conventional IK, wherever it is used in computer animation and computer vision. We demonstrate our system in the context of a number of applications: interactive character posing, trajectory keyframing, real-time motion capture with missing markers, and posing from a 2D image."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189876",
        "research_problem": " Xray spectroscopy of  highly ionized atoms",
        "orkg_properties": "['research problem', 'Paper type', 'has system qualities']",
        "nechakhin_result": "['X-ray spectroscopy',\n 'Highly ionized atoms']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 1,
        "nechakhin_deviation": 4,
        "title": "Precision Wavelength Determination of 2^1P_1 - 1^1S_0 and 2^3P_1 - 1^1S_0 Transitions in Helium-Like Sulfur Ions",
        "abstract": "Transitions from the 21P1- and 23P1-state to the ground state 11S0in helium-like sulphur ions have been measured with an accuracy of 4×10-5. Energy calibration is described in detail and two reference wavelengths have been reevaluated. Substantial line-blending was observed, due to long-lived spectator electrons. The two transition energies were corrected for Doppler shift and compared with most refined theoretical calculations, including terms of order α4Z6in the Breit operator and terms of order α5Z6in the quantum-electrodynamical corrections. The experimental contributions to the ground-state QED shifts agree within its error (∼ 15%) with the theoretical values.Export citation and abstractBibTeXRIS\nTransitions from the 21P1- and 23P1-state to the ground state 11S0in helium-like sulphur ions have been measured with an accuracy of 4×10-5. Energy calibration is described in detail and two reference wavelengths have been reevaluated. Substantial line-blending was observed, due to long-lived spectator electrons. The two transition energies were corrected for Doppler shift and compared with most refined theoretical calculations, including terms of order α4Z6in the Breit operator and terms of order α5Z6in the quantum-electrodynamical corrections. The experimental contributions to the ground-state QED shifts agree within its error (∼ 15%) with the theoretical values."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R219153",
        "research_problem": "Spectroscopy of Highly Charged Ions (HCI) using a calorimeter",
        "orkg_properties": "['research problem', 'Paper type', 'has system qualities', 'has system measurements']",
        "nechakhin_result": "['Spectroscopy method used', 'Ions properties', 'Calorimeter properties', 'Energy levels', 'Ionization potential', 'Charge of ions', 'Calorimeter techniques', 'Calorimeter sensitivity', 'Calorimeter resolution', 'Calorimeter temperature range', 'Calorimeter materials']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "An Electron Beam Ion Trap (EBIT) Plus a Microcalorimeter: A Good Combination for Laboratory Astrophysics",
        "abstract": "An EBIT can selectively create, in principle, any charge state of every naturally occurring element, has good control on atomic collision processes, and can produce nearly ideal conditions for the analysis of highly ionized plasmas of astrophysical importance. A microcalorimeter enables the broadband detection of x-ray emission with high energy resolution and near-unity quantum efficiency in the energy range wherein many cosmic x-ray sources emit the bulk of their energy (0.2 keV–10 keV). The combination (EBIT+ microcalorimeter) provides a powerful tool for laboratory studies of the atomic/plasma processes underlying the energy release mechanisms in cosmic x-ray sources. We briefly describe some early experiments with a microcalorimeter built by the Smithsonian Astrophysical Observatory (SAO) and deployed on the NIST EBIT. We also present some very recent observations with a more advanced microcalorimeter built by SAO that can obtain an energy resolution of 4.5 eV. The higher spectral quality produced by the new system will be useful in laboratory measurements of interest in x-ray astronomy.Export citation and abstractBibTeXRIS\nAn EBIT can selectively create, in principle, any charge state of every naturally occurring element, has good control on atomic collision processes, and can produce nearly ideal conditions for the analysis of highly ionized plasmas of astrophysical importance. A microcalorimeter enables the broadband detection of x-ray emission with high energy resolution and near-unity quantum efficiency in the energy range wherein many cosmic x-ray sources emit the bulk of their energy (0.2 keV–10 keV). The combination (EBIT+ microcalorimeter) provides a powerful tool for laboratory studies of the atomic/plasma processes underlying the energy release mechanisms in cosmic x-ray sources. We briefly describe some early experiments with a microcalorimeter built by the Smithsonian Astrophysical Observatory (SAO) and deployed on the NIST EBIT. We also present some very recent observations with a more advanced microcalorimeter built by SAO that can obtain an energy resolution of 4.5 eV. The higher spectral quality produced by the new system will be useful in laboratory measurements of interest in x-ray astronomy."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R235828",
        "research_problem": "Spectroscopy in  highly charged  Iron ions",
        "orkg_properties": "['research problem', 'Paper type', 'has system qualities', 'has system measurements']",
        "nechakhin_result": "['spectroscopy', 'highly charged', 'iron ions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Laboratory Measurements and Modeling of the Fe XVII X‐Ray Spectrum",
        "abstract": "Detailed measurements, line identifications, and modeling calculations of the Fe XVII L-shell emission spectrum between 9.8 and 17.5 Å are presented. The measurements were carried out on an electron beam ion trap under precisely controlled conditions where electron-impact excitation followed by radiative cascades is the dominant line formation process. In addition to the strong transitions emanating from then= 3 shell, we identify and accurately determine wavelengths for transitions from higher shells up ton= 11, including two electric quadrupole transitions that have not been previously identified. Various theoretical values, including new distorted wave calculations, are compared to our measurements, which establish definitive values for testing spectral modeling predictions. We find a value of 3.04 ± 0.12 for the ratio of the intensity of the 2p-3d1P1resonance and of the 2p-3d3D1intercombination line situated at 15.01 and 15.26 Å, respectively. This value is higher than the values observed in solar spectra, which supports claims that the solar value is affected by resonant scattering. However, because our value is significantly lower than calculated values, the amount of scattering has probably been overestimated in past analyses. Comparisons of the measured intensity ratios of the transitions originating in levels of higher principal quantum numbernwith present distorted wave calculations show good agreement up ton= 6. The combined flux of all 2p-ndtransitions withn≥ 5 and all 2s-nptransitions withn= 4 and 5 relative to the flux of the 15.01 Å resonance line has been measured to be 0.13+ 0.04−0.03.Export citation and abstractBibTeXRIS\nDetailed measurements, line identifications, and modeling calculations of the Fe XVII L-shell emission spectrum between 9.8 and 17.5 Å are presented. The measurements were carried out on an electron beam ion trap under precisely controlled conditions where electron-impact excitation followed by radiative cascades is the dominant line formation process. In addition to the strong transitions emanating from then= 3 shell, we identify and accurately determine wavelengths for transitions from higher shells up ton= 11, including two electric quadrupole transitions that have not been previously identified. Various theoretical values, including new distorted wave calculations, are compared to our measurements, which establish definitive values for testing spectral modeling predictions. We find a value of 3.04 ± 0.12 for the ratio of the intensity of the 2p-3d1P1resonance and of the 2p-3d3D1intercombination line situated at 15.01 and 15.26 Å, respectively. This value is higher than the values observed in solar spectra, which supports claims that the solar value is affected by resonant scattering. However, because our value is significantly lower than calculated values, the amount of scattering has probably been overestimated in past analyses. Comparisons of the measured intensity ratios of the transitions originating in levels of higher principal quantum numbernwith present distorted wave calculations show good agreement up ton= 6. The combined flux of all 2p-ndtransitions withn≥ 5 and all 2s-nptransitions withn= 4 and 5 relative to the flux of the 15.01 Å resonance line has been measured to be 0.13+ 0.04−0.03."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R203757",
        "research_problem": "COVID-19 and supply chain management",
        "orkg_properties": "['Methodology', 'research problem', 'Area of study', 'used method']",
        "nechakhin_result": "['COVID-19 impact', 'Supply chain disruptions', 'Supply chain resilience', 'Supply chain risk management', 'Pandemic response', 'Logistics', 'Inventory management', 'Demand forecasting', 'Supplier relationship management', 'Digital transformation', 'Information sharing', 'Transportation management']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Corona virus, tariffs, trade wars and supply chain evolutionary design",
        "abstract": "PurposeUsing the constructal law of physics this study aims to provide guidance to future scholarship on global supply chain management. Further, through two case studies the authors are developing, the authors report interview findings with two senior VPs from two multi-national corporations being disrupted by COVID-19. This study suggests how this and recent events will impact on the design of future global supply chains.Design/methodology/approachThe authors apply the constructal law to explain the recent disruptions to the global supply chain orthodoxy. Two interviews are presented from case studies the authors are developing in the USA and UK – one a multi-national automobile parts supplier and the other is a earth-moving equipment manufacture. Specifically, this is an exploratory pathway work trying to make sense of the COVID-19 pandemic and its impact on supply chain scholarship.FindingsAdopting the approach of Bejan, the authors believe that what is happening today with COVID-19 and other trade disruptions such as Brexit and the USA imposing tariffs is creating new obstacles that will redirect the future flow of supply chains.Research limitations/implicationsIt is clear that the COVID-19 response introduced a bullwhip effect in the manufacturing sector on a scale never-before seen. For scholars, the authors would suggest there are four pathway topics going forward. These topics include: the future state of global sourcing, the unique nature of a combined “demand” and “supply shortage” bullwhip effect, the resurrection of lean and local production systems and the development of risk-recovery contingency strategies to deal with pandemics.Practical implicationsSupply chain managers tend to be iterative and focused on making small and subtle changes to their current system and way of thinking, very often seeking to optimize cost or negotiate better contracts with suppliers. In the current environment, however, such activities have proved to be of little consequence compared to the massive forces of economic disruption of the past three years. Organizations that have more tightly compressed supply chains are enjoying a significant benefit during the COVID-19 crisis and are no longer being held hostage to governments of another country.Social implicationsAn implicit assumption in the press is that COVID-19 caught everyone by surprise, and that executives foolishly ignored the risks of outsourcing to China and are now paying the price. However, noted scholars and epidemiologists have been warning of the threats of pandemics since the severe acute respiratory syndrome (SARS) virus. The pundits would further posit that in their pursuit of low-cost production, global corporations made naive assumptions that nothing could disrupt them. Both the firms the authors have interviewed had to close plants to protect their workforce. It was indicated in the cases the authors are developing that it is going to take manufacturers on average one month to recover from 4–6 days of disruption. These companies employ many thousands of people, and direct and ancillary workers are now temporarily laid off and face an uncertain future as/when they will recover back to normal production.Originality/valueUsing the constructal law of physics, the authors seek to provide guidance to future scholarship on global supply chain management. Further, through two case studies, the authors provide the first insight from two senior VPs from two leading multi-national co"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R203650",
        "research_problem": "Demand forecasting in supply chain ",
        "orkg_properties": "['method', 'metric', 'research problem', 'Data Dimension']",
        "nechakhin_result": "['Time series analysis', 'Statistical modeling', 'Inventory management', 'Forecasting methods', 'Machine learning', 'Data analytics', 'Demand patterns', 'Supply chain management', 'Forecast accuracy', 'Forecast error', 'Demand variability']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Evaluation of deep learning with long short-term memory networks for time series forecasting in supply chain management",
        "abstract": "Performance analysis and forecasting the evolution of complex systems are two challenging tasks in manufacturing. Time series data from complex systems capture the dynamic behaviors of the underlying processes. However, non-linear and non-stationary dynamics pose a major challenge for accurate forecasting. To overcome statistical complexities through analyzing time series, we approach the problem with deep learning methods. In this paper, we mainly focus on the long short-term memory (LSTM) networks for demand forecasts in supply chain management, where the future demand for a certain product is the basis for the respective replenishment systems. This study contributes to the literature by conducting experiments on real data to investigate the potential of using LSTM networks for final customer demand forecasting, and hence for increasing the overall value generated by a supply chain. Both forward LSTM and bidirectional LSTM (forward-backward) for short- and long-term demand prediction in supply chain management are considered in this study.\nThe proper selection of a demand forecasting method is directly linked to the success of supply chain management (SCM). However, today’s manufacturing companies are confronted with uncertain and dynamic markets. Consequently, classical statistical methods are not always appropriate for accurate and reliable forecasting. Algorithms of Artificial intelligence (AI) are currently used to improve statistical methods. Existing literature only gives a very general overview of the AI methods used in combination with demand forecasting. This paper provides an analysis of the AI methods published in the last five years (2017-2021). Furthermore, a classification is presented by clustering the AI methods in order to define the trend of the methods applied. Finally, a classification of the different AI methods according to the dimensionality of data, volume of data, and time horizon of the forecast is presented. The goal is to support the selection of the appropriate AI method to optimize demand forecasting.\nNumerous recent studies have attempted to create efficient mechanical trading systems through the use of machine learning approaches for stock price estimation and portfolio management. Using the ability to foresee the future trends of the stock performance, the return of investment can be maximized for short-term trading. This paper will review various Artificial Intelligence (AI) and Machine Learning (ML) strategies for stock price forecasting. The aim of this review is to discuss various techniques for stock price prediction that incorporate ARIMA, LSTM, Hybrid LSTM, CNN, and Hybrid CNN. Additionally, it will also discuss the limitations and accuracy of the various models, including the ARIMA model, the LSTM model, the MI-LSTM model, the Bi-LSTM model, the LSTM-DRNN model, the CNN model, the GC-CNN model, the CNN-LSTM model, the CNN-TLSTM model, and the CNN-BiLSTM model, in terms of percentage of accuracy or error calculation in terms of standard accuracy measures like RMSE, MAPE, MAE. The models can be used to forecast either the accurate stock rate, induced by the low MSE, RMSE and MAE of LSTM models, or the general trend and deflection range of the stock the following day, induced by the ability to dynamically capture swift changes in the system of CNN models. These characteristics consequently illustrate the advantages of the hybrid model at efficiently and accurately forecasting stock attributes."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212254",
        "research_problem": "forward supply chain",
        "orkg_properties": "['research problem', 'Mathematical model', 'Solution approach', 'Disruption', 'Objectif function']",
        "nechakhin_result": "['logistics', 'inventory management', 'supplier relationship management', 'transportation management', 'distribution network design', 'demand forecasting', 'order fulfillment', 'collaboration and coordination', 'sustainability and environmental impact']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Strategies for protecting supply chain networks against facility and transportation disruptions: an improved Benders decomposition approach",
        "abstract": "Disruptions rarely occur in supply chains, but their negative financial and technical impacts make the recovery process very slow. In this paper, we propose a capacitated supply chain network design (SCND) model under random disruptions both in facility and transportation, which seeks to determine the optimal location and types of distribution centers (DC) and also the best plan to assign customers to each opened DC. Unlike other studies in the extent literature, we use new concepts of reliability to model the strategic behavior of DCs and customers at the network: (1) Failure of DCs might be partial, i.e. a disrupted DC might still be able to serve with a portion of its initial capacity (2) The lost capacity of a disrupted DC shall be provided from a non-disrupted one and (3) The lost capacity fraction of a disrupted DC depends on its initial investment amount in the design phase.In order to solve the proposed model optimally, a modified version of Benders’ Decomposition (BD) is applied. This modification tackles the difficulties of the BD’s master problem (MP), which ultimately improves the solution time of BD significantly. The classical BD approach results in low density cuts in some cases, Covering Cut Bundle (CCB) generation addresses this issue by generating a bundle of cuts instead of a single cut, which could cover more decision variables of the MP. Our inspiration to improve the CCB generation led to a new method, namely Maximum Density Cut (MDC) generation. MDC is based on the observation that in some cases CCB generation is cumbersome to solve in order to cover all decision variables of the MP rather than to cover part of them. Thus the MDC method generates a cut to cover the remaining decision variables which are not covered by CCB. Numerical experiments demonstrate the practicability of the proposed model to be promising in the SCND area, also the modified BD approach decreases the number of BD iterations and improves the CPU times, significantly.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212263",
        "research_problem": "Closed-loop supply chain",
        "orkg_properties": "['research problem', 'Uncertainty', 'Solution approach', 'Disruption', 'Objectif function']",
        "nechakhin_result": "['supply chain management', 'reverse logistics', 'circular economy', 'sustainability', 'environmental impact', 'remanufacturing', 'product recovery', 'waste management', 'inventory management', 'green logistics', 'product innovation', 'reducing carbon footprint', 'economic benefits']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Robust and reliable forward–reverse logistics network design under demand uncertainty and facility disruptions",
        "abstract": "There are two broad categories of risk, which influence the supply chain design and management. The first category is concerned with uncertainty embedded in the model parameters, which affects the problem of balancing supply and demand. The second category of risks may arise from natural disasters, strikes and economic disruptions, terroristic acts, and etc. Most of the existing studies surveyed these types of risk, separately. This paper proposes a robust and reliable model for an integrated forward–reverse logistics network design, which simultaneously takes uncertain parameters and facility disruptions into account. The proposed model is formulated based on a recent robust optimization approach to protect the network against uncertainty. Furthermore, amixed integer linear programingmodel with augmentedp-robust constraints is proposed to control the reliability of the network among disruption scenarios. The objective function of the proposed model is minimizing the nominal cost, while reducing disruption risk using thep-robustness criterion. To study the behavior of the robustness and reliability of the concerned network, several numerical examples are considered. Finally, a comparative analysis is carried out to study the performance of the augmentedp-robust criterion and other conventional robust criteria.\nThe forward/reverse logistics network design is an important and strategic issue due to its effects on efficiency and responsiveness of a supply chain. In practice, it is needed to formulate and solve real problems through efficient algorithms in a reasonable time. Hence, this paper tries to cover real case problem with a multi-objective model and an integrated forward/reverse logistics network design. Further, the model is customized and implemented for a case study in gold industry where the reverse logistics play crucial role. A new solution approach is applied for the proposed 7-layer network of the case study and the solutions are achieved in order solve the current difficulties of the investigated supply chain. This paper seeks to address how a multi objective logistics model in the gold industry can be created and solved through an efficient meta-heuristic algorithm. A green approach based on the CO2emission is considered in the network design approach. The developed model includes four echelons in the forward direction and three echelons in the reverse. First, an integer linear programming model is developed to minimize costs and emissions. Then, in order to solve the model, an algorithm based on ant colony optimization is developed. The performance of the proposed algorithm has been compared with the optimum solutions of the LINGO software through various numerical examples based on the random data and real-world instances. The evaluation studies demonstrate that the proposed model is practical and applicable and the developed algorithm is reliable and efficient. The results prove the managerial implications of the model and the solution approach in terms of presenting appropriate modifications to the mangers of the selected supply chain. Further, a Taguchi-based parameter setting is undertaken to ensure using the appropriate parameters for the algorithm.\nIn today’s globalized and highly uncertain business environments, supply chains have become more vulnerable to disruptions. This paper presents a stochastic robust optimization model for the design of a closed-loop supply chain network that performs resiliently in the fa"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212214",
        "research_problem": "knowledge graphs and Industrie 4.0",
        "orkg_properties": "['research problem', 'ontology component']",
        "nechakhin_result": "['knowledge graphs', 'Industrie 4.0']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "The industry 4.0 standards landscape from a semantic integration perspective",
        "abstract": ":Interoperability among actors, sensors, and heterogeneous systems is a crucial factor for realizing the Industry 4.0 vision, i.e., the creation of Smart Factories by enabling intelligent human-to-machine and machine-to-machine cooperation. In order to empower interoperability in Smart Factories, standards and reference architectures have been proposed. Standards allow for the description of components, systems, and processes, as well as interactions among them. Reference architectures classify, align, and integrate industrial standards according to their purposes and features. Industrial communities in Europe, the United States, and Asia have proposed various reference architectures. However, interoperability among analogous standards in these reference architectures is hampered due to different granularity representation of similar processes or production parts. In this paper, we survey the landscape of Industry 4.0 standards from a semantic perspective. To tackle the problem of interoperability between standards, we developed STO, an ontology for describing standards and their relations. Characteristics of I4.0 standards are described using STO, and these descriptions are exploited for classifying standards from different perspectives according to the reference architectures. Moreover, the semantics encoded in STO allows for the discovery of relations between I4.0 standards, and for mappings across reference architectures proposed by different industrial communities."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212229",
        "research_problem": "Exploring Role of Semantic IoT ",
        "orkg_properties": "['reasoning abilities', 'has number of modules', 'Ontology Evaluation', 'specific domain ', 'Formalization', 'research problem', 'Ontology name', 'Reused ontology', 'used methodology', 'copyright license']",
        "nechakhin_result": "['Internet of Things', 'Semantic web', 'Role-based access control', 'IoT applications', 'Semantic interoperability']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "ExtruOnt: An ontology for describing a type of manufacturing machine for Industry 4.0 systems",
        "abstract": "Semantically rich descriptions of manufacturing machines, offered in a machine-interpretable code, can provide interesting benefits in Industry 4.0 scenarios. However, the lack of that type of descriptions is evident. In this paper we present the development effort made to build an ontology, calledExtruOnt, for describing a type of manufacturing machine, more precisely, a type that performs an extrusion process (extruder). Although the scope of the ontology is restricted to a concrete domain, it could be used as a model for the development of other ontologies for describing manufacturing machines in Industry 4.0 scenarios.The terms of theExtruOntontology provide different types of information related with an extruder, which are reflected in distinct modules that constitute the ontology. Thus, it contains classes and properties for expressing descriptions about components of an extruder, spatial connections, features, and 3D representations of those components, and finally the sensors used to capture indicators about the performance of this type of machine. The ontology development process has been carried out in close collaboration with domain experts."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R192126",
        "research_problem": "Green supply chain",
        "orkg_properties": "['research problem', 'Resolution methods', 'Uncertainty', 'Objectif function']",
        "nechakhin_result": "['Sustainable practices', 'Environmental impact', 'Supply chain management', 'Carbon footprint', 'Renewable energy', 'Waste management', 'Logistics', 'Transparency', 'Corporate social responsibility', 'Circular economy', 'Green procurement']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Heuristic method for robust optimization model for green closed-loop supply chain network design of perishable goods",
        "abstract": "In the current study, a green closed-loop supply chain network design for perishable products is investigated under uncertain conditions. The demands, rate of return and the quality of returned products stand as an uncertain parameter. The considered chain, based on the study of a dairy company, is a multi-period and multi-product that comprises suppliers, manufacturers, warehouses, retailers and collection centers. A mixed-integer linear programming (MILP) model is projected to minimize the cost and environmental pollutant, simultaneously. Besides, an innovativeMILProbust model is developed for the problem under uncertainty. Due to the NP-hard nature of the problem, the research has developed an efficient heuristic, named YAG, to solve large-sized problems. Computational experiments conducted indicating that the YAG method has an average gap of less than 1.65 percent from the optimal solution within a reasonable time. Also, the YAG method finds the optimal solution in more than 34 percent of instances. The performance of the robust approach and theheuristic methodis examined in a realcase studyand a diverse range of problems. The results revealed that the robust model compared to thedeterministic modelhas better quality and seem quite more reliable. The effect of the product’s lifetime, bi-objective modeling and environmental pollutant are considered throughout the study. The results indicate that the effects of products’ lifetime and level of uncertainty vary for cost andenvironmental pollutionobjectives.\nMaximizing the value of resources and producing less waste are strategic decisions affecting sustainability and competitive advantage. Sustainable closed-loop supply chains (CLSCs) are designed to minimize waste by circling back (repairing, reselling, or dismantling for parts) previously discarded products into the value chain. This study presents a novel two-stage fuzzy supplier selection and order allocation model in a CLSC. In Stage 1, we use the fuzzy best-worst method (BWM) to select the most suitable suppliers according to economic, environmental, social, and circular criteria. In Stage 2, we use a multi-objective mixed-integer linear programming (MOMILP) model to design a multi-product, multi-period, CLSC network, and inventory-location-routing, vehicle scheduling, and quantity discounts considerations. In the proposed MOMILP, the total network costs, the undesired environmental effects, and the lost sales are minimized while job opportunities and sustainable supplier purchases are maximized. A fuzzy goal programming approach is proposed to transform the MOMILP into a single objective model. We present a case study to demonstrate the applicability of the proposed method in the garment manufacturing and distribution industry.\nThere is so much interest in online purchasing within supply chain networks nowadays. After expanding the internet access and services, customers’ behavior has changed. Today, a customer’s shopping manner usually begins with the internet search. With this approach, we face some new trends in this field, such as online-to-offline (O2O) commerce that aims to balance online and offline sales. Regarding the supply chain management, the O2O commerce can help the managers to conduct both online and offline businesses. The tire industry is one of the applications of the O2O approach, which also directly affects the supply chain network design (SCND). Therefore, this work for the first time proposes a dual-channel"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R206113",
        "research_problem": "Supply chain",
        "orkg_properties": "['research problem', 'Resolution methods', 'Uncertainty', 'Sustainability factor', 'Objectif function']",
        "nechakhin_result": "[\"Industry sector (e.g., manufacturing, retail, healthcare)\",\n\"Supply chain stages (e.g., procurement, production, distribution, logistics)\",\n\"Geographical location (e.g., global, regional, local)\",\n\"Supply chain network structure (e.g., centralized, decentralized, hybrid)\",\n\"Supply chain participants (e.g., suppliers, manufacturers, distributors)\",\n\"Supply chain processes (e.g., demand planning, inventory management, order fulfillment)\",\n\"Supply chain technologies (e.g., RFID, blockchain, AI)\",\n\"Supply chain performance metrics (e.g., cost, service level, sustainability)\",\n\"Supply chain risks and disruptions (e.g., natural disasters, global pandemics, cyber attacks)\"]",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Sustainable design of a closed-loop location-routing-inventory supply chain network under mixed uncertainty",
        "abstract": "Considering economic, environmental and social impacts, this paper presents a new sustainable closed-loop location-routing-inventory model under mixed uncertainty. The environmental impacts of CO2emissions, fuel consumption, wasted energy and the social impacts of created job opportunities and economic development are considered in this paper. The uncertain nature of the network is handled using a stochastic-possibilistic programming approach. Furthermore, for large-sized problems, a hybrid meta-heuristic algorithm and lower bounds are developed and discussed. Finally, a real case study is provided to demonstrate the applicability of the model in real-world applications, and several in-depth analyses are conducted to develop managerial implications.\nHighlights•Designing a novel sustainable closed-loopsupply chainwith routing and inventory.•Applying a stochastic-possibilistic programming method to cope with the uncertainty.•Developing a new hybrid meta-heuristic algorithm to efficiently solve the problem.•Applicability of the model is tested on a realcase study."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R218454",
        "research_problem": "Green and Sustainable Supply chain",
        "orkg_properties": "['research problem', 'Resolution methods', 'Uncertainty', 'Sustainability factor', 'Objectif function', 'CO2 footprint process']",
        "nechakhin_result": "['Supply chain management', 'Sustainability', 'Environmental impact', 'Green practices', 'Carbon footprint', 'Renewable energy', 'Circular economy', 'Waste reduction', 'Social responsibility', 'Ethical sourcing']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "A robust-heuristic optimization approach to a green supply chain design with consideration of assorted vehicle types and carbon policies under uncertainty",
        "abstract": "Adoption of carbon regulation mechanisms facilitates an evolution toward green and sustainable supply chains followed by an increased complexity. Through the development and usage of a multi-choice goal programming model solved by an improved algorithm, this article investigates sustainability strategies for carbon regulations mechanisms. We first propose a sustainable logistics model that considers assorted vehicle types and gas emissions involved with product transportation. We then construct a bi-objective model that minimizes total cost as the first objective function and follows environmental considerations in the second one. With our novel robust-heuristic optimization approach, we seek to support the decision-makers in comparison and selection of carbon emission policies in supply chains in complex settings with assorted vehicle types, demand and economic uncertainty. We deploy our model in a case-study to evaluate and analyse two carbon reduction policies, i.e., carbon-tax and cap-and-trade policies. The results demonstrate that our robust-heuristic methodology can efficiently deal with demand and economic uncertainty, especially in large-scale problems. Our findings suggest that governmental incentives for a cap-and-trade policy would be more effective for supply chains in lowering pollution by investing in cleaner technologies and adopting greener practices.\nThe integration of sustainability issues into supply chain (SC) management has progressed remarkably, most of it focused on the areas of the green supply chain (GSC) and the sustainable supply chain (SSC) (Tang and Zhou2012; Golinska-Dawson et al.2018; Heydari et al.2020). Increasing concerns about the environmental impacts and international and government regulations have attracted research attention to the GSC problems beyond merely economic aspects (Ivanov et al.2019). In an GSC, the environmental impacts from SCs need to be minimized complementing total cost minimization (Rezaee et al.2017). Moreover, social aspects in SCs became a trend and lead to introducing the SSC network (Carter and Rogers2008; Pavlov et al.2019). In general, when the financial, environmental and social impacts of the SC are considered simultaneously, the traditional SC shifts toward the SSC. The transition from the traditional goals of the SC to the new sustainable objectives is also identified as the company’s competitive advantage (Dubey et al.2015; Giannakis and Papadopoulos2016).Improvements in operating costs efficiency and service levels while paying special attention to the environmental, economic, and social considerations in the SC belong to major requirements to succeed in highly competitive markets (Golinska-Dawson et al.2018; Brandenburg et al.2019). Due to environmental pollution and increased global warming, government and international bodies have introduced laws obliging companies to address environmental issues. One of the most important parts of new regulations is reducing carbon emissions/footprint that improves the business’s environmental performance (Golinska and Romano2012). In an SC, this will bring the integrity of all parts of the SC in social commitments. A carbon footprint reduction project is therefore of a global economic importance (Fahimnia and Jabbarzadeh2016).A recent European Commission report illustrates that the amount of transport gas emissions has been continually increasing, and if no action is taken, transport emissions could make up more than 30% of tota"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212685",
        "research_problem": "effect of legume intercrops on soil nitrogen content",
        "orkg_properties": "['research problem', 'study location (country)', 'Control', 'Cropping System', 'Control Result Nutrients', 'Treatment Result Nutrients', 'Type of Soil Nutrient', 'Legume Treatment', 'Experimental Design', 'Experimental Setup', 'Planting design']",
        "nechakhin_result": "['legume species', 'intercrop management practices', 'soil type', 'soil nutrient composition', 'crop rotation', 'legume biomass production', 'legume nitrogen fixation', 'soil sampling frequency', 'nutrient management', 'soil nitrogen content measurement method']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Energizing marginal soils – The establishment of the energy crop Sida hermaphrodita as dependent on digestate fertilization, NPK, and legume intercropping",
        "abstract": "Growing energy crops in marginal, nutrient-deficient soils is a more sustainable alternative to conventional cultivation. The use of energy-intensive synthetic fertilizers needs to be reduced, preferably via closed nutrient loops in thebiomass productioncycle. In the present study based on the firstgrowing seasonof a mesocosm experiment using large bins outdoors, we evaluated the potential of the energy plantSida hermaphroditato grow in a marginalsandy soil. We applied different fertilization treatments using eitherdigestatefrombiogasproduction or a commercial mineral NPK-fertilizer. To further increase independence from synthetically produced N-fertilizers, the legume plantMedicago sativawas intercropped to introduce atmospherically fixed nitrogen and potentially facilitate the production of additionalS.hermaphroditabiomass. We found digestate to be the best performing fertilizer because it produced similar yields as the NPK fertilization but minimized nitrate leaching. Legume intercropping increased the total biomass yield by more than 100% compared toS.hermaphroditasingle cropping in the fertilized variants. However, it negatively influenced the performance ofS.hermaphroditain the following year. We conclude that a successful establishment ofS.hermaphroditafor biomass production in marginal soils is possible and digestate application formed the best fertilization method when considering a range of aspects including overall yield, nitrate leaching,nitrogen fixationofM.sativa, and sustainability over time.\nPerennial crops, as energy feedstocks, offer ecological advantages over fossil fuels by contributing to the reduction of greenhouse gases and fossil energy savings. Yet, the intensity of agricultural production may increase the pressure on soil, water resources and on biological and landscape diversity. Moreover, land use competition with food crops is demanding a spatial segregation of energy producing areas to land currently marginal for agricultural production. Therefore, the objective of this work was to determine the local and site-specific environmental impacts associated with the cultivation of perennial crops in marginal soils. The study, supported by the European Union (project OPTIMA - Optimization of Perennial Grasses for Biomass Production), was developed and applied to the cultivation phase of several perennial crops, in marginal soils of the Mediterranean region, using environmental impact assessment (EIA) protocols. Investigated crops includeMiscanthus(Miscanthus×giganteusGreef et Deu), giant reed (Arundo donaxL.), switchgrass (Panicum virgatumL.) and cardoon (Cynara cardunculusL.). Different categories were studied: fertilizers and pesticides related emissions, impact on soil and water resources and biological and landscape diversity. Results suggest that growing perennial crops in marginal Mediterranean soils do not inflict a higher impact to the environment than wheat farming (the current land use). At a scale from 0 (lower impact) to 10 (higher impact), against idle land (the reference system with a score of 5), wheat and giant reed showed the highest scores (6.7–7.3 and 6.7–7.1, respectively). Impact scores of the remaining perennials decreased in the order cardoon (5.7–6.0), Miscanthus (5.4–5.6), and switchgrass (5.2–5.5), the last one showing the lowest difference to the reference system. Overall results suggest that perennial crops provide benefits regarding soil properties and erodibility (with an average scor"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R178407",
        "research_problem": "CAN BUS Intrusion Detection",
        "orkg_properties": "['Algorithm name', 'dataset', 'research problem', 'result']",
        "nechakhin_result": "['network security', 'intrusion detection system', 'CAN bus', 'automotive cybersecurity', 'anomaly detection', 'machine learning', 'data analysis']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "WINDS: A Wavelet-Based Intrusion Detection System for Controller Area Network (CAN)",
        "abstract": ":Vehicles are equipped with Electronic Control Units (ECUs) to increase their overall system functionality and connectivity. However, the rising connectivity exposes a defenseless internal Controller Area Network (CAN) to cyberattacks. An Intrusion Detection System (IDS) is a supervisory module, proposed for identifying CAN network malicious messages, without modifying legacy ECUs and causing high traffic overhead. The traditional IDS approaches rely on time and frequency thresholding, leading to high false alarm rates, whereas state-of-the-art solutions may suffer from vehicle dependency. This paper presents a wavelet-based approach to locating the behavior change in the CAN traffic by analyzing the CAN network's transmission pattern. The proposed Wavelet-based Intrusion Detection System (WINDS) is tested on various attack scenarios, using real vehicle traffic from two independent research centers, while being expanded toward more comprehensive attack scenarios using synthetic attacks. The technique is evaluated and compared against the state-of-the-art solutions and the baseline frequency method. Experimental results show that WINDS offers a vehicle-independent solution applicable for various vehicles through a unique approach while generating low false alarms.The flowchart of wavelet-based intrusion detection system for in-vehicle communication."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R191545",
        "research_problem": "Fake Account Detection",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['Social media platform',\n 'User behavior',\n 'User profile',\n 'Account activity',\n 'Content analysis',\n 'Network analysis',\n 'Machine learning algorithms',\n 'Data mining techniques',\n 'Text classification',\n 'Image analysis',\n 'User engagement',\n 'Temporal patterns']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Towards fast and lightweight spam account detection in mobile social networks through fog computing",
        "abstract": "Now, mobile devices play an increasingly important role in social networks by sharing information quickly, such as mobile phones and wearable health surveillance devices. Mobile social networks are vulnerable to spammers because of the fragile security policies of mobile operating systems. Especially, social networks on mobile devices face many difficulties in defending against spammers due to their low computing power, poor network quality and long response time. Since graph-based algorithms require huge computing power, machine-learning classifiers require very short response time, and existing PC-based research is not suitable for mobile devices, we need a lightweight and fast response method for mobile devices to detect spammers in mobile social networks. Regarded as the extension of cloud computing, fog computing puts the data, data processing and applications in the devices that are at the edge of the Internet (without storing all of them in the cloud), which leads to a better real-time performance, adapts to the wide geographical distribution and the high mobility of mobile devices. In this paper, we propose COLOR + , a method based on fog computing that performs most computations at terminal (mobile devices). It only uses the interaction between the account and its neighbors, which makes it easy to store and calculate a local graph on a mobile device. Each interaction value can be applied to any request. COLOR + detects spammers based on a threshold of the suspicion degree. We collect 50 million normal accounts and about 40,000 spammers from Twitter. Experiments show that the accuracy of COLOR + is about 85.95%, whose average time to detect an account is 0.01s. Therefore, COLOR + is an effective detection method that can be quickly applied.\nBecause of the limitation of actual conditions, only a few part of data can be crawled from the whole Twitter dataset by us. At the same time, the Twitter dataset is downloaded from existing researches. Since spam accounts keep changing, the timeliness of data should be taken into consideration. To achieve a detection algorithm with better performance, we need to improve the algorithm on the latest dataset. However, it is still a big challenge for researchers to collect numerous data from MSNs because of the privacy protection strategy, especially real-time MSN datasets without any mistake or loss.In addition, the spam accounts analyzed and evaluated by us are still difficult to cover the typical types of MSNs, while there are still some accounts that are labeled normal by our algorithm sending spam information. As we cannot obtain the detailed information of each account accurately, there may be some mistakes in the recognition work. The wrongly labeled sample may lead to some influence on our algorithm. From another point of view, important spam accounts in the Twitter dataset come from the existing researches, and there may be different opinions about spammers. Many fans of public accounts may exceed the limit of our algorithms’ calculation and judgment.As we can see from the evaluation, we have not applied this detection system to the existing MSNs. Although the algorithms’ performance on the dataset is favorable, we’ve no idea about its real performance on the MSN server. Furthermore, the algorithm has not been compared with other existing work yet. The characteristic-based machine learning algorithm is mostly adopted in existing work. Since characteristics and program details cannot be "
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R193190",
        "research_problem": "Social Netwrok Security and Privacy",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['Security measures', 'Privacy protection', 'User authentication', 'Data encryption', 'Anonymity', 'Policy enforcement', 'Data access control', 'Privacy policies', 'Data sharing', 'User consent', 'Data breaches', 'Trust and reputation systems']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Twitter fake account detection",
        "abstract": ":Social networking sites such as Twitter and Facebook attracts millions of users across the world and their interaction with social networking has affected their life. Thi...Show MoreMetadata"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R191183",
        "research_problem": "Misinformation & Fake News",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['Topic',\n 'Author',\n 'Publication Date',\n 'Keywords',\n 'Language',\n 'Source reliability',\n 'Credibility',\n 'Fact-checking',\n 'Source type (e.g., news website, social media)',\n 'Geographical location',\n 'Demographic targeting',\n 'Level of sensationalism',\n 'Level of bias',\n 'Level of trustworthiness']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "SAFE: Similarity-Aware Multi-modal Fake News Detection",
        "abstract": "Effective detection of fake news has recently attracted significant attention. Current studies have made significant contributions to predicting fake news with less focus on exploiting the relationship (similarity) between the textual and visual information in news articles. Attaching importance to such similarity helps identify fake news stories that, for example, attempt to use irrelevant images to attract readers’ attention. In this work, we propose a\\(\\mathsf {S}\\)imilarity-\\(\\mathsf {A}\\)ware\\(\\mathsf {F}\\)ak\\(\\mathsf {E}\\)news detection method (\\(\\mathsf {SAFE}\\)) which investigates multi-modal (textual and visual) information of news articles. First, neural networks are adopted to separately extract textual and visual features for news representation. We further investigate the relationship between the extracted features across modalities. Such representations of news textual and visual information along with their relationship are jointly learned and used to predict fake news. The proposed method facilitates recognizing the falsity of news articles based on their text, images, or their “mismatches.” We conduct extensive experiments on large-scale real-world data, which demonstrate the effectiveness of the proposed method.\nCastillo, C., Mendoza, M., Poblete, B.: Information credibility on Twitter. In: The World Wide Web Conference, pp. 675–684. ACM (2011)"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212763",
        "research_problem": "COVID-19 Fake News Detection",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['Topic',\n 'Keywords',\n 'Author',\n 'Publication Date',\n 'Journal',\n 'Abstract',\n 'Introduction',\n 'Methodology',\n 'Dataset',\n 'Results',\n 'Discussion',\n 'Conclusion',\n 'References',\n 'Citation',\n 'Funding']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "COVID-19 Fake News Detection by Using BERT and RoBERTa models",
        "abstract": ":We live in a world where COVID-19 news is an everyday occurrence with which we interact. We are receiving that information, either consciously or unconsciously, without fact-checking it. In this regard, it has become an enormous challenge to keep only true COVID-19 news relevant. People are exposed to these stories on a daily basis, and not all of them are true and fact-checked reports on the COVID-19 pandemic, which was the primary reason for our research. We accepted the challenge that fake news is extremely common and that some people take these news as they are. Knowing the true power of the most recent NLP achievements, in this research we focus on detecting fake news regarding COVID-19. Our approach includes using pre-trained BERT and RoBERTa models, which we then fine-tune on real and fake news about the COVID-19 pandemic. By using pre-trained BERT and RoBERTa models on tweet data, we explore their capabilities and compare them to previous research in regard to fine-tuned BERT models for this task in which we achieve better accuracy, recall and f1 score."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R221022",
        "research_problem": "Fake News Detection",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['News source reliability', 'Content analysis', 'Social media analysis', 'Fact-checking', 'Text analysis', 'NLP techniques', 'Machine learning algorithms', 'Data sources', 'Language', 'Geographic location', 'Publication date']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Fake news detection using Deep Learning",
        "abstract": ":The evolution of the information and communication technologies has dramatically increased the number of people with access to the Internet, which has changed the way the information is consumed. As a consequence of the above, fake news have become one of the major concerns because its potential to destabilize governments, which makes them a potential danger to modern society. An example of this can be found in the US. electoral campaign, where the term \"fake news\" gained great notoriety due to the influence of the hoaxes in the final result of these. In this work the feasibility of applying deep learning techniques to discriminate fake news on the Internet using only their text is studied. In order to accomplish that, three different neural network architectures are proposed, one of them based on BERT, a modern language model created by Google which achieves state-of-the-art results."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R178376",
        "research_problem": "What is the correlation between SARS-CoV-2 viral load and disease severity",
        "orkg_properties": "['material', 'method', 'Study type', 'research problem', 'Statistical tests', 'Location', 'Time period for data collection', 'patient age', 'result']",
        "nechakhin_result": "['Viral load',\n 'Disease severity',\n 'Correlation',\n 'SARS-CoV-2']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "SARS-CoV-2 viral load as a predictor for disease severity in outpatients and hospitalised patients with COVID-19: A prospective cohort study",
        "abstract": "IntroductionWe aimed to examine if severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) polymerase chain reaction (PCR) cycle quantification (Cq) value, as a surrogate for SARS-CoV-2 viral load, could predict hospitalisation and disease severity in adult patients with coronavirus disease 2019 (COVID-19).MethodsWe performed a prospective cohort study of adult patients with PCR positive SARS-CoV-2 airway samples including all out-patients registered at the Department of Infectious Diseases, Odense University Hospital (OUH) March 9-March 17 2020, and all hospitalised patients at OUH March 10-April 21 2020. To identify associations between Cq-values and a) hospital admission and b) a severe outcome, logistic regression analyses were used to compute odds ratios (OR) and 95% Confidence Intervals (CI), adjusting for confounding factors (aOR).ResultsWe included 87 non-hospitalised and 82 hospitalised patients. The median baseline Cq-value was 25.5 (interquartile range 22.3–29.0). We found a significant association between increasing Cq-value and hospital-admission in univariate analysis (OR 1.11, 95% CI 1.04–1.19). However, this was due to an association between time from symptom onset to testing and Cq-values, and no association was found in the adjusted analysis (aOR 1.08, 95% CI 0.94–1.23). In hospitalised patients, a significant association between lower Cq-values and higher risk of severe disease was found (aOR 0.89, 95% CI 0.81–0.98), independent of timing of testing.ConclusionsSARS-CoV-2 PCR Cq-values in outpatients correlated with time after symptom onset, but was not a predictor of hospitalisation. However, in hospitalised patients lower Cq-values were associated with higher risk of severe disease."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R199183",
        "research_problem": "Can animal noroviruses infect humans?",
        "orkg_properties": "['material', 'method', 'research problem', 'Location', 'type study', 'Norovirus', 'antigen used', 'immunoglobulin class']",
        "nechakhin_result": "['Animal species', 'Viral strains', 'Host range', 'Transmission methods', 'Pathogenicity', 'Clinical symptoms', 'Human susceptibility', 'Animal model studies', 'Genetic similarity', 'Host cell receptor recognition', 'Immune response']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Exposure to Human and Bovine Noroviruses in a Birth Cohort in Southern India from 2002 to 2006",
        "abstract": "ABSTRACTHuman and bovine norovirus virus-like particles were used to evaluate antibodies in Indian children at ages 6 and 36 months and their mothers. Antibodies to genogroup II viruses were acquired early and were more prevalent than antibodies to genogroup I. Low levels of IgG antibodies against bovine noroviruses indicate possible zoonotic transmission."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R200000",
        "research_problem": "Do human noroviruses infect animals",
        "orkg_properties": "['material', 'method', 'Symptoms and signs', 'research problem', 'Species ', 'Location', 'antigen used', 'immunoglobulin class', 'primers', 'noroviruses found', 'norovirus genotype']",
        "nechakhin_result": "['viral infectivity', 'host range', 'cross-species transmission', 'zoonotic potential']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Seroprevalence for norovirus genogroup II, IV and VI in dogs",
        "abstract": "Molecular and serological data suggest that noroviruses (NoVs) might be transmitted between humans and domestic carnivores. In this study we screened an age-stratified collection of canine sera (n=516) by using an ELISA assay based on virus-like particles (VLPs) of human NoVs GII.4 and GIV.1 and carnivore NoVs GIV.2 and GVI.2. Antibodies against GII.4 and GIV.1 human NoVs and GIV.2 and GVI.2 NoVs from carnivores were identified in dog sera (13.0%, 67/516) suggesting their exposure to homologous and heterologous NoVs. Analysis of the trends of age-class prevalence showed a gradual increase in the positive rate from 9.0% and 7.0%, in young dogs <1year of age to 15.0% in dogs older than 12 years, for GII.4 and GVI.2 NoVs, respectively. A significant difference in the IgG distribution by age classes was observed for GIV.1 NoVs, with the highest rate of antibodies (7.0%) in the age group <1year and the lowest (1.0%) in the age-classes 7–9 (P=0.049). High correlation between the reactivity to GII.4 and GVI.2 NoVs was observed, likely due to conserved epitopes in the capsid structure.\nViral metagenomics is slowly taking over the traditional and widely used molecular techniques for the investigation of pathogenic viruses responsible for illness and inflicting great economic burden on the farm animal industry. Owing to the continued improvements in sequencing technologies and the dramatic reduction of per base costs of sequencing the use of next generation sequencing have been key factors in this progress. Discoveries linked to viral metagenomics are expected to be beneficial to the field of veterinary medicine starting from the development of better diagnostic assays to the design of new subunit vaccines with minimal investments. With these achievements the research has taken a giant leap even toward the better healthcare of animals and, as a result, the animal sector could be growing at an unprecedented pace.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R196928",
        "research_problem": "What is the tropism of human norovirus",
        "orkg_properties": "['material', 'method', 'Symptoms and signs', 'research problem', 'Species ', 'Source', 'patient age', 'Result: Cell tropism', 'Norovirus', 'route of infection']",
        "nechakhin_result": "['Tropism of human norovirus']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 2,
        "title": "Detection of human norovirus in intestinal biopsies from immunocompromised transplant patients",
        "abstract": "Human noroviruses (HuNoVs) can often cause chronic infections in solid organ and haematopoietic stem cell transplant (HSCT) patients. Based on histopathological changes observed during HuNoV infections, the intestine is the presumed site of virus replication in patients; however, the cell types infected by HuNoVs remain unknown. The objective of this study was to characterize histopathological changes during HuNoV infection and to determine the cell types that may be permissive for HuNoV replication in transplant patients. We analysed biopsies from HuNoV-infected and non-infected (control) transplant patients to assess histopathological changes in conjunction with detection of HuNoV antigens to identify the infected cell types. HuNoV infection in immunocompromised patients was associated with histopathological changes such as disorganization and flattening of the intestinal epithelium. The HuNoV major capsid protein, VP1, was detected in all segments of the small intestine, in areas of biopsies that showed histopathological changes. Specifically, VP1 was detected in enterocytes, macrophages, T cells and dendritic cells. HuNoV replication was investigated by detecting the non-structural proteins, RdRp and VPg. We detected RdRp and VPg along with VP1 in duodenal and jejunal enterocytes. These results provide critical insights into histological changes due to HuNoV infection in immunocompromised patients and propose human enterocytes as a physiologically relevant cell type for HuNoV cultivation.Received:22/06/2016Accepted:08/07/2016Published Online:01/09/2016Keyword(s):enterocytes,immunocompromised patients,intestinal biopsy,norovirus replicationandtransplant patients© Microbiology Society\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R206347",
        "research_problem": "What is the tropism of SARS-CoV-2",
        "orkg_properties": "['material', 'method', 'Date', 'Symptoms and signs', 'research problem', 'Species ', 'Result: Organ tropism', 'Result: Cell tropism']",
        "nechakhin_result": "['viral tropism', 'SARS-CoV-2', 'host cell tropism', 'cellular receptors', 'tissue tropism']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "SARS-CoV-2 cell tropism and multiorgan infection",
        "abstract": "We acknowledge Mingyue Xu, Hengrui Hu, Xijia Liu, Zhengyuan Su, Min Zhou for their critical support. We thank Jia Wu, Hao Tang and Jun Liu from National Biosafety Laboratory (Wuhan), Chinese Academy of Sciences for their support during the study. The study was supported in part by grants from Ministry of Science and Technology of China (2020YFC0844700 and 2020FYC0841700), Hubei Science and Technology funding (2020FCA003 and 2020FCA045), Open Research Fund Program of the State Key Laboratory of Virology of China (2021IOV004), and the National Natural Science Foundation of China (31621061).\nOpen AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visithttp://creativecommons.org/licenses/by/4.0/.Reprints and permissions\nTo date, the number of confirmed coronavirus disease 2019 (COVID-19) cases has surpassed 100 million, with deaths exceeding 2 million, yet the mechanism by which severe acute respiratory syndrome coronavirus (SARS-CoV)-2 attacks the body remains unclear. Although SARS-CoV-2 is known to primarily target the lung, it is also believed to cause multi-organ dysfunction and comprehensive studies on SARS-CoV-2 cell tropism in humans are lacking. SARS-CoV-2 exploits the host angiotensin-converting enzyme 2 (ACE2) as its receptor for cell entry1, but the correlation between SARS-CoV-2 organ/cell tropism and ACE2 distribution is unclear. Here, we studied these issues via a systemic analysis of postmortem specimens from a 66-year-old female COVID-19 patient who had rapidly developed multiorgan failure. The patient died in the hospital on Day 13 of admission (Day 16 of illness) and her autopsy was performed at 8 h after death.\nIn conclusion, our results identified SARS-CoV-2 cell tropism in multiple organs (Fig.1e), indicating that SARS-CoV-2 infects not only the respiratory system (e.g., lungs and trachea) but also the kidneys, small intestines, pancreas, blood vessels, and other tissues. Recently, we disclosed that SARS-CoV-2 also targeted sweat glands and vascular endothelial cells in the skin15. These findings suggest that direct viral infection could, at least partially, contribute to multiorgan injury. Our results also proved a possible correlation between SARS-CoV-2 organotropism and ACE2 distribution, providing supporting evidence for the multiorgan infection of the virus. While our study was limited by its size, its sheds new light into the mechanism of viral infection/transmission and provides valuable information for COVID-19 control.\nIn the trachea, epithelial marker KRT7 clearly revealed viral infection in epithelial cells of the mucosa (Supplementary Fig.S3a-i), conduits, and glands (Supplementary Fig.S3a-ii). Among mucosal epithelial cells, viral antigens were found in trefoil factor 1-positiv"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R191261",
        "research_problem": "Representation Learning",
        "orkg_properties": "['metric', 'method', 'On evaluation dataset', 'research problem', 'Has evaluation task']",
        "nechakhin_result": "['neural networks', 'unsupervised learning', 'feature extraction', 'dimensionality reduction', 'clustering', 'deep learning', 'autoencoders', 'generative models']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "LinkBERT: Pretraining Language Models with Document Links",
        "abstract": "Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R191297",
        "research_problem": "Representation Learning on Biomedical Data",
        "orkg_properties": "['metric', 'method', 'On evaluation dataset', 'research problem', 'Has evaluation task']",
        "nechakhin_result": "['biomedical domain', 'representation learning', 'data', 'algorithm', 'techniques', 'deep learning', 'machine learning', 'feature extraction', 'dimensionality reduction', 'classification', 'clustering', 'similarity measures', 'evaluation metrics']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Publicly Available Clinical BERT Embeddings",
        "abstract": "Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189554",
        "research_problem": "Mechanical properties of nacre-inspired materials",
        "orkg_properties": "['method', 'result', 'research problem', 'has material']",
        "nechakhin_result": "['Material composition', 'Nacre structure', 'Mechanical testing methods', 'Stress-strain behavior', 'Strength and toughness', 'Material processing techniques', 'Microstructure', 'Biocompatibility', 'Environmental conditions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "A Constrained Assembly Strategy for High-Strength Natural Nanoclay Film",
        "abstract": "Click to copy section linkSection link copied!High Resolution ImageDownload MS PowerPoint SlideDeveloping high-performance materials from existing natural materials is highly desired because of their environmental friendliness and low cost; two-dimensional nanoclay exfoliated from layered silicate minerals is a good building block to construct multilayered macroscopic assemblies for achieving high mechanical and functional properties. Nevertheless, the efforts have been frustrated by insufficient inter-nanosheet stress transfer and nanosheet misalignment caused by capillary force during solution-based spontaneous assembly, degrading the mechanical strength of clay-based materials. Herein, a constrained assembly strategy that is implemented by in-plane stretching a robust water-containing nanoclay network with hydrogen and ionic bonding is developed to adjust the 2D topography of nanosheets within multilayered nanoclay film. In-plane stretching overcomes capillary force during water removal and thus restrains nanosheet conformation transition from nearly flat to wrinkled, leading to a highly aligned multilayered nanostructure with synergistic hydrogen and ionic bonding. It is proved that inter-nanosheet hydrogen and ionic bonding and nanosheet conformation extension generate profound mechanical reinforcement. The tensile strength and modulus of natural nanoclay film reach up to 429.0 MPa and 43.8 GPa and surpass the counterparts fabricated by normal spontaneous assembly. Additionally, improved heat insulation function and good nonflammability are shown for the natural nanoclay film and extend its potential for realistic uses.This publication is licensed under the terms of your\n                            institutional subscription.Request reuse permissions."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R178353",
        "research_problem": "Barrier properties of nanocomposite coating",
        "orkg_properties": "['method', 'result', 'research problem', 'has material']",
        "nechakhin_result": "['Nanocomposite materials',\n 'Coating technology',\n 'Barrier properties',\n 'Film thickness',\n 'Material composition',\n 'Nanoparticle dispersion',\n 'Surface roughness',\n 'Coating morphology',\n 'Adhesion strength',\n 'Permeability',\n 'Chemical resistance',\n 'Temperature stability',\n 'Humidity resistance',\n 'Mechanical properties',\n 'Curing conditions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Can high oxygen and water vapor barrier nanocomposite coatings be obtained with a waterborne formulation?",
        "abstract": "Modification of synthetic,high aspect ratioclay with 6-aminocaprohydroxamic acid hydrochloride pushes the interaction between thepolyvinyl alcohol(PVA) matrix and the filler to the level where the waterbornenanocompositebecomes rather insensitive to swelling, even at an elevatedrelative humidity(RH). The modifier can form stronghydrogen bondswith the hydroxyl groups of PVA via the hydroxamic acid functional group. This prevents the swelling of crystalline PVA domains. Perfectly texturednanocomposite filmsare obtained by spraying polymer-filler suspensions. The combination of the various effects shifts the onset of significant swelling of the nanocomposites to high RH regions. Even at 90% RH, surprisingly low oxygen andwater vaportransmission rates (0.11cm3m−2day−1bar−1and 0.18gm−2day−1, respectively, for a coating of 0.42µm) are observed that may render PVA-based, waterborne coatings interesting for food packaging applications.\nHigh performance poly(vinyl alcohol) (PVA)/lignin nanomicelle (LNM) nanocomposite films with good vapor barrier and advanced UV-shielding properties were fabricated in this study. LNM was homogeneously distributed in the PVA matrix and strong robust hydrogen bonds were successfully constructed between LNM and PVA matrix. With only 5 wt% loading of LNM into the PVA/LNM nanocomposite, the water vapor transmission rate (WVTR) was declined by about 189% compared with pure PVA. Moreover, after introducing lignin, the PVA nanocomposite films showed improved tensile strength and toughness, excellent UV-blocking and good thermal stability. As both lignin and PVA are biodegradable, this study shows a meaningful design approach for biodegradable functional nanocomposite films using cheap and easily available biomass and biodegradable raw materials.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R197288",
        "research_problem": "transformer model",
        "orkg_properties": "['has model', 'Pretraining Architecture', 'Pretraining Task', 'Optimizer', 'organization', 'research problem', 'Model Family', 'license']",
        "nechakhin_result": "['NLP', 'deep learning', 'language modeling', 'attention mechanism', 'neural network', 'natural language processing', 'sequence to sequence', 'BERT', 'GPT', 'transformer architecture', 'self-attention', 'position encoding', 'pre-training', 'fine-tuning']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 3,
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "abstract": ":Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R575541",
        "research_problem": "Large Language Models (LLMs)",
        "orkg_properties": "['has model', 'Pretraining Architecture', 'Pretraining Task', 'Optimizer', 'organization', 'research problem', 'Model Family', 'license']",
        "nechakhin_result": "['Natural Language Processing',\n 'Machine Learning',\n 'Deep Learning',\n 'Text Mining',\n 'Information Retrieval',\n 'Text Generation',\n 'Language Understanding',\n 'Language Modeling',\n 'Semantic Analysis',\n 'Sequence Modeling',\n 'Pre-training and Fine-tuning']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "ERNIE: Enhanced Language Representation with Informative Entities",
        "abstract": "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R213454",
        "research_problem": "Named Entity Recognition",
        "orkg_properties": "['data source', 'research problem', 'Concept types', 'Data Domain', 'inter-annotator agreement', 'genre', 'inLanguage']",
        "nechakhin_result": "['NLP technique', 'Text analysis', 'Machine learning', 'Feature extraction', 'Semantic understanding', 'Named entity types', 'Entity detection', 'Language processing', 'Information extraction', 'Text classification', 'Training data', 'Supervised learning', 'Unsupervised learning', 'Deep learning', 'Semi-supervised learning', 'Rule-based approaches', 'Evaluation metrics', 'Data preprocessing', 'Entity linking', 'Word embeddings', 'Contextual word representations']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 3,
        "title": "Named Entity Recognition for Hindi-English Code-Mixed Social Media Text",
        "abstract": "Named Entity Recognition (NER) is a major task in the field of Natural Language Processing (NLP), and also is a sub-task of Information Extraction. The challenge of NER for tweets lie in the insufficient information available in a tweet. There has been a significant amount of work done related to entity extraction, but only for resource rich languages and domains such as newswire. Entity extraction is, in general, a challenging task for such an informal text, and code-mixed text further complicates the process with it’s unstructured and incomplete information. We propose experiments with different machine learning classification algorithms with word, character and lexical features. The algorithms we experimented with are Decision tree, Long Short-Term Memory (LSTM), and Conditional Random Field (CRF). In this paper, we present a corpus for NER in Hindi-English Code-Mixed along with extensive experiments on our machine learning models which achieved the best f1-score of 0.95 with both CRF and LSTM."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162364",
        "research_problem": "Protein interaction theme article retrieval",
        "orkg_properties": "['data source', 'research problem', 'Data coverage']",
        "nechakhin_result": "['Protein interaction databases',\n 'Biological networks',\n 'Protein-protein interaction prediction',\n 'Machine learning',\n 'Data mining',\n 'Text mining',\n 'Protein interaction networks',\n 'Functional proteomics',\n 'Protein interaction analysis',\n 'Gene ontology',\n 'Pathway analysis',\n 'Protein interaction visualization',\n 'Protein structure analysis']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Overview of the protein-protein interaction annotation extraction task of BioCreative II",
        "abstract": "Background:The biomedical literature is the primary information source for manual protein-protein interaction annotations. Text-mining systems have been implemented to extract binary protein interactions from articles, but a comprehensive comparison between the different techniques as well as with manual curation was missing.Results:We designed a community challenge, the BioCreative II protein-protein interaction (PPI) task, based on the main steps of a manual protein interaction annotation workflow. It was structured into four distinct subtasks related to: (a) detection of protein interaction-relevant articles; (b) extraction and normalization of protein interaction pairs; (c) retrieval of the interaction detection methods used; and (d) retrieval of actual text passages that provide evidence for protein interactions. A total of 26 teams submitted runs for at least one of the proposed subtasks. In the interaction article detection subtask, the top scoring team reached an F-score of 0.78. In the interaction pair extraction and mapping to SwissProt, a precision of 0.37 (with recall of 0.33) was obtained. For associating articles with an experimental interaction detection method, an F-score of 0.65 was achieved. As for the retrieval of the PPI passages best summarizing a given protein interaction in full-text articles, 19% of the submissions returned by one of the runs corresponded to curator-selected sentences. Curators extracted only the passages that best summarized a given interaction, implying that many of the automatically extracted ones could contain interaction information but did not correspond to the most informative sentences.Conclusion:The BioCreative II PPI task is the first attempt to compare the performance of text-mining tools specific for each of the basic steps of the PPI extraction pipeline. The challenges identified range from problems in full-text format conversion of articles to difficulties in detecting interactor protein pairs and then linking them to their database records. Some limitations were also encountered when using a single (and possibly incomplete) reference database for protein normalization or when limiting search for interactor proteins to co-occurrence within a single sentence, when a mention might span neighboring sentences. Finally, distinguishing between novel, experimentally verified interactions (annotation relevant) and previously known interactions adds additional complexity to these tasks.\nPhysical protein-protein interactions have been studied extensively because of their crucial role in controlling central biological processes such as cell division and their implications in a range of human diseases including cancer. A collection of experimental techniques is available to characterize protein-protein interactions; some of them are more suitable to determine stable complexes whereas others are generally considered better for detecting transient interactions. The use of large-scale proteomics approaches for experimentally obtaining protein interaction information has resulted in an additional source of interaction data. Also, bioinformatics techniques based on sequence, structural, or evolutionary information have been devised to predict binary protein interactions.To capture and provide efficient access to the underlying information, structured interaction annotations have been stored in public databases. These databases vary in annotation depth and type of interactions, but a common character"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162369",
        "research_problem": "Protein Interaction Article Classification Task",
        "orkg_properties": "['research problem', 'Data coverage']",
        "nechakhin_result": "['Protein', 'Interaction', 'Article', 'Classification', 'Task']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "The Protein-Protein Interaction tasks of BioCreative III: classification/ranking of articles and linking bio-ontology concepts to full text",
        "abstract": "BackgroundDetermining usefulness of biomedical text mining systems requires realistic task definition and data selection criteria without artificial constraints, measuring performance aspects that go beyond traditional metrics. The BioCreative III Protein-Protein Interaction (PPI) tasks were motivated by such considerations, trying to address aspects including how the end user would oversee the generated output, for instance by providing ranked results, textual evidence for human interpretation or measuring time savings by using automated systems. Detecting articles describing complex biological events like PPIs was addressed in the Article Classification Task (ACT), where participants were asked to implement tools for detecting PPI-describing abstracts. Therefore the BCIII-ACT corpus was provided, which includes a training, development and test set of over 12,000 PPI relevant and non-relevant PubMed abstracts labeled manually by domain experts and recording also the human classification times. The Interaction Method Task (IMT) went beyond abstracts and required mining for associations between more than 3,500 full text articles and interaction detection method ontology concepts that had been applied to detect the PPIs reported in them.ResultsA total of 11 teams participated in at least one of the two PPI tasks (10 in ACT and 8 in the IMT) and a total of 62 persons were involved either as participants or in preparing data sets/evaluating these tasks. Per task, each team was allowed to submit five runs offline and another five online via the BioCreative Meta-Server. From the 52 runs submitted for the ACT, the highest Matthew's Correlation Coefficient (MCC) score measured was 0.55 at an accuracy of 89% and the best AUC iP/R was 68%. Most ACT teams explored machine learning methods, some of them also used lexical resources like MeSH terms, PSI-MI concepts or particular lists of verbs and nouns, some integrated NER approaches. For the IMT, a total of 42 runs were evaluated by comparing systems against manually generated annotations done by curators from the BioGRID and MINT databases. The highest AUC iP/R achieved by any run was 53%, the best MCC score 0.55. In case of competitive systems with an acceptable recall (above 35%) the macro-averaged precision ranged between 50% and 80%, with a maximum F-Score of 55%.ConclusionsThe results of the ACT task of BioCreative III indicate that classification of large unbalanced article collections reflecting the real class imbalance is still challenging. Nevertheless, text-mining tools that report ranked lists of relevant articles for manual selection can potentially reduce the time needed to identify half of the relevant articles to less than 1/4 of the time when compared to unranked results. Detecting associations between full text articles and interaction detection method PSI-MI terms (IMT) is more difficult than might be anticipated. This is due to the variability of method term mentions, errors resulting from pre-processing of articles provided as PDF files, and the heterogeneity and different granularity of method term concepts encountered in the ontology. However, combining the sophisticated techniques developed by the participants with supporting evidence strings derived from the articles for human interpretation could result in practical modules for biological annotation workflows.\nCompeting interestsThe authors declare that they have no competing interests.Authors' contributionsMK, MV and FL o"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162526",
        "research_problem": "Human kinome full-text triage",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains']",
        "nechakhin_result": "['Kinome', 'Human', 'Triage', 'Full-text', 'Research problem']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Overview of the BioCreative VI text-mining services for Kinome Curation Track",
        "abstract": "The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants’ systems had to identify and rank relevant articles in a collection of 5.2 M MEDLINE citations (task 1) or 530 000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162526",
        "research_problem": "Human kinome abstracts triage",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains']",
        "nechakhin_result": "['Kinome',\n 'Human',\n 'Abstracts',\n 'Triage']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Overview of the BioCreative VI text-mining services for Kinome Curation Track",
        "abstract": "The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants’ systems had to identify and rank relevant articles in a collection of 5.2 M MEDLINE citations (task 1) or 530 000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162546",
        "research_problem": "Identification and ranking of relevant PubMed citations describing protein–protein interactions (PPIs) affected by mutations",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains']",
        "nechakhin_result": "['PubMed citations', 'Protein–protein interactions (PPIs)', 'Mutations']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Overview of the BioCreative VI Precision Medicine Track: mining protein interactions and mutations for precision medicine",
        "abstract": "The Precision Medicine Initiative is a multicenter effort aiming at formulating personalized treatments leveraging on individual patient data (clinical, genome sequence and functional genomic data) together with the information in large knowledge bases (KBs) that integrate genome annotation, disease association studies, electronic health records and other data types. The biomedical literature provides a rich foundation for populating these\nKBs, reporting genetic and molecular interactions that provide the scaffold for the cellular regulatory systems and detailing the influence of genetic variants in these interactions. The goal of BioCreative VI Precision Medicine Track was to extract this particular type of information and was organized in two tasks: (i) document triage task, focused on identifying scientific literature containing experimentally verified protein–protein interactions (PPIs) affected by genetic mutations and (ii) relation extraction task, focused on extracting the affected interactions (protein pairs). To assist system developers and task participants, a large-scale corpus of PubMed documents was manually annotated for this task. Ten teams worldwide contributed 22 distinct text-mining models for the document triage task, and six teams worldwide contributed 14 different text-mining systems for the relation extraction task. When comparing the text-mining system predictions with human annotations, for the triage task, the best F-score was 69.06%, the best precision was 62.89%, the best recall was 98.0% and the best average precision was 72.5%. For the relation extraction task, when taking homologous genes into account, the best F-score was 37.73%, the best precision was 46.5% and the best recall was 54.1%. Submitted systems explored a wide range of methods, from traditional rule-based, statistical and machine learning systems to state-of-the-art deep learning methods. Given the level of participation and the individual team results we find the precision medicine track to be successful in engaging the text-mining research community. In the meantime, the track produced a manually annotated corpus of 5509 PubMed documents developed by BioGRID curators and relevant for precision medicine. The data set is freely available to the community, and the specific interactions have been integrated into the BioGRID data set. In addition, this challenge provided the first results of automatically identifying PubMed articles that describe PPI affected by mutations, as well as extracting the affected relations from those articles. Still, much progress is needed for computer-assisted precision medicine text mining to become mainstream. Future work should focus on addressing the remaining technical challenges and incorporating the practical benefits of text-mining tools into real-world precision medicine information-related curation."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162352",
        "research_problem": "extract text fragments to support the annotation of a given protein- GO term association",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains', 'Coarse-grained Entity types', 'Number of training data mentions', 'Number of test data mentions', 'Annotation type']",
        "nechakhin_result": "['Protein sequence',\n 'GO term',\n 'Text fragments',\n 'Annotation',\n 'Association']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Evaluation of BioCreAtIvE assessment of task 2",
        "abstract": "BackgroundMolecular Biology accumulated substantial amounts of data concerning functions of genes and proteins. Information relating to functional descriptions is generally extracted manually from textual data and stored in biological databases to build up annotations for large collections of gene products. Those annotation databases are crucial for the interpretation of large scale analysis approaches using bioinformatics or experimental techniques. Due to the growing accumulation of functional descriptions in biomedical literature the need for text mining tools to facilitate the extraction of such annotations is urgent. In order to make text mining tools useable in real world scenarios, for instance to assist database curators during annotation of protein function, comparisons and evaluations of different approaches on full text articles are needed.ResultsThe Critical Assessment for Information Extraction in Biology (BioCreAtIvE) contest consists of a community wide competition aiming to evaluate different strategies for text mining tools, as applied to biomedical literature. We report on task two which addressed the automatic extraction and assignment of Gene Ontology (GO) annotations of human proteins, using full text articles. The predictions of task 2 are based on triplets ofprotein – GO term – article passage. The annotation-relevant text passages were returned by the participants and evaluated by expert curators of the GO annotation (GOA) team at the European Institute of Bioinformatics (EBI). Each participant could submit up to three results for each sub-task comprising task 2. In total more than 15,000 individual results were provided by the participants. The curators evaluated in addition to the annotation itself, whether the protein and the GO term were correctly predicted and traceable through the submitted text fragment.ConclusionConcepts provided by GO are currently the most extended set of terms used for annotating gene products, thus they were explored to assess how effectively text mining tools are able to extract those annotations automatically. Although the obtained results are promising, they are still far from reaching the required performance demanded by real world applications. Among the principal difficulties encountered to address the proposed task, were the complex nature of the GO terms and protein names (the large range of variants which are used to express proteins and especially GO terms in free text), and the lack of a standard training set. A range of very different strategies were used to tackle this task. The dataset generated in line with the BioCreative challenge is publicly available and will allow new possibilities for training information extraction methods in the domain of molecular biology.\nThe dataset produced at the BioCreative contest task two is freely available from:http://www.pdg.cnb.uam.es/BioLINK/BioCreative.eval.html[18] and is given in as an XML-like format. From the nine registered users who participated in task 2.1, a total of 15,992 evidence passages were provided to the curators. Out of those, 12,014 corresponded to the requested queries (the rest corresponded to new predictions which were not contained in the test set). On average 11.34 (standard deviation of 2.30) submissions of annotation predictions were sent for each single query triplet across all the user submissions (21 runs). Users submitted between a single run up to the maximum of three runs allowed; there were 21 runs s"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162364",
        "research_problem": "Protein interaction description sentences retrieval",
        "orkg_properties": "['data source', 'research problem', 'Data coverage']",
        "nechakhin_result": "['Protein domain',\n 'Protein structure',\n 'Protein function',\n 'Protein interaction type',\n 'Protein interaction strength',\n 'Protein interaction network',\n 'Protein interaction database',\n 'Text mining',\n 'Natural language processing',\n 'Semantic similarity',\n 'Machine learning',\n 'Information retrieval']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Overview of the protein-protein interaction annotation extraction task of BioCreative II",
        "abstract": "Background:The biomedical literature is the primary information source for manual protein-protein interaction annotations. Text-mining systems have been implemented to extract binary protein interactions from articles, but a comprehensive comparison between the different techniques as well as with manual curation was missing.Results:We designed a community challenge, the BioCreative II protein-protein interaction (PPI) task, based on the main steps of a manual protein interaction annotation workflow. It was structured into four distinct subtasks related to: (a) detection of protein interaction-relevant articles; (b) extraction and normalization of protein interaction pairs; (c) retrieval of the interaction detection methods used; and (d) retrieval of actual text passages that provide evidence for protein interactions. A total of 26 teams submitted runs for at least one of the proposed subtasks. In the interaction article detection subtask, the top scoring team reached an F-score of 0.78. In the interaction pair extraction and mapping to SwissProt, a precision of 0.37 (with recall of 0.33) was obtained. For associating articles with an experimental interaction detection method, an F-score of 0.65 was achieved. As for the retrieval of the PPI passages best summarizing a given protein interaction in full-text articles, 19% of the submissions returned by one of the runs corresponded to curator-selected sentences. Curators extracted only the passages that best summarized a given interaction, implying that many of the automatically extracted ones could contain interaction information but did not correspond to the most informative sentences.Conclusion:The BioCreative II PPI task is the first attempt to compare the performance of text-mining tools specific for each of the basic steps of the PPI extraction pipeline. The challenges identified range from problems in full-text format conversion of articles to difficulties in detecting interactor protein pairs and then linking them to their database records. Some limitations were also encountered when using a single (and possibly incomplete) reference database for protein normalization or when limiting search for interactor proteins to co-occurrence within a single sentence, when a mention might span neighboring sentences. Finally, distinguishing between novel, experimentally verified interactions (annotation relevant) and previously known interactions adds additional complexity to these tasks.\nPhysical protein-protein interactions have been studied extensively because of their crucial role in controlling central biological processes such as cell division and their implications in a range of human diseases including cancer. A collection of experimental techniques is available to characterize protein-protein interactions; some of them are more suitable to determine stable complexes whereas others are generally considered better for detecting transient interactions. The use of large-scale proteomics approaches for experimentally obtaining protein interaction information has resulted in an additional source of interaction data. Also, bioinformatics techniques based on sequence, structural, or evolutionary information have been devised to predict binary protein interactions.To capture and provide efficient access to the underlying information, structured interaction annotations have been stored in public databases. These databases vary in annotation depth and type of interactions, but a common character"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162482",
        "research_problem": "Retrieve evidence sentences for a BEL statement",
        "orkg_properties": "['research problem', 'Data coverage']",
        "nechakhin_result": "[\"biological evidence\", \"BEL statement\", \"text mining\", \"information retrieval\", \"natural language processing\"]",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "BioCreative V track 4: a shared task for the extraction of causal network information using the Biological Expression Language",
        "abstract": "Automatic extraction of biological network information is one of the most desired and most complex tasks in biological and medical text mining. Track 4 at BioCreative V attempts to approach this complexity using fragments of large-scale manually curated biological networks, represented in Biological Expression Language (BEL), as training and test data. BEL is an advanced knowledge representation format which has been designed to be both human readable and machine processable. The specific goal of track 4 was to evaluate text mining systems capable of automatically constructing BEL statements from given evidence text, and of retrieving evidence text for given BEL statements. Given the complexity of the task, we designed an evaluation methodology which gives credit to partially correct statements. We identified various levels of information expressed by BEL statements, such as entities, functions, relations, and introduced an evaluation framework which rewards systems capable of delivering useful BEL fragments at each of these levels. The aim of this evaluation method is to help identify the characteristics of the systems which, if combined, would be most useful for achieving the overall goal of automatically constructing causal biological networks from text."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162526",
        "research_problem": "Human kinase snippet extraction",
        "orkg_properties": "['research problem', 'Data coverage', 'Data domains', 'Coarse-grained Entity types']",
        "nechakhin_result": "['Kinase activity', 'Human subjects', 'Biological pathways', 'Protein phosphorylation', 'Enzyme inhibitors', 'Protein kinases', 'Cancer research', 'Signal transduction', 'Protein-protein interactions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Overview of the BioCreative VI text-mining services for Kinome Curation Track",
        "abstract": "The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants’ systems had to identify and rank relevant articles in a collection of 5.2 M MEDLINE citations (task 1) or 530 000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R171842",
        "research_problem": "Retrieving GO evidence sentences for relevant genes",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains', 'Annotation type']",
        "nechakhin_result": "['Gene ID', 'GO ID', 'Evidence Code', 'Evidence Sentence']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "BC4GO: a full-text corpus for the BioCreative IV GO task",
        "abstract": "Gene function curation via Gene Ontology (GO) annotation is a common task among Model Organism Database groups. Owing to its manual nature, this task is considered one of the bottlenecks in literature curation. There have been many previous attempts at automatic identification of GO terms and supporting information from full text. However, few systems have delivered an accuracy that is comparable with humans. One recognized challenge in developing such systems is the lack of marked sentence-level evidence text that provides the basis for making GO annotations. We aim to create a corpus that includes the GO evidence text along with the three core elements of GO annotations: (i) a gene or gene product, (ii) a GO term and (iii) a GO evidence code. To ensure our results are consistent with real-life GO data, we recruited eight professional GO curators and asked them to follow their routine GO annotation protocols. Our annotators marked up more than 5000 text passages in 200 articles for 1356 distinct GO terms. For evidence sentence selection, the inter-annotator agreement (IAA) results are 9.3% (strict) and 42.7% (relaxed) in F1-measures. For GO term selection, the IAAs are 47% (strict) and 62.9% (hierarchical). Our corpus analysis further shows that abstracts contain ∼10% of relevant evidence sentences and 30% distinct GO terms, while the Results/Experiment section has nearly 60% relevant sentences and >70% GO terms. Further, of those evidence sentences found in abstracts, less than one-third contain enough experimental detail to fulfill the three core criteria of a GO annotation. This result demonstrates the need of using full-text articles for text mining GO annotations. Through its use at the BioCreative IV GO (BC4GO) task, we expect our corpus to become a valuable resource for the BioNLP research community.Database URL:http://www.biocreative.org/resources/corpora/bc-iv-go-task-corpus/."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162352",
        "research_problem": "automatically assigning GO terms to protein and article pairs",
        "orkg_properties": "['data source', 'research problem', 'Data Domain', 'Data coverage', 'Ontology used', 'Number of training data mentions', 'Number of test data mentions', 'Annotation type']",
        "nechakhin_result": "['protein similarity', 'article content', 'gene ontology (GO) terms', 'semantic similarity', 'classification algorithm', 'text mining', 'machine learning', 'knowledge base']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Evaluation of BioCreAtIvE assessment of task 2",
        "abstract": "BackgroundMolecular Biology accumulated substantial amounts of data concerning functions of genes and proteins. Information relating to functional descriptions is generally extracted manually from textual data and stored in biological databases to build up annotations for large collections of gene products. Those annotation databases are crucial for the interpretation of large scale analysis approaches using bioinformatics or experimental techniques. Due to the growing accumulation of functional descriptions in biomedical literature the need for text mining tools to facilitate the extraction of such annotations is urgent. In order to make text mining tools useable in real world scenarios, for instance to assist database curators during annotation of protein function, comparisons and evaluations of different approaches on full text articles are needed.ResultsThe Critical Assessment for Information Extraction in Biology (BioCreAtIvE) contest consists of a community wide competition aiming to evaluate different strategies for text mining tools, as applied to biomedical literature. We report on task two which addressed the automatic extraction and assignment of Gene Ontology (GO) annotations of human proteins, using full text articles. The predictions of task 2 are based on triplets ofprotein – GO term – article passage. The annotation-relevant text passages were returned by the participants and evaluated by expert curators of the GO annotation (GOA) team at the European Institute of Bioinformatics (EBI). Each participant could submit up to three results for each sub-task comprising task 2. In total more than 15,000 individual results were provided by the participants. The curators evaluated in addition to the annotation itself, whether the protein and the GO term were correctly predicted and traceable through the submitted text fragment.ConclusionConcepts provided by GO are currently the most extended set of terms used for annotating gene products, thus they were explored to assess how effectively text mining tools are able to extract those annotations automatically. Although the obtained results are promising, they are still far from reaching the required performance demanded by real world applications. Among the principal difficulties encountered to address the proposed task, were the complex nature of the GO terms and protein names (the large range of variants which are used to express proteins and especially GO terms in free text), and the lack of a standard training set. A range of very different strategies were used to tackle this task. The dataset generated in line with the BioCreative challenge is publicly available and will allow new possibilities for training information extraction methods in the domain of molecular biology.\nThe dataset produced at the BioCreative contest task two is freely available from:http://www.pdg.cnb.uam.es/BioLINK/BioCreative.eval.html[18] and is given in as an XML-like format. From the nine registered users who participated in task 2.1, a total of 15,992 evidence passages were provided to the curators. Out of those, 12,014 corresponded to the requested queries (the rest corresponded to new predictions which were not contained in the test set). On average 11.34 (standard deviation of 2.30) submissions of annotation predictions were sent for each single query triplet across all the user submissions (21 runs). Users submitted between a single run up to the maximum of three runs allowed; there were 21 runs s"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162360",
        "research_problem": "gene name normalization",
        "orkg_properties": "['data source', 'research problem', 'Dataset name', 'Data coverage', 'Coarse-grained Entity types', 'Ontology used', 'Annotation type']",
        "nechakhin_result": "['gene ontology', 'gene expression', 'gene sequences', 'gene function', 'gene mutations', 'gene interactions', 'gene regulation', 'gene variants']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Overview of BioCreative II gene normalization",
        "abstract": "Background:The goal of the gene normalization task is to link genes or gene products mentioned in the literature to biological databases. This is a key step in an accurate search of the biological literature. It is a challenging task, even for the human expert; genes are often described rather than referred to by gene symbol and, confusingly, one gene name may refer to different genes (often from different organisms). For BioCreative II, the task was to list the Entrez Gene identifiers for human genes or gene products mentioned in PubMed/MEDLINE abstracts. We selected abstracts associated with articles previously curated for human genes. We provided 281 expert-annotated abstracts containing 684 gene identifiers for training, and a blind test set of 262 documents containing 785 identifiers, with a gold standard created by expert annotators. Inter-annotator agreement was measured at over 90%.Results:Twenty groups submitted one to three runs each, for a total of 54 runs. Three systems achieved F-measures (balanced precision and recall) between 0.80 and 0.81. Combining the system outputs using simple voting schemes and classifiers obtained improved results; the best composite system achieved an F-measure of 0.92 with 10-fold cross-validation. A 'maximum recall' system based on the pooled responses of all participants gave a recall of 0.97 (with precision 0.23), identifying 763 out of 785 identifiers.Conclusion:Major advances for the BioCreative II gene normalization task include broader participation (20 versus 8 teams) and a pooled system performance comparable to human experts, at over 90% agreement. These results show promise as tools to link the literature with biological databases.\nPerformance on the BioCreative II GN task demonstrates progress since the first BioCreative workshop in 2004. The results obtained for human gene/protein identification are comparable to results obtained earlier for mouse and fly; three teams achieved an F-measure of 0.80 or above for one of their runs. However, there is significant progress along several new dimensions. First, the assessment involved 20 groups, as compared with eight groups for BioCreative I. The results achieved by combining input from all of the participating systems outperformed any single system, achieving F-measures from 0.85 to 0.92, depending on the method of combination.The participating teams explored the 'solution space' for this challenge evaluation well. Four teams incorporated explicit handling of conjunction and enumeration; this no longer seems to be a significant cause of loss in recall. The 'maximum recall' system achieved a recall of 96.2% (precision 23.1%). A number of groups did contrastive studies on the utility of adding lexical resources and contextual resources, and on the benefits of lexicon curation. The participants also explored novel approaches to the matching of mentions and lexical resources, and there was significant exploration of contextual models for disambiguation and removal of false positives. An interesting finding was that many groups did not feel the need for large training corpora, especially those using lexicon-based approaches.What does this mean in terms of practical performance? Performance depends on a number of factors: the quality and completeness of the lexical resources; the selection criteria of the articles, including date, journal, domain, and whether they are likely to contain curatable information; the amount of both intra-species and "
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162364",
        "research_problem": "Protein-protein interaction confirmation experiment name recognition and normalization",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Ontology used', 'Annotation type']",
        "nechakhin_result": "['Protein-protein interaction experiment type', 'Name recognition method', 'Normalization method']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Overview of the protein-protein interaction annotation extraction task of BioCreative II",
        "abstract": "Background:The biomedical literature is the primary information source for manual protein-protein interaction annotations. Text-mining systems have been implemented to extract binary protein interactions from articles, but a comprehensive comparison between the different techniques as well as with manual curation was missing.Results:We designed a community challenge, the BioCreative II protein-protein interaction (PPI) task, based on the main steps of a manual protein interaction annotation workflow. It was structured into four distinct subtasks related to: (a) detection of protein interaction-relevant articles; (b) extraction and normalization of protein interaction pairs; (c) retrieval of the interaction detection methods used; and (d) retrieval of actual text passages that provide evidence for protein interactions. A total of 26 teams submitted runs for at least one of the proposed subtasks. In the interaction article detection subtask, the top scoring team reached an F-score of 0.78. In the interaction pair extraction and mapping to SwissProt, a precision of 0.37 (with recall of 0.33) was obtained. For associating articles with an experimental interaction detection method, an F-score of 0.65 was achieved. As for the retrieval of the PPI passages best summarizing a given protein interaction in full-text articles, 19% of the submissions returned by one of the runs corresponded to curator-selected sentences. Curators extracted only the passages that best summarized a given interaction, implying that many of the automatically extracted ones could contain interaction information but did not correspond to the most informative sentences.Conclusion:The BioCreative II PPI task is the first attempt to compare the performance of text-mining tools specific for each of the basic steps of the PPI extraction pipeline. The challenges identified range from problems in full-text format conversion of articles to difficulties in detecting interactor protein pairs and then linking them to their database records. Some limitations were also encountered when using a single (and possibly incomplete) reference database for protein normalization or when limiting search for interactor proteins to co-occurrence within a single sentence, when a mention might span neighboring sentences. Finally, distinguishing between novel, experimentally verified interactions (annotation relevant) and previously known interactions adds additional complexity to these tasks.\nPhysical protein-protein interactions have been studied extensively because of their crucial role in controlling central biological processes such as cell division and their implications in a range of human diseases including cancer. A collection of experimental techniques is available to characterize protein-protein interactions; some of them are more suitable to determine stable complexes whereas others are generally considered better for detecting transient interactions. The use of large-scale proteomics approaches for experimentally obtaining protein interaction information has resulted in an additional source of interaction data. Also, bioinformatics techniques based on sequence, structural, or evolutionary information have been devised to predict binary protein interactions.To capture and provide efficient access to the underlying information, structured interaction annotations have been stored in public databases. These databases vary in annotation depth and type of interactions, but a common character"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R163875",
        "research_problem": "Software named entity recognition",
        "orkg_properties": "['data source', 'research problem', 'Relation types', 'Data coverage', 'Data domains', 'Entity types', 'Annotation type', 'Software entity types']",
        "nechakhin_result": "['Natural language processing techniques', 'Machine learning algorithms', 'Text mining', 'Information retrieval', 'Text classification', 'Named entity recognition', 'Software engineering']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "The role of software in science: a knowledge graph-based analysis of software mentions in PubMed Central",
        "abstract": "View X "
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R166497",
        "research_problem": "Named entity recognition",
        "orkg_properties": "['model', 'data source', 'research problem', 'Concept types', 'Results', 'Data coverage', 'Data domains', 'inter-annotator agreement', 'Annotation level', 'Generation method', 'Has-statistics', 'inLanguage']",
        "nechakhin_result": "['Natural Language Processing',\n 'Machine Learning',\n 'Information Extraction',\n 'Text Mining',\n 'Language Modeling',\n 'Entity Extraction',\n 'Named Entity Classification',\n 'Deep Learning']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "Softcite dataset: A dataset of software mentions in biomedical and economic research publications",
        "abstract": "Software contributions to academic research are relatively invisible, especially to the formalized scholarly reputation system based on bibliometrics. In this article, we introduce a gold-standard dataset of software mentions from the manual annotation of 4,971 academic PDFs in biomedicine and economics. The dataset is intended to be used for automatic extraction of software mentions from PDF format research publications by supervised learning at scale. We provide a description of the dataset and an extended discussion of its creation process, including improved text conversion of academic PDFs. Finally, we reflect on our challenges and lessons learned during the dataset creation, in hope of encouraging more discussion about creating datasets for machine learning use."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R166504",
        "research_problem": "Bioinformatics databases and software named entity recognition",
        "orkg_properties": "['data source', 'research problem', 'Dataset name', 'Data coverage', 'Data domains', 'Knowledge Base', 'Entity types']",
        "nechakhin_result": "['Bioinformatics databases', 'Software', 'Named entity recognition']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "bioNerDS: exploring bioinformatics’ database and software use through literature mining",
        "abstract": "BackgroundBiology-focused databases and software define bioinformatics and their use is central to computational biology. In such a complex and dynamic field, it is of interest to understand what resources are available, which are used, how much they are used, and for what they are used. While scholarly literature surveys can provide some insights, large-scale computer-based approaches to identify mentions of bioinformatics databases and software from primary literature would automate systematic cataloguing, facilitate the monitoring of usage, and provide the foundations for the recovery of computational methods for analysing biological data, with the long-term aim of identifying best/common practice in different areas of biology.ResultsWe have developed bioNerDS, a named entity recogniser for the recovery of bioinformatics databases and software from primary literature. We identify such entities with an F-measure ranging from 63% to 91% at the mention level and 63-78% at the document level, depending on corpus. Not attaining a higher F-measure is mostly due to high ambiguity in resource naming, which is compounded by the on-going introduction of new resources. To demonstrate the software, we applied bioNerDS to full-text articles from BMC Bioinformatics and Genome Biology. General mention patterns reflect the remit of these journals, highlighting BMC Bioinformatics’s emphasis on new tools and Genome Biology’s greater emphasis on data analysis. The data also illustrates some shifts in resource usage: for example, the past decade has seen R and the Gene Ontology join BLAST and GenBank as the main components in bioinformatics processing.ConclusionsWe demonstrate the feasibility of automatically identifying resource names on a large-scale from the scientific literature and show that the generated data can be used for exploration of bioinformatics database and software usage. For example, our results help to investigate the rate of change in resource usage and corroborate the suspicion that a vast majority of resources are created, but rarely (if ever) used thereafter. bioNerDS is available athttp://bionerds.sourceforge.net/.\nCompeting interestsThe authors declare that they have no competing interests.Authors’ contributionsGD programmed and ran bioNerDS on the Genome Biology and BMC Bioinformatics journal corpora, and drafted the manuscript. AB helped with the data analysis. GN, DLR and RS initially conceptualised the project and provided continual guidance and discussion. All authors read and approved the final manuscript.\nBelow are the links to the authors’ original submitted files for images.Authors’ original file for figure 1Authors’ original file for figure 2Authors’ original file for figure 3Authors’ original file for figure 4Authors’ original file for figure 5Authors’ original file for figure 6Authors’ original file for figure 7\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R186167",
        "research_problem": "SPARQL query optimization",
        "orkg_properties": "['Algorithm', 'research problem', 'Hardware', 'uses benchmark', 'has performance']",
        "nechakhin_result": "['Query language', 'Semantic web', 'Optimization techniques', 'RDF graph', 'Triplestore', 'Query execution', 'Query rewriting', 'Query planning', 'Query evaluation', 'Indexing', 'Join algorithms', 'Parallelization', 'Cost estimation', 'Query performance']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Scalable SPARQL querying of large RDF graphs",
        "abstract": "The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R76786",
        "research_problem": "SPARQL query ",
        "orkg_properties": "['Algorithm', 'research problem', 'uses benchmark', 'has performance']",
        "nechakhin_result": "['Research problem', 'Keywords', 'Title', 'Abstract', 'Authors', 'Publication year', 'Citation count', 'Journal/conference name', 'Methodology', 'Results', 'Conclusion', 'Institution/organization', 'Funding', 'Data source', 'Field/domain', 'Topic', 'References']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "SPARQL basic graph pattern optimization using selectivity estimation",
        "abstract": "In this paper, we formalize the problem of Basic Graph Pattern (BGP) optimization for SPARQL queries and main memory graph implementations of RDF data. We define and analyze the characteristics of heuristics for selectivity-based static BGP optimization. The heuristics range from simple triple pattern variable counting to more sophisticated selectivity estimation techniques. Customized summary statistics for RDF data enable the selectivity estimation of joined triple patterns and the development of efficient heuristics. Using the Lehigh University Benchmark (LUBM), we evaluate the performance of the heuristics for the queries provided by the LUBM and discuss some of them in more details."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R215710",
        "research_problem": "empirical research in requirements engineering",
        "orkg_properties": "['method', 'Database', 'research problem', 'number of papers', 'research_field_investigated', 'time interval', 'venue investigated', 'topic investigated', 'result']",
        "nechakhin_result": "['Research question', 'Methodology', 'Data collection', 'Data analysis', 'Sample size', 'Research context', 'Publication year']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Empirical research methodologies and studies in Requirements Engineering: How far did we come?",
        "abstract": "Since the inception of the RE conference series (1992), both researchers and practitioners in the RE community have acknowledged the significance of empirical evaluation as an instrument to gain knowledge about various aspects of RE phenomena and the validity of our research results. A significant number of empirical studies have been conducted in the search for knowledge about RE problems as well as evidence of successful and less successful application of proposed solutions. This editorial presents the progress empirical RE research has made since 1992. Based on a search in the Scopus digital library, we report from an analysis of peer-reviewed systematic literature reviews and mapping studies to showcase major areas of RE research that use methods from the Empirical Software Engineering paradigm. We summarize prior empirical research in RE and introduce the contributors to this special issue on empirical research methodologies and studies in RE.\nOver the past 20 years, software startups have created many products that have changed human life. Since these companies are creating brand-new products or services, requirements are difficult to gather and highly volatile. Although scientific interest in software development in this context has increased, the studies on requirements engineering in software startups are still scarce and mostly focused on elicitation activities.This study overcomes this gap by answering how requirements engineering practices are performed in this context.We conducted a grounded theory study based on 17 interviews with software startups practitioners.We constructed a model to show that software startups do not follow a single set of practices but, instead, build a custom process, changed throughout the development of the company, combining different practices according to a set of influences (Founders, Software Development Manager, Developers, Market, Business Model and Startup Ecosystem).Our findings show that requirements engineering activities in software startups are similar to those in agile teams, but some steps vary as a consequence of the lack of an accessible customer.\nHighlights•This editorial provides a reflection on past empirical research inRequirements Engineering(RE) and challenges lying ahead.•We describe the growth of empirical RE publications, based on a search in the Scopus digital library.•We compare observations from published mapping studies and systematic literature reviews in RE, with RE topicsin two RE roadmaps.•We propose RE sub-areas where more evaluation of evidence is needed and also call for more careful use of theories in empirical RE research.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R216298",
        "research_problem": "empirical research in software engineering",
        "orkg_properties": "['method', 'research problem', 'number of papers', 'research_field_investigated', 'time interval', 'venue investigated', 'topic investigated', 'result']",
        "nechakhin_result": "['research topic', 'research methodology', 'software engineering domain', 'research objectives', 'research design', 'data collection methods', 'data analysis techniques', 'research outcomes', 'research participants', 'research context']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Empirical Research in Software Engineering — A Literature Survey",
        "abstract": "Empirical research is playing a significant role in software engineering (SE), and it has been applied to evaluate software artifacts and technologies. There have been a great number of empirical research articles published recently. There is also a large research community in empirical software engineering (ESE). In this paper, we identify both the overall landscape and detailed implementations of ESE, and investigate frequently applied empirical methods, targeted research purposes, used data sources, and applied data processing approaches and tools in ESE. The aim is to identify new trends and obtain interesting observations of empirical software engineering across different sub-fields of software engineering. We conduct a mapping study on 538 selected articles from January 2013 to November 2017, with four research questions. We observe that the trend of applying empirical methods in software engineering is continuously increasing and the most commonly applied methods are experiment, case study and survey. Moreover, open source projects are the most frequently used data sources. We also observe that most of researchers have paid attention to the validity and the possibility to replicate their studies. These observations are carefully analyzed and presented as carefully designed diagrams. We also reveal shortcomings and demanded knowledge/strategies in ESE and propose recommendations for researchers.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162731",
        "research_problem": "Application and forming of hard material coatings ",
        "orkg_properties": "['result', 'research problem', 'has material', 'realizes']",
        "nechakhin_result": "['Material type', 'Coating methods', 'Coating properties', 'Application techniques', 'Surface preparation', 'Coating thickness', 'Coating adhesion', 'Coating durability', 'Coating performance', 'Coating structure', 'Coating composition', 'Coating deposition', 'Coating substrate', 'Coating temperature', 'Coating environment']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Cross-wedge rolling of PTA-welded hybrid steel billets with rolling bearing steel and hard material coatings",
        "abstract": "Within the Collaborative Research Centre 1153 “Tailored Forming“ a process chain for the manufacturing of hybrid high performance components is developed. Exemplary process steps consist of deposit welding of high performance steel on low-cost steel, pre-shaping by cross-wedge rolling and finishing by milling.Hard material coatings such as Stellite 6 or Delcrome 253 are used as wear or corrosion protection coatings in industrial applications. Scientists of the Institute of Material Science welded these hard material alloys onto a base material, in this case C22.8, to create a hybrid workpiece. Scientists of the Institut für Integrierte Produktion Hannover have shown that these hybrid workpieces can be formed without defects (e.g. detachment of the coating) by cross-wedge rolling. After forming, the properties of the coatings are retained or in some cases even improved (e.g. the transition zone between base material and coating). By adjustments in the welding process, it was possible to apply the 100Cr6 rolling bearing steel, as of now declared as non-weldable, on the low-cost steel C22.8. 100Cr6 was formed afterwards in its hybrid bonding state with C22.8 by cross-wedge rolling, thus a component-integrated bearing seat was produced. Even after welding and forming, the rolling bearing steel coating could still be quench-hardened to a hardness of over 60 HRC. This paper shows the potential of forming hybrid billets to tailored parts. Since industrially available standard materials can be used for hard material coatings by this approach, even though they are not weldable by conventional methods, it is not necessary to use expensive, for welding designed materials to implement a hybrid component concept.TopicsCorrosion,Alloys,Machining,Welding,Scientific society and organization"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R171846",
        "research_problem": "Lifespan of bearings with various material combinations",
        "orkg_properties": "['result', 'research problem', 'has material', 'realizes']",
        "nechakhin_result": "['Materials of bearings', 'Lifespan of bearings', 'Material combinations', 'Bearing performance', 'Wear and tear of bearings', 'Friction properties of bearing materials', 'Load capacity of bearings', 'Operating conditions of bearings']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Investigation of the material combination 20MnCr5 and X45CrSi9-3 in the Tailored Forming of shafts with bearing seats",
        "abstract": "The Tailored Forming process chain is used to manufacture hybrid components and consists of a joining process or Additive Manufacturing for various materials (e.g. deposition welding), subsequent hot forming, machining and heat treatment. In this way, components can be produced with materials adapted to the load case. For this paper, hybrid shafts are produced by deposition welding of a cladding made of X45CrSi9-3 onto a workpiece made from 20MnCr5. The hybrid shafts are then formed by means of cross-wedge rolling. It is investigated, how the thickness of the cladding and the type of cooling after hot forming (in air or in water) affect the properties of the cladding. The hybrid shafts are formed without layer separation. However, slight core loosening occurres in the area of the bearing seat due to the Mannesmann effect. The microhardness of the cladding is only slightly effected by the cooling strategy, while the microhardness of the base material is significantly higher in water cooled shafts. The microstructure of the cladding after both cooling strategies consists mainly of martensite. In the base material, air cooling results in a mainly ferritic microstructure with grains of ferrite-pearlite. Quenching in water results in a microstructure containing mainly martensite.\nComponents, e.g. shafts, must withstand various chemical, tribological and physical stresses during usage. During operation, some component areas are exposed to high mechanical loads. If the high stresses only occur in the area close to the surface, the use of a cost-intensive, high-strength material for the component can be avoided and the affected area can be cladded with a harder material, e.g. 100Cr6 or X45CrSi9-3, with a thickness of up to several millimeters instead. Various processes are available for applying these high-strength claddings. In order to achieve the final geometry, the cladding is subsequently machined. A disadvantage of cladding is the weld microstructure which is present in the cladding and in the heat-affected zone of the base material after the material is deposited. This can reduce the load capacity in the component area. Mildebrath et al. have demonstrated that the microstructure can be transformed into a fine-grained forming structure by subsequent hot forming [1].\nThe combination of a joining process or Additive Manufacturing with a hot forming process, subsequent machining and, if necessary, heat treatment is known as a Tailored Forming process chain. The process chain is used for the production of hybrid components like axial bearing washers, bushings, shafts and bevel gears. Various material combinations were investigated for the production of Tailored Forming components. The research results on the materials and process steps used in this investigation are presented in this section and the research question is derived.2.1Deposition weldingVarious processes can be used for the production of claddings. The claddings can serve different purposes, such as corrosion or wear resistance. They are also used to improve surface hardness and strength. This is known as hardfacing. The layers can range from 0.5 mm to several millimeters in thickness [2]. Various processes are known for the welding of protective claddings. These processes are laser metal deposition with powder (LMD-P), laser hot-wire cladding (LHWC), laser induction cladding (LIC), gas metal arc welding (GMAW) or plasma transferred arc welding (PTA) [2,3,4,5].The LHWC process is "
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R108199",
        "research_problem": "Focus of Crowd-based Requirements Engineering",
        "orkg_properties": "['research problem', 'RE activities with crowd involvement', 'Utilities in CrowdRE']",
        "nechakhin_result": "['Requirements engineering', 'Crowdsourcing', 'Crowd-based', 'Collective intelligence', 'Crowd wisdom', 'User requirements', 'Collaborative requirements', 'Crowd collaboration', 'Crowd participation', 'Crowd evaluation']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution",
        "abstract": ":Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number---in the range of thousands per day for popular applications---a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R138661",
        "research_problem": "Biomedical research in  Psychiatric Disorders",
        "orkg_properties": "['Data', 'research problem', 'Used models', 'Study cohort', 'Outcome assessment']",
        "nechakhin_result": "['Biomedical approaches', 'Psychiatric disorders', 'Research methodology', 'Clinical trials', 'Neurology', 'Genetics', 'Pharmacology', 'Psychology', 'Mental health', 'Neuroimaging', 'Treatment options', 'Epidemiology', 'Biological markers']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Clinical data Neuroimage data",
        "abstract": ":Effective discrimination of attention deficit hyperactivity disorder (ADHD) using imaging and functional biomarkers would have fundamental influence on public health. In usual, the discrimination is based on the standards of American Psychiatric Association. In this paper, we modified one of the deep learning method on structure and parameters according to the properties of ADHD data, to discriminate ADHD on the unique public dataset of ADHD-200. We predicted the subjects as control, combined, inattentive or hyperactive through their frequency features. The results achieved improvement greatly compared to the performance released by the competition. Besides, the imbalance in datasets of deep learning model influenced the results of classification. As far as we know, it is the first time that the deep learning method has been used for the discrimination of ADHD with fMRI data.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R44731",
        "research_problem": "Determination of the COVID-19 basic reproduction number",
        "orkg_properties": "['Time period', 'Basic reproduction number', 'research problem', 'location']",
        "nechakhin_result": "['COVID-19', 'basic reproduction number', 'determination']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Transmission interval estimates suggest pre-symptomatic spread of COVID-19",
        "abstract": "BackgroundAs the COVID-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. We determine the incubation period and serial interval distribution for transmission clusters in Singapore and in Tianjin. We infer the basic reproduction number and identify the extent of pre-symptomatic transmission.MethodsWe collected outbreak information from Singapore and Tianjin, China, reported from Jan.19-Feb.26 and Jan.21-Feb.27, respectively. We estimated incubation periods and serial intervals in both populations.ResultsThe mean incubation period was 7.1 (6.13, 8.25) days for Singapore and 9 (7.92, 10.2) days for Tianjin. Both datasets had shorter incubation periods for earlier-occurring cases. The mean serial interval was 4.56 (2.69, 6.42) days for Singapore and 4.22 (3.43, 5.01) for Tianjin. We inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (Singapore, Tianjin). The estimated basic reproduction number for Singapore was 1.97 (1.45, 2.48) secondary cases per infective; for Tianjin it was 1.87 (1.65, 2.09) secondary cases per infective.ConclusionsEstimated serial intervals are shorter than incubation periods in both Singapore and Tianjin, suggesting that pre-symptomatic transmission is occurring. Shorter serial intervals lead to lower estimates of R0, which suggest that half of all secondary infections should be prevented to control spread."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189433",
        "research_problem": "Key Information Extraction",
        "orkg_properties": "['research problem', 'operates on', 'targets']",
        "nechakhin_result": "['Research problem', 'Information extraction techniques', 'Similar papers', 'Dimensions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "An end-to-end joint model for evidence information extraction from court record document",
        "abstract": "Information extraction is one of the important tasks in the field of Natural Language Processing (NLP). Most of the existing methods focus on general texts and little attention is paid to information extraction in specialized domains such as legal texts. This paper explores the task of information extraction in the legal field, which aims to extract evidence information from court record documents (CRDs). In the general domain, entities and relations are mostly words and phrases, indicating that they do not span multiple sentences. In contrast, evidence information in CRDs may span multiple sentences, while existing models cannot handle this situation. To address this issue, we first add a classification task in addition to the extraction task. We then formulate the two tasks as a multi-task learning problem and present a novel end-to-end model to jointly address the two tasks. The joint model adopts a shared encoder followed by separate decoders for the two tasks. The experimental results on the dataset show the effectiveness of the proposed model, which can obtain 72.36% F1 score, outperforming previous methods and strong baselines by a large margin.\nAutomated legal text classification is a prominent research topic in the legal field. It lays the foundation for building an intelligent legal system. Current literature focuses on international legal texts, such as Chinese cases, European cases, and Australian cases. Little attention is paid to text classification for U.S. legal texts. Deep learning has been applied to improving text classification performance. Its effectiveness needs further exploration in domains such as the legal field. This paper investigates legal text classification with a large collection of labeled U.S. case documents through comparing the effectiveness of different text classification techniques. We propose a machine learning algorithm using domain concepts as features and random forests as the classifier. Our experiment results on 30,000 full U.S. case documents in 50 categories demonstrated that our approach significantly outperforms a deep learning system built on multiple pre-trained word embeddings and deep neural networks. In addition, applying only the top 400 domain concepts as features for building the random forests could achieve the best performance. This study provides a reference to select machine learning techniques for building high-performance text classification systems in the legal domain or other fields.\nNatural language processing (NLP) based approaches have recently received attention for legal systems of several countries. It is of interest to study the wide variety of legal systems that have so far not received any attention. In particular, for the legal system of the Republic of Turkey, codified in Turkish, no works have been published. We first review the state-of-the-art of NLP in law, and then study the problem of predicting verdicts for several different courts, using several different algorithms. This study is much broader than earlier studies in the number of different courts and the variety of algorithms it includes. Therefore it provides a reference point and baseline for further studies in this area. We further hope the scope and systematic nature of this study can set a framework that can be applied to the study of other legal systems. We present novel results on predicting the rulings of the Turkish Constitutional Court and Courts of Appeal, using only fact descriptions, and wi"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189439",
        "research_problem": "Document-Level Information Extraction",
        "orkg_properties": "['implementation', 'research problem', 'targets']",
        "nechakhin_result": "['Document type', 'Keyword extraction', 'Named entity recognition', 'Sentence-level information extraction', 'Information extraction methods', 'Machine learning techniques', 'Natural language processing', 'Data source', 'Domain specific information extraction', 'Text classification', 'Data preprocessing']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End Information Extraction",
        "abstract": "We address the challenge of extracting structured information from business documents without detailed annotations. We propose Deep Conditional Probabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional complex documents and use Recursive Neural Networks to create an end-to-end system for finding the most probable parse that represents the structured information to be extracted. This system is trained end-to-end with scanned documents as input and only relational-records as labels. The relational-records are extracted from existing databases avoiding the cost of annotating documents by hand. We apply this approach to extract information from scanned invoices achieving state-of-the-art results despite using no hand-annotations.\n"
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189444",
        "research_problem": "Knowledge Graph Completion",
        "orkg_properties": "['implementation', 'research problem', 'operates on', 'targets']",
        "nechakhin_result": "['Graph structure', 'Semantic relationships', 'Entities', 'Attributes', 'Ontologies', 'Linked data', 'Triplets', 'Embeddings', 'Graph algorithms', 'Graph mining', 'Graph neural networks']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Triple Classification for Scholarly Knowledge Graph Completion",
        "abstract": "structured information representing knowledge encoded in scientific publications. With the sheer volume of published scientific literature comprising a plethora of inhomogeneous entities and relations to describe scientific concepts, these KGs are inherently incomplete. We present exBERT, a method for leveraging pre-trained transformer language models to perform scholarly knowledge graph completion. We model triples of a knowledge graph as text and perform triple classification (i.e., belongs to KG or not). The evaluation shows that exBERT outperforms other baselines on three scholarly KG completion datasets in the tasks of triple classification, link prediction, and relation prediction. Furthermore, we present two scholarly datasets as resources for the research community, collected from public KGs and online resources."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189447",
        "research_problem": "Question Answering",
        "orkg_properties": "['implementation', 'research problem', 'operates on']",
        "nechakhin_result": "['Natural Language Processing',\n 'Machine Learning',\n 'Information Retrieval',\n 'Text Mining',\n 'Semantic Analysis']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
        "abstract": "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189455",
        "research_problem": "Information Extraction",
        "orkg_properties": "['implementation', 'research problem']",
        "nechakhin_result": "['Topic/Subject', 'Keywords', 'Abstract', 'Methods/Approach', 'Results', 'Conclusions', 'Authors', 'Publication Venue', 'Citations', 'Date of Publication', 'Researcher/Author Affiliation', 'Funding Source', 'Dataset Used', 'Language']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "TableSeer: automatic table metadata extraction and searching in digital libraries",
        "abstract": "Tables are ubiquitous in digital libraries. In scientific documents, tables are widely used to present experimental results or statistical data in a condensed fashion. However, current search engines do not support table search. The difficulty of automatic extracting tables from un-tagged documents, the lack of a universal table metadata specification, and the limitation of the existing ranking schemes make table search problem challenging. In this paper, we describeTableSeer, a search engine for tables.TableSeercrawls digital libraries, detects tables from documents, extracts tables metadata, indexes and ranks tables, and provides a user-friendly search interface. We propose an extensive set of medium-independent metadata for tables that scientists and other users can adopt for representing table information. In addition, we devise a novel pagebox-cuttingmethod to improve the performance of the table detection. Given a query,TableSeerranks the matched tables using an innovative ranking algorithm -TableRank.TableRankrates each ⃭query, tableℂ pair with a tailored vector space model and a specific term weighting scheme. Overall,TableSeereliminates the burden of manually extract table data from digital libraries and enables users to automatically examine tables. We demonstrate the value ofTableSeerwith empirical studies on scientific documents."
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R74055",
        "research_problem": "COVID-19 case fatality rate",
        "orkg_properties": "['Time period', 'research problem', 'hazard ratio', 'is about', 'location']",
        "nechakhin_result": "['Disease',\n 'Epidemiology',\n 'Public health',\n 'Virus',\n 'Mortality',\n 'Statistics',\n 'Healthcare',\n 'Pandemic',\n 'Risk factors',\n 'Comorbidities',\n 'Testing',\n 'Treatment',\n 'Interventions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Case fatality risk of the SARS-CoV-2 variant of concern B.1.1.7 in England",
        "abstract": "The B.1.1.7 variant of concern (VOC) is increasing in prevalence across Europe. Accurate estimation of disease severity associated with this VOC is critical for pandemic planning. We found increased risk of death for VOC compared with non-VOC cases in England (HR: 1.67 (95% CI: 1.34 - 2.09; P<.0001)). Absolute risk of death by 28-days increased with age and comorbidities. VOC has potential to spread faster with higher mortality than the pandemic to date."
    }
]
