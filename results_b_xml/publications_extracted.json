[
    {
        "paper_id": "http://orkg.org/orkg/resource/R138187",
        "research_problem": "Recommender systems for Smart city",
        "orkg_properties": "['research problem', 'has contribution type', 'has been evaluated in the City', 'has been evaluated in the Country', 'belongs to Smart City Dimension', 'helps achieve Smart city Goal', 'Addresses Smart city Action', 'has Application Scope', 'has Data Source', 'has Target users', 'has Recommended items', 'uses Recommendation approach', 'uses Recommendation Method', 'is based on User Preferences Type', 'Is based on Explicit User Preferences', 'Is based on Implicit User Preferences', 'Exploited data', 'has Implementation level']",
        "nechakhin_result": "['Machine Learning Algorithms',\n 'Collaborative Filtering',\n 'Content-based Filtering',\n 'Hybrid Filtering',\n 'Matrix Factorization',\n 'Deep Learning',\n 'Feature Extraction',\n 'Data Mining',\n 'Data Preprocessing',\n 'User-based Filtering',\n 'Item-based Filtering',\n 'Association Rule Mining',\n 'Context-aware Filtering',\n 'Location-based Filtering',\n 'Temporal Filtering',\n 'Evaluation Metrics',\n 'Recommendation Accuracy',\n 'Recommendation Diversity',\n 'Recommendation Serendipity',\n 'Cold Start Problem',\n 'Scalability',\n 'Real-time Recommendation',\n 'Privacy',\n 'Trustworthiness',\n 'Data Fusion',\n 'Contextual Information',\n 'User Profile',\n 'Item Profile',\n 'Contextual Factors',\n 'Information Retrieval',\n 'Semantic Analysis',\n 'Data Integration']",
        "nechakhin_mappings": 5,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "A building permit system for smart cities: A cloud-based framework",
        "abstract": "In this paper we propose a novel, cloud-based framework to support citizens and city officials in the building permit process. The proposed framework is efficient, user-friendly, and transparent with a quick turn-around time for homeowners. Compared to existing permit systems, the proposed smart city permit framework provides a pre-permitting decision workflow, and incorporates adata analyticsand mining module that enables the continuous improvement of both theend user experienceand the permitting and urban planning processes. This is enabled through a data mining-powered permit recommendation engine as well as a data analytics process that allow a gleaning of key insights for real estate development and city planning purposes, by analyzing how users interact with the system depending on their location, time, and type of request. The novelty of the proposed framework lies in the integration of a pre-permit processing front-end with permit processing and data analytics & mining modules, along with utilization of techniques for extracting knowledge from the data generated through the use of the system. The proposed framework is completely cloud-based, such that any city can deploy it with lower initial as well as maintenance costs. We also present a proof-of-concept use case, using real permit data from New York City.\nA smart built environment has become necessary for ensuring social well-being due to uncontrolled population growth and unrestrainable urban sprawl. In this connection, effective land administration is a significant element to actualize sustainable development. Yet existing building permit procedures fail to satisfy the need for current construction demands because of the insufficient transparency and inefficient procedures. Two dimensional (2D) based systems also remain incapable to unambiguously delineate the property ownership related to complex buildings. Keeping up-to-date the three dimensional (3D) urban models is another key for smart cities but this issue has become difficult owing to the rapid changes in the cities. In this sense, this paper first examines thoroughly the current situation in Turkey in terms of the building permit procedures, land administration, and 3D city modeling. Then, the paper detailedly proposes a reformative framework. The framework consists of the use of digital building models for both building permit processes and 3D registration of property ownership, as well as updating the 3D city model databases. The proposed framework is evaluated in terms of its applicability with a discussion of the prospective directions.\n",
        "dimensions": [
            [
                "Recommender system algorithms",
                "Smart city technologies",
                "Urban planning",
                "Data mining",
                "Machine learning",
                "User preferences",
                "Context-aware recommendations",
                "IoT (Internet of Things)",
                "Big data analytics",
                "Personalization",
                "User behavior modeling"
            ],
            [
                "type of recommender system",
                "data sources",
                "user preferences",
                "contextual information",
                "evaluation metrics",
                "implementation platform",
                "user feedback",
                "privacy concerns"
            ],
            [
                "Application Domain",
                "Data Types",
                "Recommender System Techniques",
                "Evaluation Metrics",
                "Smart City Infrastructure",
                "User Context",
                "Regulatory and Ethical Considerations"
            ],
            [
                "Type of recommender system",
                "Smart city domains",
                "Data sources",
                "Evaluation metrics",
                "User feedback incorporation",
                "Scalability and real-time capabilities",
                "Privacy and security considerations",
                "Urban context and user diversity"
            ],
            [
                "Recommender system type",
                "Data sources",
                "User preferences",
                "Recommendation algorithm",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Framework type",
            "Data analytics",
            "Permit process",
            "Urban planning",
            "Cloud deployment",
            "Proof-of-concept use case",
            "Land administration",
            "Building models"
        ],
        "impr+120b": [
            "research problem",
            "application domain",
            "system type",
            "methodology",
            "evaluation metric",
            "data source"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Proposed framework",
            "Deployment platform",
            "Data source",
            "Evaluation method",
            "Geographic focus"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139297",
        "research_problem": "Intelligent systems for Smart city",
        "orkg_properties": "['research problem', 'has contribution type', 'has been evaluated in the City', 'has been evaluated in the Country', 'belongs to Smart City Dimension', 'helps achieve Smart city Goal', 'Addresses Smart city Action', 'has Application Scope', 'has Data Source', 'has Target users', 'has Recommended items', 'uses Recommendation approach', 'uses Recommendation Method', 'is based on User Preferences Type', 'Is based on Explicit User Preferences', 'Is based on Implicit User Preferences', 'Exploited data', 'has Implementation level']",
        "nechakhin_result": "['Artificial Intelligence', 'Machine Learning', 'Deep Learning', 'Data Analytics', 'Internet of Things', 'Urban Planning', 'Smart Infrastructure', 'Sustainable Development', 'Energy Efficiency', 'Smart Governance']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "What's going on in my city?: recommender systems and electronic participatory budgeting",
        "abstract": "In this paper, we present electronic participatory budgeting (ePB) as a novel application domain for recommender systems. On public data from the ePB platforms of three major US cities - Cambridge, Miami and New York City-, we evaluate various methods that exploit heterogeneous sources and models of user preferences to provide personalized recommendations of citizen proposals. We show that depending on characteristics of the cities and their participatory processes, particular methods are more effective than others for each city. This result, together with open issues identified in the paper, call for further research in the area.",
        "dimensions": [
            [
                "Artificial Intelligence",
                "Smart City",
                "Internet of Things",
                "Urban Planning",
                "Machine Learning",
                "Data Analytics",
                "Sustainability",
                "Urban Infrastructure",
                "Big Data",
                "Sensor Networks"
            ],
            [
                "application areas",
                "data sources",
                "sensing technologies",
                "decision-making algorithms",
                "communication infrastructure",
                "privacy and security measures",
                "energy efficiency",
                "urban infrastructure integration"
            ],
            [
                "Technology",
                "Applications",
                "Urban Planning",
                "Sustainability",
                "Social Impact",
                "Policy and Governance",
                "Case Studies"
            ],
            [
                "Technology used",
                "Application domain",
                "Data sources",
                "Stakeholders involved",
                "Urban challenges addressed",
                "Ethical and privacy considerations",
                "Case studies and best practices"
            ],
            [
                "Intelligent system type",
                "Smart city domain",
                "Data sources",
                "Decision-making process",
                "Integration with infrastructure"
            ]
        ],
        "impr+abs": [
            "Application domain",
            "Data source",
            "Evaluation methods",
            "User preferences",
            "City characteristics",
            "Research area",
            "Open issues"
        ],
        "impr+120b": [
            "research problem",
            "application domain",
            "intelligent techniques",
            "system architecture",
            "evaluation metric",
            "implementation platform"
        ],
        "impr+120b+abs": [
            "Application domain",
            "Data source",
            "Study location",
            "Methodology",
            "Evaluation metric",
            "Findings",
            "Future research directions"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R146022",
        "research_problem": "Smart city digital transformation",
        "orkg_properties": "['Context', 'research problem', 'has methodology', 'Components ', 'Issue(s) Addressed\\t', 'Technologies Deployed']",
        "nechakhin_result": "['City infrastructure',\n 'Internet of Things (IoT)',\n 'Data analytics',\n 'Cloud computing',\n 'Artificial intelligence (AI)',\n 'Communication networks',\n 'Urban planning',\n 'Energy management',\n 'Transportation systems',\n 'Citizen engagement',\n 'Cybersecurity',\n 'Sustainability']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "A Framework for a Smart City Design: Digital Transformation in the Helsinki Smart City",
        "abstract": "Recently, there has been substantial interest in the concept of a smart city, as it has been a viable solution to the dilemmas created by the urbanization of cities. Digital technologies\u2014such as Internet-of-Things, artificial intelligence, big data, and geospatial technologies\u2014are closely associated with the concept of a smart city. By means of modern digital technologies, cities aim to optimize their performance and services. Further, cities actively endorse modern digital technologies to foster digitalization and the emergence of data-based innovations and a knowledge economy. In this paper, a framework for a smart city design is presented. The framework considers a smart city from the perspective of four dimensions\u2014strategy, technology, governance, and stakeholders. The framework is complemented with sub-dimensions, and the purpose of this framework is to strengthen the governance and sustainability of smart city initiatives. Further, the proposed framework is applied to the Helsinki smart city, the capital of Finland. The objective is to analyse the Helsinki smart city through dimensions presented in the framework and learn how the city of Helsinki governs and implements its smart city initiatives.\nThe foundation for the smart city framework presented in this paper originates from the prior work presented by H\u00e4m\u00e4l\u00e4inen and Tyrv\u00e4inen (2018). The framework was applied to the Helsinki smart city. Data for empirical research were collected by interviewing persons and stakeholders involved in the development of the Helsinki smart city (Table1). The semi-structured interview protocol was employed in interviews, which provided flexibility and the possibility for a deeper understanding of the development of Helsinki. Interviewee 1 represented the Helsinki environmental protection unit and was in charge of Helsinki city\u2019s energy and climate statistical data. Interviewee 2, Deputy CEO, represented the Smart Kalasatama project at Forum Virium Ltd. Interviewees 3 and 4 represented Helsinki Region Infoshare, an organization that releases Helsinki city\u2019s open data. Interviewee 5 was a community manager at Smart Kalasatama project, who was responsible of stakeholder relations. Interviewees 6 and 7 represented residents of the Smart Kalasatama district. All interviews were audio recorded and transcribed after the interviews. Additional data were collected by attending workshops related to smart cities and seminars in Finland, as well as reviewing official Helsinki city reports, documents, and websites. Data were collected during the period May 2017\u2013February 2019.Table 1 Empirical data collectionFull size table\nThe capital of Finland, Helsinki, has over 600,000 inhabitants. The total area of the city is 719\u00a0km2, of which almost 70% is sea (502\u00a0km2) and 30% is land (217\u00a0km2). The population density in Helsinki is almost 3000 inhabitants per km2. Smart Kalasatama is a strategic smart city development district in Helsinki. It is a new residential area, which is expected to provide homes for approximately 25,000 inhabitants by 2040. As a strategic smart city development area, Smart Kalasatama provides facilities for agile smart city pilots with a multi-stakeholder collaboration. The development of Smart Kalasatama is facilitated by Forum Virium Helsinki (FVH) Ltd., an innovation business unit owned by Helsinki city. Further, Helsinki is part of the \u2018The Six City Strategy\u2019 project, which delivers smart city pilot projects in fields such as smart mobility",
        "dimensions": [
            [
                "Smart city",
                "Digital transformation",
                "Urban development",
                "Internet of Things (IoT)",
                "Big data",
                "Urban infrastructure",
                "Sustainability",
                "Urban planning",
                "Information technology",
                "Smart governance"
            ],
            [
                "city size",
                "digital infrastructure",
                "smart technologies",
                "data collection methods",
                "governance model",
                "sustainability initiatives",
                "citizen engagement",
                "economic impact"
            ],
            [
                "Technology",
                "Urban Infrastructure",
                "Sustainability",
                "Governance and Policy",
                "Case Studies",
                "Social Impacts",
                "Economic Aspects",
                "Interdisciplinary Approaches"
            ],
            [
                "Smart city components",
                "Digital transformation technologies",
                "Urban governance and policy",
                "Case studies and best practices",
                "Sustainability and resilience",
                "Stakeholder engagement",
                "Economic impact",
                "Data privacy and security"
            ],
            [
                "Smart city components",
                "Data collection methods",
                "Technological infrastructure",
                "Sustainability factors",
                "Urban planning integration",
                "Security measures",
                "Citizen engagement strategies"
            ]
        ],
        "impr+abs": [
            "Smart city dimensions",
            "Digital technologies",
            "Empirical research methods",
            "Smart city development area",
            "Population statistics",
            "Stakeholder collaboration",
            "Smart city pilot projects"
        ],
        "impr+120b": [
            "research problem",
            "application domain",
            "digital technologies",
            "implementation strategy",
            "expected outcomes",
            "evaluation metrics"
        ],
        "impr+120b+abs": [
            "Research objective",
            "Framework dimensions",
            "Case study location",
            "Data collection method",
            "Stakeholder groups",
            "Methodology",
            "Theoretical foundation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R138642",
        "research_problem": "Smart city ontology",
        "orkg_properties": "['research problem', 'Linked Ontology', 'Ontology domains']",
        "nechakhin_result": "['Urban planning', 'Internet of Things', 'Data analytics', 'Sustainable development', 'Wireless communication', 'Energy management', 'Citizen participation', 'Traffic management', 'Environmental monitoring', 'Infrastructure management']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Km4City ontology building vs data harvesting and cleaning for smart-city services",
        "abstract": "Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies andknowledge basefor smart city. Smart City ontology is not yet standardized, and a lot ofresearch workis needed to identify models that can easily support thedata reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system fordata ingestionand reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing abig datavolume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and thebig data architecturefor the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection.",
        "dimensions": [
            [
                "Smart city",
                "Ontology",
                "Urban development",
                "Internet of Things (IoT)",
                "Sustainable development",
                "Urban planning",
                "Information technology",
                "Data management",
                "Semantic web",
                "Knowledge representation",
                "Urban infrastructure",
                "Environmental sustainability"
            ],
            [
                "smart city applications",
                "smart city infrastructure",
                "data collection methods",
                "data analysis techniques",
                "privacy and security measures",
                "governance and policy",
                "sustainability initiatives",
                "community engagement strategies"
            ],
            [
                "Technology",
                "Urban Infrastructure",
                "Data Management",
                "Interoperability",
                "Sustainability",
                "Policy and Governance",
                "Case Studies",
                "User Experience"
            ],
            [
                "Domain",
                "Ontology structure",
                "Use cases",
                "Semantic technologies",
                "Data integration",
                "Urban infrastructure",
                "Governance and policy",
                "Evaluation metrics"
            ],
            [
                "Smart city components",
                "Data sources",
                "Infrastructure",
                "Sustainability factors",
                "Security measures",
                "Urban services",
                "Interconnected systems"
            ]
        ],
        "impr+abs": [
            "Data source",
            "Ontology creation",
            "Data reconciliation",
            "Data management",
            "Data reasoning",
            "Data ingestion",
            "Big data architecture",
            "Data validation"
        ],
        "impr+120b": [
            "research problem",
            "Domain",
            "Ontology scope",
            "Application area",
            "Methodology"
        ],
        "impr+120b+abs": [
            "Data sources",
            "Ontology",
            "Data ingestion and reconciliation method",
            "Storage technology",
            "Access method",
            "Data validation",
            "Reconciliation algorithm"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R151413",
        "research_problem": "digital twin",
        "orkg_properties": "['research problem', 'has form', 'has functionality', 'has miscellaneous qualitative']",
        "nechakhin_result": "['Technology', 'Simulation', 'Data', 'Virtualization', 'Internet of Things', 'Cyber-Physical Systems', 'Modeling', 'Machine Learning', 'Artificial Intelligence', 'Analytics', 'Predictive Maintenance']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Consistency check to synchronize the Digital Twin of manufacturing automation based on anchor points",
        "abstract": "Increasing product variety and the shortening of product lifecycles require a fast and inexpensive reconfiguration of existing manufacturing automation systems. To face this challenge one solution is a Digital Twin, which can be used to reduce the complexity and time of reconfiguration by early detection of design or process sequence errors of the system with a cross-domain simulation. For engineering the Digital Twin and systemically synchronizing the data of mechatronic components in the interdisciplinary engineering models of a Digital Twin during the life cycle of manufacturing automation systems, this paper presents a concept for the engineering of a Digital Twin based on model integration in a PLM IT-Platform and an Anchor-Point method to systematically detect variances of the mechatronic data structure between the digital models and the physical system. The data of a mechatronic component from interdisciplinary domains, developed by the corresponding engineering tools are referred to as anchor points. This paper analyses domain-specific challenges in automation software-code to develop an assistance system for rule-based consistency check and for synchronizing the engineering models of the Digital Twin of the manufacturing automation system based on the Anchor-Point method.\nDigital Twin is one of the promising digital technologies being developed at present to support digital transformation and decision making in multiple industries. While the concept of a Digital Twin is nearly 20\u202fyears old, it continues to evolve as it expands to new industries and use cases. This has resulted in a continually increasing variety of definitions that threatens to dilute the concept and lead to ineffective implementations of the technology. There is a need for a consolidated and generalized definition, with clearly established characteristics to distinguish what constitutes a Digital Twin and what does not. This paper reviews 46 Digital Twin definitions given in the literature over the past ten years to propose a generalized definition that encompasses the breadth of options available and provides a detailed characterization which includes criteria to distinguish the Digital Twin from other digital technologies. Next, a process and considerations for the implementation of Digital Twins is presented through a case study. Digital Twin future needs and opportunities are also outlined.\nVarious kinds of engineering software and digitalized equipment are widely applied through the lifecycle of industrial products. As a result, massive data of different types are being produced. However, these data are hysteretic and isolated from each other, leading to low efficiency and low utilization of these valuable data. Simulation based on theoretical and static model has been a conventional and powerful tool for the verification, validation, and optimization of a system in its early planning stage, but no attention is paid to the simulation application during system run-time. With the development of new-generation information and digitalization technologies, more data can be collected, and it is time to find a way for the deep application of all these data. As a result, the concept of digital twin has aroused much concern and is developing rapidly. Dispute and discussions around concepts, paradigms, frameworks, applications, and technologies of digital twin are on the rise both in academic and industrial communities. After a complete search of several databases ",
        "dimensions": [
            [
                "Internet of Things (IoT)",
                "Cyber-physical systems",
                "Data analytics",
                "Simulation",
                "Virtual reality",
                "Machine learning",
                "Predictive maintenance",
                "Smart manufacturing",
                "Industry 4.0"
            ],
            [
                "industry",
                "application",
                "data sources",
                "modeling techniques",
                "real-time data integration",
                "simulation",
                "predictive maintenance",
                "cyber-physical systems"
            ],
            [
                "Industry",
                "Technology",
                "Application",
                "Challenges",
                "Implementation",
                "Benefits",
                "Research Methods"
            ],
            [
                "Industry",
                "Technology",
                "Use case",
                "Data sources",
                "Integration",
                "Lifecycle stage",
                "Performance metrics",
                "Case studies"
            ],
            [
                "Data source",
                "Simulation model",
                "Real-time data integration",
                "Virtual representation",
                "Physical system synchronization"
            ]
        ],
        "impr+abs": [
            "Digital Twin",
            "Manufacturing automation",
            "Model integration",
            "Anchor-Point method",
            "Consistency check",
            "Interdisciplinary engineering models",
            "Simulation application",
            "Data utilization"
        ],
        "impr+120b": [
            "Research problem",
            "Technology",
            "Application domain",
            "Implementation platform",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Proposed methodology",
            "Technology platform",
            "Domain of application",
            "Data integration approach",
            "Evaluation method",
            "Case study"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139853",
        "research_problem": "What is the tole of conservation of cultural heritage in the progres towards UN Development Goals?",
        "orkg_properties": "['method', 'research problem', 'has subject domain', 'Material', 'Process', 'has conclusion']",
        "nechakhin_result": "['conservation of cultural heritage', 'UN Development Goals', 'role', 'progress']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "SMART CITIES AND HERITAGE CONSERVATION: DEVELOPING A SMARTHERITAGE AGENDA FOR SUSTAINABLE INCLUSIVE COMMUNITIES",
        "abstract": "This paper discusses the potential of current advancements in Information Communication Technologies (ICT) for cultural heritage preservation, valorization and management within contemporary cities. The paper highlights the potential of virtual environments to assess the impacts of heritage policies on urban development. It does so by discussing the implications of virtual globes and crowdsourcing to support the participatory valuation and management of cultural heritage assets. To this purpose, a review of available valuation techniques is here presented together with a discussion on how these techniques might be coupled with ICT tools to promote inclusive governance.",
        "dimensions": [
            [
                "conservation of cultural heritage",
                "UN Development Goals",
                "cultural preservation",
                "sustainable development",
                "cultural diversity",
                "cultural identity",
                "cultural sustainability",
                "cultural policy",
                "cultural economics"
            ],
            [
                "conservation approach",
                "cultural heritage types",
                "UN Development Goals",
                "impact assessment",
                "policy framework",
                "funding sources",
                "community involvement",
                "technology used"
            ],
            [
                "Topic",
                "Keywords",
                "Geographic Scope",
                "Time Period",
                "Interdisciplinary Approach",
                "Policy and Governance",
                "Case Studies",
                "Stakeholder Involvement"
            ],
            [
                "Type of cultural heritage",
                "Conservation methods",
                "Case studies or examples",
                "Stakeholders involved",
                "Policy and governance",
                "Impact assessment"
            ],
            [
                "Conservation method",
                "Cultural heritage type",
                "UN Development Goals",
                "Impact assessment"
            ]
        ],
        "impr+abs": [
            "ICT advancements",
            "Cultural heritage preservation",
            "Virtual environments",
            "Valuation techniques",
            "Inclusive governance"
        ],
        "impr+120b": [
            "research problem",
            "subject area",
            "target framework",
            "societal impact",
            "conservation focus",
            "development goals alignment"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Application domain",
            "Technology used",
            "Methodology",
            "Evaluation technique",
            "Stakeholder involvement",
            "Governance approach"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139927",
        "research_problem": "developing an energy efficiency solution for a historicaal building",
        "orkg_properties": "['method', 'research problem', 'study location (country)', 'has subject domain', 'Material', 'has smart city instance']",
        "nechakhin_result": "['building type', 'energy consumption', 'historical preservation', 'energy efficiency technology', 'building materials', 'architectural design', 'heating and cooling systems', 'energy saving strategies', 'urban environment', 'building codes and regulations']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Smart Cities and Historical Heritage",
        "abstract": "The theme of smart grids will connote in the immediate future the production and distribution of electricity, integrating effectively and in a sustainable way energy deriving from large power stations with that distributed and supplied by renewable sources. In programmes of urban redevelopment, however, the historical city has not yet been subject to significant experimentation, also due to the specific safeguard on this kind of Heritage. This reflection opens up interesting new perspectives of research and operations, which could significantly contribute to the pursuit of the aims of the Smart City. This is the main goal of the research here presented and focused on the binomial renovation of a historical complex/enhancement and upgrading of its energy efficiency.PDFDownloads\n",
        "dimensions": [
            [
                "Energy efficiency",
                "Historical building",
                "Renovation",
                "Sustainability",
                "Building materials",
                "Architectural design",
                "Heritage conservation",
                "Green technology"
            ],
            [
                "building type",
                "energy consumption",
                "historical preservation regulations",
                "energy efficiency technologies",
                "architectural features",
                "HVAC system",
                "insulation materials",
                "lighting system",
                "renewable energy sources"
            ],
            [
                "Building Type",
                "Historical Period",
                "Energy Sources",
                "Architectural Features",
                "Preservation Constraints",
                "Climate Zone",
                "Retrofitting Challenges",
                "Case Studies"
            ],
            [
                "Type of historical building",
                "Building materials",
                "Historical significance",
                "Local climate",
                "Architectural features",
                "Energy usage patterns",
                "Preservation regulations",
                "Previous renovation history"
            ],
            [
                "Energy efficiency solution",
                "Historical building",
                "Sustainability factor",
                "Implementation method"
            ]
        ],
        "impr+abs": [
            "Smart grids",
            "Urban redevelopment",
            "Historical heritage",
            "Energy integration"
        ],
        "impr+120b": [
            "research problem",
            "application domain",
            "target building type",
            "objective",
            "solution approach"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Study focus",
            "Technology",
            "Application domain",
            "Goal",
            "Target location",
            "Methodology"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139975",
        "research_problem": "What are the reasons for the weak substantiation of cultural heritage in smart city strategies?",
        "orkg_properties": "['method', 'uses framework', 'research problem', 'has subject domain', 'has smart city instance', 'Data', 'Process', 'has objective']",
        "nechakhin_result": "['Cultural heritage', 'Smart city strategies', 'Weak substantiation', 'Reasons']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "CULTURAL HERITAGE IN SMART CITY ENVIRONMENTS: THE UPDATE",
        "abstract": ".In 2017 we published a seminal research study in the International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences about how smart city tools, solutions and applications underpinned historical and cultural heritage of cities at that time (Angelidou et al. 2017). We now return to investigate the progress that has been made during the past three years, and specifically whether the weak substantiation of cultural heritage in smart city strategies that we observed in 2017 has been improved. The newest literature suggests that smart cities should capitalize on local strengths and give prominence to local culture and traditions and provides a handful of solutions to this end. However, a more thorough examination of what has been actually implemented reveals a (still) rather immature approach. The smart city cases that were selected for the purposes of this research include Tarragona (Spain), Budapest (Hungary) and Karlsruhe (Germany). For each one we collected information regarding the overarching structure of the initiative, the positioning of cultural heritage and the inclusion of heritage-related smart city applications. We then performed a comparative analysis based on a simplified version of the Digital Strategy Canvas. Our findings suggest that a rich cultural heritage and a broader strategic focus on touristic branding and promotion are key ingredients of smart city development in this domain; this is a commonality of all the investigated cities. Moreover, three different strategy architectures emerge, representing the different interplays among the smart city, cultural heritage and sustainable urban development. We conclude that a new generation of smart city initiatives is emerging, in which cultural heritage is of increasing importance. This generation tends to associate cultural heritage with social and cultural values, liveability and sustainable urban development.\n",
        "dimensions": [
            [
                "Cultural heritage preservation",
                "Smart city strategies",
                "Lack of integration between cultural heritage and smart city initiatives",
                "Challenges in incorporating cultural heritage into urban planning",
                "Technological solutions for cultural heritage preservation",
                "Policy and governance issues in smart city strategies",
                "Community engagement and participation in cultural heritage preservation",
                "Socio-economic impact of cultural heritage in smart city development"
            ],
            [
                "smart city strategies",
                "cultural heritage",
                "substantiation",
                "reasons",
                "implementation",
                "stakeholder involvement",
                "technological integration",
                "policy framework",
                "funding",
                "public engagement"
            ],
            [
                "Smart City Strategies",
                "Cultural Heritage Preservation",
                "Technological Integration",
                "Stakeholder Engagement",
                "Policy and Governance",
                "Social and Economic Impact",
                "Case Studies and Best Practices"
            ],
            [
                "Smart City Strategies",
                "Cultural Heritage Preservation",
                "Stakeholder Involvement",
                "Technological Integration",
                "Policy and Governance",
                "Funding and Resources",
                "Public Awareness and Education",
                "Case Studies and Best Practices"
            ],
            [
                "research problem",
                "Cultural heritage",
                "Smart city strategies",
                "Challenges",
                "Integration methods",
                "Data sources",
                "Policy implications"
            ]
        ],
        "impr+abs": [
            "Research study focus",
            "Geographical scope",
            "Smart city strategy",
            "Cultural heritage integration",
            "Comparative analysis",
            "Strategic focus",
            "Urban development",
            "Social and cultural values"
        ],
        "impr+120b": [
            "research problem",
            "Domain",
            "Target concept",
            "Context",
            "Investigation focus",
            "Motivation"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Study locations",
            "Data sources",
            "Methodology",
            "Timeframe",
            "Findings"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140112",
        "research_problem": "to explain the notion of the city as a laboratory for innovation",
        "orkg_properties": "['research problem', 'Data', 'Process', 'Has finding', 'has outcome', 'has objective']",
        "nechakhin_result": "['urban studies', 'innovation', 'city planning', 'urban development', 'smart cities', 'urban design', 'technology', 'urbanization', 'sustainability', 'social innovation']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Smart cities of the future",
        "abstract": "Here we sketch the rudiments of what constitutes a smart city which we define as a city in which ICT is merged with traditional infrastructures, coordinated and integrated using new digital technologies. We first sketch our vision defining seven goals which concern: developing a new understanding of urban problems; effective and feasible ways to coordinate urban technologies; models and methods for using urban data across spatial and temporal scales; developing new technologies for communication and dissemination; developing new forms of urban governance and organisation; defining critical problems relating to cities, transport, and energy; and identifying risk, uncertainty, and hazards in the smart city. To this, we add six research challenges: to relate the infrastructure of smart cities to their operational functioning and planning through management, control and optimisation; to explore the notion of the city as a laboratory for innovation; to provide portfolios of urban simulation which inform future designs; to develop technologies that ensure equity, fairness and realise a better quality of city life; to develop technologies that ensure informed participation and create shared knowledge for democratic city governance; and to ensure greater and more effective mobility and access to opportunities for urban populations. We begin by defining the state of the art, explaining the science of smart cities. We define six scenarios based on new cities badging themselves as smart, older cities regenerating themselves as smart, the development of science parks, tech cities, and technopoles focused on high technologies, the development of urban services using contemporary ICT, the use of ICT to develop new urban intelligence functions, and the development of online and mobile forms of participation. Seven project areas are then proposed: Integrated Databases for the Smart City, Sensing, Networking and the Impact of New Social Media, Modelling Network Performance, Mobility and Travel Behaviour, Modelling Urban Land Use, Transport and Economic Interactions, Modelling Urban Transactional Activities in Labour and Housing Markets, Decision Support as Urban Intelligence, Participatory Governance and Planning Structures for the Smart City. Finally we anticipate the paradigm shifts that will occur in this research and define a series of key demonstrators which we believe are important to progressing a science of smart cities.",
        "dimensions": [
            [
                "City size",
                "Urban infrastructure",
                "Innovation ecosystem",
                "Government policies",
                "Technological advancements",
                "Urban planning",
                "Economic development",
                "Social dynamics"
            ],
            [
                "city size",
                "urban infrastructure",
                "innovation ecosystem",
                "government policies",
                "public-private partnerships",
                "technological advancements",
                "social dynamics",
                "economic factors"
            ],
            [
                "Topic",
                "Keywords",
                "Geographic Focus",
                "Disciplinary Perspective",
                "Methodology",
                "Time Period",
                "Stakeholders"
            ],
            [
                "Urban Environment",
                "Innovation Ecosystem",
                "Technological Advancements",
                "Policy and Governance",
                "Case Studies",
                "Collaboration and Co-creation",
                "Economic Impact",
                "Social and Cultural Dynamics"
            ],
            [
                "Concept",
                "Urban environment",
                "Innovation factors",
                "Case studies"
            ]
        ],
        "impr+abs": [
            "Definition",
            "Research goals",
            "Research challenges",
            "State of the art",
            "Scenarios",
            "Project areas",
            "Paradigm shifts",
            "Key demonstrators"
        ],
        "impr+120b": [
            "research problem",
            "conceptual focus",
            "domain",
            "theoretical framework",
            "application area"
        ],
        "impr+120b+abs": [
            "Research objectives",
            "Research challenges",
            "Proposed scenarios",
            "Proposed project areas",
            "Methodological approaches",
            "Key demonstrators"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R74705",
        "research_problem": "Contributions of smart cities technologies to cultural heritage documentation and services ",
        "orkg_properties": "['approach', 'research problem']",
        "nechakhin_result": "['Smart cities technologies',\n 'Cultural heritage documentation',\n 'Cultural heritage services']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "Smart Cities and Cultural Heritage \u2013 A Review of Developments and Future Opportunities",
        "abstract": "This paper reviews a cross-section of international developments in smart cities and their implications for the cultural heritage sector. A main focus of the paper is an assessment of selected case studies in the cultural heritage sector exploring the use of smart platforms and visualisation technologies. The results highlight a particular set of current challenges, as well as providing scope for identifying future opportunities in developing smart cultural heritage services.",
        "dimensions": [
            [
                "Smart cities technologies",
                "Cultural heritage documentation",
                "Cultural heritage services",
                "Technology impact on cultural heritage",
                "Smart cities and cultural heritage",
                "Documentation of cultural heritage",
                "Services for cultural heritage",
                "Digital preservation of cultural heritage"
            ],
            [
                "smart cities technologies",
                "cultural heritage documentation",
                "cultural heritage services",
                "technology impact",
                "documentation methods",
                "service delivery",
                "community engagement",
                "preservation efforts"
            ],
            [
                "Technology",
                "Cultural Heritage Domain",
                "Documentation Methods",
                "Services Offered",
                "Stakeholders",
                "Preservation Challenges",
                "User Experience"
            ],
            [
                "Technology used for documentation",
                "Cultural heritage domains covered",
                "Data management and preservation",
                "User interaction and services",
                "Impact assessment",
                "Geographic location and case studies",
                "Integration with existing heritage systems",
                "Policy and ethical considerations"
            ],
            [
                "Smart city technology",
                "Cultural heritage documentation",
                "Service provision",
                "Data collection method",
                "Impact assessment",
                "Integration approach"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Case studies",
            "Technology used",
            "Challenges",
            "Future opportunities"
        ],
        "impr+120b": [
            "research problem",
            "Technology type",
            "Domain",
            "Application",
            "Service category",
            "Documentation approach"
        ],
        "impr+120b+abs": [
            "Study type",
            "Research domain",
            "Methodology",
            "Data sources",
            "Technology used",
            "Key findings",
            "Future directions"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140051",
        "research_problem": "The research draws on Mouffe\u2019s concept of agonistic relations to explore the diversifying ideals, rhetoric, and practices of hackathon organization.",
        "orkg_properties": "['method', 'research problem', 'has methodology', 'has subject domain', 'Material', 'Has finding']",
        "nechakhin_result": "['Concept of agonistic relations',\n'Exploring diversifying ideals',\n'Rhetoric of hackathon organization',\n'Practices of hackathon organization']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Hackathons and the Practices and Possibilities of Participation",
        "abstract": "Smart city developments have been subjected to technocratic envisioning and neoliberal urban developments. However, there have been attempts to reclaim the right to the city through organizing civic initiatives to widen the access to the making of future technologies and cities. This chapter draws on Mouffe\u2019s concept of agonistic relations to explore the diversifying ideals, rhetoric, and practices of hackathon organization to consider how they might cooperate with or contest one another and provide alternative means to technology and city making. The chapter analyzes different ways of organizing hackathons and discusses the opportunities for participants with diverse social backgrounds, knowledges and technical competences to join and work together. By examining the conflictual positions, articulations, and arrangements to widen participation, the chapter suggests that more open, inclusive, and collaborative city-making events might be possible. Further work is needed to examine conflictual hackathon participation practices and other civic initiatives to pursue a more egalitarian smart city.\nSmart city developments have been subjected to technocratic envisioning and neoliberal urban developments. However, there have been attempts to reclaim the right to the city through organizing civic initiatives to widen the access to the making of future technologies and cities. This chapter draws on Mouffe\u2019s concept of agonistic relations to explore the diversifying ideals, rhetoric, and practices of hackathon organization to consider how they might cooperate with or contest one another and provide alternative means to technology and city making. The chapter analyzes different ways of organizing hackathons and discusses the opportunities for participants with diverse social backgrounds, knowledges and technical competences to join and work together. By examining the conflictual positions, articulations, and arrangements to widen participation, the chapter suggests that more open, inclusive, and collaborative city-making events might be possible. Further work is needed to examine conflictual hackathon participation practices and other civic initiatives to pursue a more egalitarian smart city.",
        "dimensions": [
            [
                "Mouffe's concept of agonistic relations",
                "hackathon organization",
                "diversifying ideals",
                "rhetoric",
                "practices"
            ],
            [
                "conceptual framework",
                "Mouffe's concept of agonistic relations",
                "ideals",
                "rhetoric",
                "practices",
                "hackathon organization"
            ],
            [
                "Concept of agonistic relations",
                "Hackathon organization",
                "Diversifying ideals",
                "Rhetoric and practices",
                "Technology and innovation"
            ],
            [
                "Theoretical framework",
                "Research focus",
                "Methodology",
                "Disciplinary perspective",
                "Publication date",
                "Geographic focus",
                "Keywords and concepts"
            ],
            [
                "Research problem",
                "Conceptual framework",
                "Research methodology",
                "Data sources",
                "Analysis approach"
            ]
        ],
        "impr+abs": [
            "Smart city development",
            "Civic initiatives",
            "Hackathon organization",
            "Participant diversity",
            "City-making events",
            "Conflictual positions",
            "Collaborative practices",
            "Egalitarian smart city pursuit"
        ],
        "impr+120b": [
            "Theoretical framework",
            "Research objective",
            "Key concepts",
            "Domain of study",
            "Phenomena investigated",
            "Research approach"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Theoretical framework",
            "Methodology",
            "Context",
            "Participant diversity",
            "Findings",
            "Future work"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140059",
        "research_problem": "aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons",
        "orkg_properties": "['result', 'research problem', 'has size', 'Material', 'Data', 'Method', 'has dubject domain', 'Has finding']",
        "nechakhin_result": "['startup factors', 'decision factors', 'open data hackathons', 'developer participation']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Open data hackathons: an innovative strategy to enhance entrepreneurial intention",
        "abstract": "PurposeIn terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services. Hackathons encourage the development of new applications using open data and the creation of startups based on these applications. Researchers focus on factors that affect nascent entrepreneurs\u2019 decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet. This paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons.Design/methodology/approachIn total, 70 papers were examined and analyzed using a three-phased literature review methodology, which was suggested byWebster and Watson (2002).These surveys investigated several factors that affect a nascent entrepreneur to create a startup.FindingsEventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors.Originality/valueThe paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons.\nFindingsEventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors.\nPurposeIn terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services. Hackathons encourage the development of new applications using open data and the creation of startups based on these applications. Researchers focus on factors that affect nascent entrepreneurs\u2019 decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet. This paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons.",
        "dimensions": [
            [
                "startup establishment",
                "developer",
                "open data hackathons",
                "decision factors",
                "model",
                "entrepreneurship",
                "technology",
                "innovation",
                "business",
                "entrepreneurial intention"
            ],
            [
                "startup success factors",
                "open data hackathons",
                "developer participation",
                "decision-making",
                "entrepreneurship",
                "startup establishment",
                "factors affecting decision",
                "model development"
            ],
            [
                "Open Data Hackathons",
                "Startup Decision Factors",
                "Developer Participation",
                "Entrepreneurship",
                "Decision-Making Models",
                "Open Data Utilization",
                "Technology Adoption"
            ],
            [
                "Open data hackathon participation",
                "Startup decision factors",
                "Developer motivations",
                "Entrepreneurial ecosystem",
                "Technology trends",
                "Funding and investment",
                "Team dynamics",
                "Risk assessment"
            ],
            [
                "Startup decision factors",
                "Developer participation",
                "Open data hackathons",
                "Model suggestion"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Entrepreneurial intention",
            "Methodology",
            "Innovation",
            "Empowerment",
            "Startup creation",
            "Open data benefits",
            "Hackathon participation"
        ],
        "impr+120b": [
            "research problem",
            "objective",
            "target population",
            "data source",
            "methodology",
            "model type",
            "factors considered"
        ],
        "impr+120b+abs": [
            "Research objective",
            "Methodology",
            "Data sources",
            "Sample size",
            "Analysis technique",
            "Findings",
            "Contribution"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139736",
        "research_problem": "How to present contested heritage in a digital archive?",
        "orkg_properties": "['method', 'Country of study', 'has  start of period', 'has end of period', 'research problem', 'Institution', 'has subject domain', 'has stakeholder']",
        "nechakhin_result": "['heritage', 'contested heritage', 'digital archive', 'presentation', 'archival presentation', 'heritage preservation', 'digital preservation', 'archival materials', 'cultural heritage', 'cultural preservation', 'digital technologies', 'heritage representation', 'heritage documentation', 'archival practices']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Public History and Contested Heritage: Archival Memories of the Bombing of Italy",
        "abstract": "This article presents a case study of a collaborative public history project between participants in two countries, the United Kingdom and Italy. Its subject matter is the bombing war in Europe, 1939-1945, which is remembered and commemorated in very different ways in these two countries: the sensitivities involved thus constitute not only a case of public history conducted at the national level but also one involving contested heritage. An account of the ways in which public history has developed in the UK and Italy is presented. This is followed by an explanation of how the bombing war has been remembered in each country. In the UK, veterans of RAF Bomber Command have long felt a sense of neglect, largely because the deliberate targeting of civilians has not fitted comfortably into the dominant victor narrative. In Italy, recollections of being bombed have remained profoundly dissonant within the received liberation discourse. The International Bomber Command Centre Digital Archive (or Archive) is then described as a case study that employs a public history approach, focusing on various aspects of its inclusive ethos, intended to preserve multiple perspectives. The Italian component of the project is highlighted, problematising the digitisation of contested heritage within the broader context of twentieth-century history. Reflections on the use of digital archiving practices and working in partnership are offered, as well as a brief account of user analytics of the Archive through its first eighteen months online.",
        "dimensions": [
            [
                "Heritage preservation",
                "Digital archives",
                "Contested history",
                "Cultural heritage",
                "Archival practices",
                "Digital presentation",
                "Public history",
                "Community engagement"
            ],
            [
                "type of heritage",
                "controversy level",
                "stakeholder perspectives",
                "digital archive platform",
                "metadata standards",
                "user interface design",
                "access restrictions",
                "narrative construction"
            ],
            [
                "Heritage Type",
                "Digital Tools and Technologies",
                "Cultural Context",
                "Stakeholder Engagement",
                "Ethical Considerations",
                "User Experience",
                "Legal and Political Aspects",
                "Community Involvement"
            ],
            [
                "Type of heritage",
                "Digital presentation methods",
                "Stakeholder engagement",
                "Ethical considerations",
                "User interaction and experience",
                "Technology and tools",
                "Cultural context",
                "Legal and regulatory frameworks"
            ],
            [
                "Digital platform",
                "Heritage representation",
                "User interaction",
                "Metadata schema"
            ]
        ],
        "impr+abs": [
            "Collaborative project",
            "Historical event",
            "Archival approach",
            "Digital archiving",
            "User analytics"
        ],
        "impr+120b": [
            "research problem",
            "heritage type",
            "digital archive",
            "presentation approach",
            "user audience",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Study locations",
            "Research problem",
            "Methodology",
            "Data source",
            "Analysis technique",
            "Collaboration type",
            "Digital platform"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139784",
        "research_problem": "How to integrate participatory approaches in hertage diplomacy?",
        "orkg_properties": "['Country of study', 'research problem', 'has subject domain', 'has stakeholder']",
        "nechakhin_result": "['participatory approaches', 'heritage diplomacy']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "The Management Of Heritage In Contested Cross-Border Contexts: Emerging Research On The Island Of Ireland",
        "abstract": "DescriptionThis paper introduces the recently begun REINVENT research project focused on the management of heritage in the cross-border cultural landscape of Derry~Londonderry. The importance of facilitating dialogue over cultural heritage to the maintenance of \u2018thin\u2019 borders in contested crossborder contexts is underlined in the paper, as is the relatively favourable strategic policy context for progressing \u2018heritage diplomacy\u2019 on the island of Ireland. However, it is argued that more inclusive and participatory approaches to the management of heritage are required to assist in the mediation of contestation, particularly accommodating a greater diversity of \u2018non-expert\u2019 opinion, in addition to helping identify value conflicts and dissonance. The application of digital technologies in the form of Public Participation Geographic Information Systems (PPGIS) is proposed, and this is briefly discussed in relation to some of the expected benefits and methodological challenges that must be addressed in the REINVENT project. The paper concludes by emphasising the importance of dialogue and knowledge exchange between academia and heritage policymakers/practitioners.",
        "dimensions": [
            [
                "Participatory approaches",
                "Heritage diplomacy",
                "Integrating participatory approaches",
                "Cultural heritage",
                "Diplomacy",
                "Community engagement",
                "Public participation",
                "International relations",
                "Cultural diplomacy"
            ],
            [
                "participatory approaches",
                "heritage diplomacy",
                "cultural heritage",
                "community engagement",
                "stakeholder involvement",
                "cultural preservation",
                "diplomatic strategies",
                "cultural representation"
            ],
            [
                "Participatory Approaches",
                "Heritage Diplomacy",
                "Case Studies",
                "Stakeholder Engagement",
                "Cultural Heritage Preservation",
                "International Relations"
            ],
            [
                "Type of participatory approaches",
                "Cultural heritage context",
                "Diplomatic strategies",
                "Case studies and best practices",
                "Stakeholder engagement",
                "Policy and governance frameworks",
                "Impact assessment"
            ],
            [
                "Integration approach",
                "Participatory methods",
                "Diplomacy context",
                "Heritage domain"
            ]
        ],
        "impr+abs": [
            "Research project focus",
            "Policy context",
            "Management approach",
            "Digital technology application",
            "Knowledge exchange"
        ],
        "impr+120b": [
            "research problem",
            "participatory approach",
            "integration strategy",
            "domain",
            "stakeholder group",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Project name",
            "Study location",
            "Research problem",
            "Methodology",
            "Technology used",
            "Policy context",
            "Stakeholder groups"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139788",
        "research_problem": "How walking methods can better illuminate post\u2010conflict space?",
        "orkg_properties": "['has countries', 'research problem', 'Institution', 'has subject domain', 'Data', 'Material', 'discusses method', 'has stakeholder']",
        "nechakhin_result": "['walking methods', 'post\u2010conflict space', 'illumination']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Troubling places: Walking the \u201ctroubling remnants\u201d of post\u2010conflict space",
        "abstract": "This paper explores the productive potential of walking methods in post-conflict space, with particular emphasis on Northern Ireland. We argue that walking methods are especially well suited to studying post-conflict spatial arrangements, yet remain underutilised for a variety of reasons. Specifically, we argue that walking methods can \u201ctrouble\u201d dominant productions of post-conflict space, revealing its storied depth, multi-temporality, and the alternative narratives of the past that frequently remain hidden in places touched by violence.\n",
        "dimensions": [
            [
                "Walking methods",
                "Post-conflict space",
                "Urban planning",
                "Conflict resolution",
                "Spatial analysis",
                "Qualitative research",
                "Community engagement",
                "Public space",
                "Reconciliation",
                "Social geography"
            ],
            [
                "walking methods",
                "post-conflict space",
                "urban planning",
                "community engagement",
                "socio-cultural context",
                "reconciliation",
                "memory",
                "trauma",
                "public space design"
            ],
            [
                "Geographic Location",
                "Conflict Context",
                "Urban Planning and Design",
                "Qualitative Research Methods",
                "Social and Cultural Dynamics",
                "Technology and Mapping"
            ],
            [
                "Methodology",
                "Post-conflict context",
                "Urban design and architecture",
                "Social interaction and community engagement",
                "Impact assessment"
            ],
            [
                "Research problem",
                "Methodology",
                "Spatial analysis",
                "Data collection method"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Study location",
            "Methodology",
            "Temporal aspect"
        ],
        "impr+120b": [
            "research problem",
            "Methodology",
            "Study context",
            "Objective",
            "Data collection method",
            "Evaluation approach"
        ],
        "impr+120b+abs": [
            "research problem",
            "methodology",
            "study location",
            "theoretical framework",
            "contribution"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139800",
        "research_problem": "What characterises contested heritage?",
        "orkg_properties": "['method', 'research problem', 'Has endpoint', 'has subject domain', 'has sources', 'has stakeholder']",
        "nechakhin_result": "['Historical significance',\n 'Cultural importance',\n 'Social relevance',\n 'Differing perspectives',\n 'Historical narratives',\n 'Political implications',\n 'Public perception',\n 'Ownership claims',\n 'Legal disputes',\n 'Restoration and preservation efforts',\n 'Collaborative processes',\n 'Ethics and values',\n 'Memory and identity',\n 'Representation and portrayal']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "A systematic review of literature on contested heritage",
        "abstract": "ABSTRACTContested heritage has increasingly been studied by scholars over the last two decades in multiple disciplines, however, there is still limited knowledge about what contested heritage is and how it is realized in society. Therefore, the purpose of this paper is to produce a systematic literature review on this topic to provide a holistic understanding of contested heritage, and delineate its current state, trends and gaps. Methodologically, four electronic databases were searched, and 102 journal articles published before 2020 were extracted. A content analysis of each article was then conducted to identify key themes and variables for classification. Findings show that while its research often lacks theoretical underpinnings, contested heritage is marked by its diversity and complexity as it becomes a global issue for both tourism and urbanization. By presenting a holistic understanding of contested heritage, this review offers an extensive investigation of the topic area to help move literature pertaining contested heritage forward.\n",
        "dimensions": [
            [
                "Historical significance",
                "Cultural significance",
                "Political significance",
                "Social significance",
                "Ownership and control",
                "Memory and identity",
                "Public perception",
                "Legal frameworks",
                "Ethical considerations",
                "Representation and interpretation"
            ],
            [
                "historical significance",
                "cultural significance",
                "ownership",
                "representation",
                "memory",
                "politics",
                "conflict",
                "interpretation",
                "public perception"
            ],
            [
                "Historical Context",
                "Stakeholders Involved",
                "Legal and Political Dimensions",
                "Cultural and Social Impact",
                "Conservation and Preservation Efforts",
                "Public Discourse and Media Representation",
                "International Perspectives"
            ],
            [
                "Definition and conceptualization",
                "Case studies and examples",
                "Stakeholders and perspectives",
                "Legal and ethical considerations",
                "Impact and management strategies"
            ],
            [
                "Definition",
                "Historical significance",
                "Controversy",
                "Preservation efforts"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Methodology",
            "Data sources",
            "Key themes",
            "Variables for classification",
            "Theoretical underpinnings",
            "Global impact",
            "Literature review findings"
        ],
        "impr+120b": [
            "research problem",
            "subject area",
            "key concepts",
            "scope",
            "methodological approach"
        ],
        "impr+120b+abs": [
            "Research subject",
            "Study type",
            "Data sources",
            "Sample size",
            "Analysis method",
            "Time period covered",
            "Findings summary"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139810",
        "research_problem": "How to define digital heritage interpretation?",
        "orkg_properties": "['result', 'research problem', 'has subject domain', 'has stakeholder', 'Material', 'Data']",
        "nechakhin_result": "['Digital technology', 'Cultural heritage', 'Interpretation techniques', 'Digital preservation', 'Museum studies', 'Information technology', 'Virtual reality', 'Archaeology', 'Art history']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Digital heritage interpretation: a conceptual framework",
        "abstract": "ABSTRACT\u2018Heritage Interpretation\u2019 has always been considered as an effective learning, communication and management tool that increases visitors\u2019 awareness of and empathy to heritage sites or artefacts. Yet the definition of \u2018digital heritage interpretation\u2019 is still wide and so far, no significant method and objective are evident within the domain of \u2018digital heritage\u2019 theory and discourse. Considering \u2018digital heritage interpretation\u2019 as a process rather than as a tool to present or communicate with end-users, this paper presents a critical application of a theoretical construct ascertained from multiple disciplines and explicates four objectives for a comprehensive interpretive process. A conceptual model is proposed and further developed into a conceptual framework with fifteen considerations. This framework is then implemented and tested on an online platform to assess its impact on end-users\u2019 interpretation level. We believe the presented interpretive framework (PrEDiC) will help heritage professionals and media designers to develop interpretive heritage project.\n",
        "dimensions": [
            [
                "Digital technology",
                "Heritage interpretation",
                "Cultural preservation",
                "Museum studies",
                "Archival practices",
                "User experience",
                "Interactive media",
                "Virtual reality",
                "Augmented reality",
                "Digital storytelling"
            ],
            [
                "definition",
                "digital technology",
                "heritage",
                "interpretation",
                "cultural preservation",
                "audience engagement",
                "virtual reality",
                "augmented reality",
                "interactive media"
            ],
            [
                "Keywords",
                "Time Period",
                "Geographic Scope",
                "Interdisciplinary Approach",
                "Technology Used",
                "Stakeholders Involved",
                "Methodology"
            ],
            [
                "Definition and concept",
                "Case studies and examples",
                "Technology and tools",
                "Stakeholders and audience engagement",
                "Preservation and curation",
                "Cultural and historical context"
            ],
            [
                "Definition",
                "Scope",
                "Interpretation methods",
                "Digital tools",
                "Case studies"
            ]
        ],
        "impr+abs": [
            "Conceptual framework",
            "Interpretive process",
            "Theoretical construct",
            "Online platform",
            "Impact assessment",
            "Heritage project",
            "End-users' interpretation level"
        ],
        "impr+120b": [
            "research problem",
            "definition scope",
            "domain",
            "methodology",
            "theoretical framework",
            "intended application"
        ],
        "impr+120b+abs": [
            "research problem",
            "methodology",
            "objectives",
            "conceptual framework",
            "implementation platform",
            "evaluation metric",
            "target audience"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139820",
        "research_problem": "How to connect the scholarly modes of communication and stakeholder-led participatory cultures in difficult heritage?",
        "orkg_properties": "['research problem', 'has subject domain', 'has stakeholder', 'materal', 'has communication channel']",
        "nechakhin_result": "['scholarly modes of communication', 'stakeholder-led participatory cultures', 'difficult heritage']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Digital Media, Participatory Culture, and Difficult Heritage: Online Remediation and the Trans-Atlantic Slave Trade",
        "abstract": "A diverse and changing array of digital media have been used to present heritage online. While websites have been created for online heritage outreach for nearly two decades, social media is employed increasingly to complement and in some cases replace the use of websites. These same social media are used by stakeholders as a form of participatory culture, to create communities and to discuss heritage independently of narratives offered by official institutions such as museums, memorials, and universities. With difficult or \u201cdark\u201d heritage\u2014places of memory centering on deaths, disasters, and atrocities\u2014these online representations and conversations can be deeply contested. Examining the websites and social media of difficult heritage, with an emphasis on the trans-Atlantic slave trade provides insights into the efficacy of online resources provided by official institutions, as well as the unofficial, participatory communities of stakeholders who use social media for collective memories.",
        "dimensions": [
            [
                "Scholarly modes of communication",
                "Stakeholder-led participatory cultures",
                "Difficult heritage"
            ],
            [
                "scholarly modes of communication",
                "stakeholder-led participatory cultures",
                "difficult heritage",
                "heritage preservation methods",
                "community engagement strategies",
                "cultural representation",
                "historical narratives",
                "ethical considerations"
            ],
            [
                "Scholarly Modes of Communication",
                "Difficult Heritage",
                "Stakeholder-led Participatory Cultures",
                "Cultural Heritage Management",
                "Interdisciplinary Approaches"
            ],
            [
                "Scholarly modes of communication",
                "Stakeholder-led participatory cultures",
                "Difficult heritage",
                "Heritage communication strategies",
                "Stakeholder engagement"
            ],
            [
                "Scholarly modes of communication",
                "Stakeholder engagement",
                "Difficult heritage",
                "Integration methods",
                "Challenges",
                "Impact assessment"
            ]
        ],
        "impr+abs": [
            "Digital media used",
            "Participatory culture",
            "Difficult heritage",
            "Online representation"
        ],
        "impr+120b": [
            "research problem",
            "communication mode",
            "stakeholder involvement",
            "participatory culture",
            "heritage type",
            "methodology"
        ],
        "impr+120b+abs": [
            "Digital media platforms",
            "Heritage category",
            "Geographic focus",
            "Research objective",
            "Stakeholder types",
            "Methodology"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139670",
        "research_problem": "How to create and deploy smart replicas in interactive musem exhibitions?",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality']",
        "nechakhin_result": "['interactive museum exhibitions', 'smart replicas', 'creation process', 'deployment process', 'replica design', 'interactivity', 'technology integration', 'visitor experience', 'augmented reality', 'user interface', 'user engagement', 'interaction design', 'sensor technology', 'data analytics', 'storage and retrieval', 'accessibility', 'security', 'sustainability']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Using Tangible Smart Replicas as Controls for an Interactive Museum Exhibition",
        "abstract": "This paper presents the design, creation and use of tangible smart replicas in a large-scale museum exhibition. We describe the design rationale for the replicas, the process used in their creation, as well as the implementation and deployment of these replicas in a live museum exhibition. Deployment of the exhibition resulted in over 14000 visitors interacting with the system during the 6 months that the exhibition was open. Based on log data, interviews and observations, we examine the reaction to these smart replicas from the point of view of the museum curators and also of the museum's visitors and reflect on the fulfillment of our expectations.",
        "dimensions": [
            [
                "Smart replicas",
                "Interactive museum exhibitions",
                "Creation",
                "Deployment",
                "Museum technology",
                "Interactive experiences",
                "Replica technology",
                "Visitor engagement"
            ],
            [
                "type of smart replica",
                "interactive technology used",
                "design process",
                "deployment location",
                "visitor engagement",
                "educational content",
                "sensor technology",
                "data collection",
                "user experience evaluation"
            ],
            [
                "Technology",
                "User Experience",
                "Museum Studies",
                "Deployment Strategies",
                "Content Creation",
                "Evaluation and Assessment",
                "Cultural Heritage Preservation"
            ],
            [
                "Technology used",
                "Deployment methods",
                "User interaction design",
                "Content creation",
                "Visitor engagement metrics",
                "Case studies and best practices"
            ],
            [
                "Creation process",
                "Deployment method",
                "Interactive technology",
                "Replica materials"
            ]
        ],
        "impr+abs": [
            "Design rationale",
            "Replica creation process",
            "Implementation and deployment",
            "Visitor interaction"
        ],
        "impr+120b": [
            "Research problem",
            "Application domain",
            "Methodology",
            "Deployment strategy",
            "Interaction design",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Design rationale",
            "Creation process",
            "Implementation details",
            "Deployment environment",
            "Evaluation methods",
            "Participant groups",
            "Interaction metrics"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139674",
        "research_problem": "What types of information architecture support smart exhibits in museums?",
        "orkg_properties": "['method', 'research problem', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality', 'Has number of user interactions']",
        "nechakhin_result": "['information architecture', 'smart exhibits', 'museums']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "exhiSTORY: Smart exhibits that tell their own stories",
        "abstract": "Museum exhibitions are designed to tell a story; this story is woven by curators and in its context a particular aspect of each exhibit, fitting to the message that the story is intended to convey, is highlighted. Adding new exhibits to the story requires curators to identify for each exhibit its aspects that fit to the message of the story and position the exhibit at the right place in the story thread. The availability of rich semantic information for exhibits, allows for exploiting the wealth of meanings that museum exhibits express, enabling the automated or semi-automated generation of practically countless stories that can be told. Personalization algorithms can then be employed to choose from these stories the ones most suitable for each individual user, based on the semantics of the stories and information within the user profile. In this work we examine how opportunities arising from technological advances in the fields of IoT and semantics can be used to develop smart, self-organizing exhibits that cooperate with each other and provide visitors with comprehensible, rich, diverse, personalized and highly stimulating experiences. These notions are included in the design of a system named exhiSTORY, which also exploits previously ignored information and identifies previously unseen semantic links. We present the architecture of the system and discuss its application potential.\n",
        "dimensions": [
            [
                "Information architecture",
                "Smart exhibits",
                "Museums",
                "User experience",
                "Interactive technology",
                "Content management",
                "Visitor engagement",
                "Digital storytelling",
                "Experiential design",
                "Cultural heritage",
                "Technology integration"
            ],
            [
                "type of smart exhibits",
                "information organization",
                "user interaction",
                "content management system",
                "sensory technology",
                "data analytics",
                "personalization",
                "accessibility features"
            ],
            [
                "Information Architecture Components",
                "User Experience Design",
                "Technology Integration",
                "Visitor Engagement",
                "Content Curation and Presentation",
                "Case Studies and Best Practices",
                "Accessibility and Inclusivity"
            ],
            [
                "Type of Information Architecture",
                "Interactive Technologies",
                "Content Organization",
                "User Engagement",
                "Data Integration",
                "Accessibility and Inclusivity",
                "User Behavior Analysis",
                "Exhibition Context"
            ],
            [
                "Information architecture",
                "Smart exhibits",
                "Museum",
                "Technology used",
                "User interaction",
                "Content management",
                "Data collection"
            ]
        ],
        "impr+abs": [
            "Museum exhibition",
            "Semantic information",
            "Personalization algorithm",
            "Technological advances",
            "System architecture"
        ],
        "impr+120b": [
            "research problem",
            "Application domain",
            "Target environment",
            "Information architecture type",
            "Exhibit type",
            "Support mechanisms"
        ],
        "impr+120b+abs": [
            "Domain",
            "Research problem",
            "Technology",
            "System name",
            "Methodology",
            "Personalization approach",
            "Architecture"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139681",
        "research_problem": "How does the physical affordance of tangible interaction affect the communication of built heritage information?",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality', 'Has number of user interactions']",
        "nechakhin_result": "['physical affordance', 'tangible interaction', 'communication', 'built heritage information']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Communicating Built Heritage Information Using Tangible Interaction Approach",
        "abstract": "Built heritage objects possess multiple types of information, varying from simple, factual aspects to more complex qualitative information and values, such as the architectural qualities, the construction techniques, or symbolic meanings of monuments. This qualitative information is relatively difficult to communicate using the conventional ways like museum labels or audio guides. Nonetheless, tangible interaction is a promising paradigm for communicating tacit information, its qualities have been demonstrated in a wide range of applications in different realms. Therefore, this study investigates how tangible interaction can enable the communication of qualitative information of built heritage to lay visitors. The main objectives of this study are communicating tacit and architectural qualities of built heritage in a physical form, investigating the effect of tangible interaction on social interaction among heritage visitors, and enhancing visitors' in-situ experience of built heritage or 1:1 replicas. Our early findings indicate the capability of tangible interaction for engaging museum visitors to accomplish additional endeavors, and facilitating their understanding of cultural values and architectural qualities of built heritage.",
        "dimensions": [
            [
                "Physical affordance",
                "Tangible interaction",
                "Communication",
                "Built heritage information"
            ],
            [
                "physical affordance",
                "tangible interaction",
                "communication",
                "built heritage information"
            ],
            [
                "Physical Affordance",
                "Tangible Interaction",
                "Built Heritage Information",
                "Communication",
                "User Experience",
                "Cultural Heritage",
                "Design",
                "Technology"
            ],
            [
                "Tangible Interaction",
                "Physical Affordance",
                "Communication",
                "Built Heritage Information",
                "User Experience",
                "Cultural Context",
                "Technology",
                "Evaluation Methods"
            ],
            [
                "Physical affordance",
                "Tangible interaction",
                "Communication impact",
                "Built heritage information"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Communication method",
            "Objectives",
            "Evaluation method"
        ],
        "impr+120b": [
            "research problem",
            "interaction modality",
            "physical affordance",
            "communication channel",
            "information domain",
            "user experience aspect"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Study objectives",
            "Interaction technique",
            "Target audience",
            "Evaluation method",
            "Key findings",
            "Application domain"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139691",
        "research_problem": "What is the value of personalised tangible data souvenirs as a bridge between the physical, personal museum experience? ",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality']",
        "nechakhin_result": "['Personalisation', 'Tangible data souvenirs', 'Physical museum experience', 'Personal museum experience']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Tangible data souvenirs as a bridge between a physical museum visit and online digital experience",
        "abstract": "This paper presents the design, implementation, use and evaluation of a tangible data souvenir for an interactive museum exhibition. We define a data souvenir as the materialisation of the personal visiting experience: a data souvenir is dynamically created on the basis of data recorded throughout the visit and therefore captures and represents the experience as lived. The souvenir provides visitors with a memento of their visit and acts as a gateway to further online content. A step further is to enable visitors to contribute, in other words the data souvenir can become a means to collect visitor-generated content. We discuss the rationale behind the use of a data souvenir, the design process and resulting artefacts, and the implementation of both the data souvenir and online content system. Finally, we examine the installation of the data souvenirs as part of a long-lasting exhibition: the use of this souvenir by visitors has been logged over 7\u00a0months and issues around the gathering of user-generated content in such a way are discussed.\nThe design brief was to create a souvenir that fit with the theme of the exhibition, while being easily produced in the thousands with minimal maintenance and also offering layers of information to the visitor. This included a representation of some aspects of their visit, the connection between the displays within the exhibition and the city of The Hague and access to an online post-visit experience where the visitors could find curated content as well as visitor-generated content, such as personal and family memories.A number of concepts for the data souvenir were created (Fig.7) in order to explore the design space and to look at issues such as ease of production, cost and customisation. These concepts included generative postcards, an overlay for a street map, and a set of travel or identification papers. Each of these connected in some way to the exhibition, whether through the use of place (the street map), the type of object (the travel papers), or the content used to create them (the postcard).Fig.\u00a07Different concepts explore at the creative phase: a set of foldable tickets (top left); a written postcard with a personalised stamp (top right); different personalised stamps (bottom left); and a layered and annotated city map (bottom right)Full size imageThe postcard concept was selected based on considerations such as the number of expected visitors (about 20,000), production process and cost. Industrial printers and paper used to print tickets for events were considered; such a technology is very fast and reliably prints thousands of cards before any maintenance for ink or paper is needed. In addition, the card background can be pre-printed on both sides with a high-quality customised image in colour (Fig.8), while patterns in black can be generated and printed dynamically so as to implement the personalisation component (Fig.9).Fig.\u00a08The standard pre-printed postcard front and back. Thefronthas predisposed areas for the printing that is done dynamically after the analysis of the logs (Fig.9); thebackshows a stylised map of The Hague with the locations represented in the exhibition indicated. Thenumberson this map also match the ones on the stampsFull size imageFig.\u00a09The personalised souvenir summarises the highlights of the visit. The postcard on theleftshows the visitor received the English narratives, followed the story of the German and spent the most time at locations 1, 3 and 9; the postca",
        "dimensions": [
            [
                "personalization",
                "tangible data souvenirs",
                "museum experience",
                "physical interaction",
                "value",
                "bridge"
            ],
            [
                "personalization",
                "tangible data souvenirs",
                "museum experience",
                "value",
                "bridge",
                "physical experience"
            ],
            [
                "Topic",
                "Keywords",
                "Museum Studies",
                "Data Souvenirs",
                "Personalization",
                "Tangible Interaction",
                "User Experience",
                "Technology in Museums",
                "Cultural Heritage",
                "Visitor Engagement"
            ],
            [
                "Type of tangible data souvenirs",
                "Personalization techniques",
                "Visitor engagement",
                "Impact on memory retention",
                "Technological integration",
                "Cultural context",
                "User experience design",
                "Long-term value"
            ],
            [
                "Value proposition",
                "Personalization",
                "Tangible data souvenirs",
                "Bridge to physical experience",
                "Bridge to personal experience"
            ]
        ],
        "impr+abs": [
            "Design process",
            "Implementation",
            "Evaluation",
            "Data souvenir type",
            "Visitor interaction",
            "Online content",
            "Production process",
            "Cost considerations"
        ],
        "impr+120b": [
            "research problem",
            "conceptual focus",
            "application domain",
            "artifact type",
            "evaluation metric",
            "target audience"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Design methodology",
            "Implementation technology",
            "Evaluation method",
            "Production scalability",
            "User-generated content handling",
            "Deployment context"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139698",
        "research_problem": "How to build a holistic approach for planning of multimedia, virtual, and mixed reality applications based on the concept of \u201caugmented\u201d and multisensory experience, innovative tangible user interfaces, and storytelling techniques?",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality', 'Has number of user interactions']",
        "nechakhin_result": "['Multimedia applications', 'Virtual reality applications', 'Mixed reality applications', 'Augmented experience', 'Multisensory experience', 'Innovative tangible user interfaces', 'Storytelling techniques']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Accessibility, Natural User Interfaces and Interactions in Museums: The IntARSI Project",
        "abstract": "In a museum context, people have specific needs in terms of physical, cognitive, and social accessibility that cannot be ignored. Therefore, we need to find a way to make art and culture accessible to them through the aid of Universal Design principles, advanced technologies, and suitable interfaces and contents. Integration of such factors is a priority of the Museums General Direction of the Italian Ministry of Cultural Heritage, within the wider strategy of museum exploitation. In accordance with this issue, the IntARSI project, publicly funded, consists of a pre-evaluation and a report of technical specifications for a new concept of museology applied to the new Museum of Civilization in Rome (MuCIV). It relates to planning of multimedia, virtual, and mixed reality applications based on the concept of \u201caugmented\u201d and multisensory experience, innovative tangible user interfaces, and storytelling techniques. An inclusive approach is applied, taking into account the needs and attitudes of a wide audience with different ages, cultural interests, skills, and expectations, as well as cognitive and physical abilities.",
        "dimensions": [
            [
                "Multimedia",
                "Virtual reality",
                "Mixed reality",
                "Augmented experience",
                "Multisensory experience",
                "Tangible user interfaces",
                "Storytelling techniques",
                "Holistic approach",
                "Planning",
                "Innovative",
                "Applications"
            ],
            [
                "multimedia applications",
                "virtual reality applications",
                "mixed reality applications",
                "augmented experience",
                "multisensory experience",
                "tangible user interfaces",
                "storytelling techniques",
                "planning approach"
            ],
            [
                "Technology",
                "User Experience",
                "Interactivity",
                "Design",
                "Application Domains",
                "Human-Computer Interaction",
                "Cognitive Science",
                "Storytelling"
            ],
            [
                "Technology",
                "User experience",
                "Tangible user interfaces",
                "Storytelling techniques",
                "Application domains",
                "Interactivity",
                "Development methodologies"
            ],
            [
                "Research problem",
                "Approach",
                "Multimedia type",
                "User interface",
                "Experience type",
                "Storytelling technique"
            ]
        ],
        "impr+abs": [
            "Accessibility",
            "User interface",
            "Museum context",
            "Technology integration",
            "Project scope",
            "Inclusive approach",
            "Cultural accessibility",
            "Multisensory experience"
        ],
        "impr+120b": [
            "research problem",
            "application domain",
            "conceptual framework",
            "design methodology",
            "interaction techniques",
            "technology focus"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Project name",
            "Funding source",
            "Application domain",
            "Technology used",
            "Evaluation method",
            "Target audience"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139703",
        "research_problem": "How to bridge the gap between the physical and the digital by means of technology in museums?",
        "orkg_properties": "['method', 'research problem', 'Has Dataset size', 'Museum involved', \"Language of smart objects' description\", 'Has user interaction modality', 'Has number of user interactions']",
        "nechakhin_result": "['technology', 'museums', 'digital', 'physical', 'bridge', 'gap']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "\"\n            Mobiles for museum visit should be abolished\n            \": a comparison of smart replicas, smart cards, and phones",
        "abstract": "A comparative evaluation of smart replicas, phone app and smart cards looked at the personal preferences of visitors and the appeal of mobiles in museum exhibitions. As part of an exhibition evaluation, 76 participants used all three interactions modes and gave their opinions in a questionnaire. The result shows that Phone and Replica are equally liked but the Phone is the most disliked interaction mode. Preference for the phone is due to its mobility as opposed to a listen in place interaction; but the phone takes the attention away from the exhibition and isolates from the group. Visitors expect museums to provide the phones as opposed to apps for \"bring your own\".",
        "dimensions": [
            [
                "Technology",
                "Museums",
                "Digitalization",
                "Visitor Experience",
                "Interactive Exhibits",
                "Cultural Heritage",
                "Augmented Reality",
                "Virtual Reality",
                "User Engagement",
                "Experiential Learning"
            ],
            [
                "museum type",
                "technology used",
                "visitor experience",
                "digital interaction",
                "exhibit type",
                "educational goals",
                "cultural context"
            ],
            [
                "Technology Type",
                "Museum Type",
                "User Engagement",
                "Content Integration",
                "Visitor Experience",
                "Cultural and Educational Context",
                "Technological Challenges"
            ],
            [
                "Type of technology",
                "Museum type",
                "User engagement",
                "Impact on visitor experience",
                "Implementation challenges",
                "Evaluation methods",
                "Cultural and ethical considerations"
            ],
            [
                "research problem",
                "Digital technology",
                "Physical artifacts",
                "Integration methods",
                "User experience",
                "Evaluation metrics"
            ]
        ],
        "impr+abs": [
            "Evaluation method",
            "Interaction mode",
            "Visitor preference",
            "Museum expectation"
        ],
        "impr+120b": [
            "Research problem",
            "Domain",
            "Technology focus",
            "Target audience",
            "Objective",
            "Methodology"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Interaction modes compared",
            "Evaluation method",
            "Number of participants",
            "Data collection instrument",
            "Key findings"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R161811",
        "research_problem": "X-ray laser advances and applications",
        "orkg_properties": "['research problem', 'Paper type', 'has system qualities']",
        "nechakhin_result": "['X-ray laser technology', 'X-ray laser physics', 'X-ray laser applications', 'X-ray laser development', 'X-ray laser experiments', 'X-ray laser materials', 'X-ray laser interactions', 'X-ray laser spectroscopy', 'X-ray laser diagnostics', 'X-ray laser imaging', 'X-ray laser sources', 'X-ray laser optics', 'X-ray laser simulation', 'X-ray laser theory', 'X-ray laser design']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Beam optics of exploding foil plasma x\u2010ray lasers",
        "abstract": "In soft x\u2010ray lasers, amplification is achieved as the x rays propagate down a long narrow plasma column. Refraction, due to electron density gradients, tends to direct the x\u2010rays out of high density regions. This can have the undesirable effect of shortening the distance that the x ray stay within the plasma, thereby limiting the amount of amplification. The exploding foil design lessens refraction, but does not eliminate it. In this paper, a quantitative analysis of propagation and amplification in an exploding foil x\u2010ray laser is presented. The density and gain profiles within the plasma are modeled in an approximate manner, which enables considerable analytic progress. It is found that refraction introduces a loss term to the laser amplification. The beam pattern from a parabolic gain profile laser has a dominant peak on the x\u2010ray laser axis. The pattern from a quartic gain profile having a dip on\u2010axis can produce a profile with off\u2010axis peaks, in better agreement with recent experimental data.",
        "dimensions": [
            [
                "X-ray technology",
                "Laser technology",
                "Advancements",
                "Applications",
                "X-ray crystallography",
                "X-ray spectroscopy",
                "X-ray imaging",
                "Materials science",
                "Biological imaging",
                "Medical imaging",
                "Physics",
                "Chemistry",
                "Engineering"
            ],
            [
                "X-ray laser technology",
                "Advancements",
                "Applications",
                "X-ray sources",
                "Experimental techniques",
                "Materials studied",
                "Research institutions",
                "Publication date"
            ],
            [
                "Keywords",
                "Publication Date",
                "Research Methodology",
                "Application Areas",
                "Authors and Institutions",
                "Citations and References",
                "Conference Proceedings and Journals"
            ],
            [
                "Publication date",
                "Laser technology",
                "Applications",
                "Experimental setup",
                "Target material",
                "Research institution"
            ],
            [
                "X-ray technology",
                "Applications",
                "Advancements",
                "Research impact"
            ]
        ],
        "impr+abs": [
            "Laser design",
            "Amplification mechanism",
            "Refraction effect",
            "Analytical modeling"
        ],
        "impr+120b": [
            "research problem",
            "technology",
            "advancements",
            "applications",
            "field of study",
            "methodology"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Modeling approach",
            "Analytical technique",
            "Key findings",
            "Experimental validation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R108960",
        "research_problem": "Biodiversity inventories with DNA based-tools",
        "orkg_properties": "['research problem', 'Biogeographical region', 'Locus (genetics)', 'DNA sequencing method', 'Phylum (Biology)', 'higher number estimated species (Method)', 'No. of estimated species (Method)', 'lower number estimated species (Method)', 'study location (country)', 'Order (Taxonomy - biology)', 'Class (Taxonomy - biology)', 'studied taxonomic group (Biology)']",
        "nechakhin_result": "['Taxonomic', 'Genetic', 'Spatial', 'Temporal', 'Methodological']",
        "nechakhin_mappings": 4,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Use of species delimitation approaches to tackle the cryptic diversity of an assemblage of high Andean butterflies (Lepidoptera: Papilionoidea)",
        "abstract": "Cryptic biological diversity has generated ambiguity in taxonomic and evolutionary studies. Single-locus methods and other approaches for species delimitation are useful for addressing this challenge, enabling the practical processing of large numbers of samples for identification and inventory purposes. This study analyzed an assemblage of high Andean butterflies using DNA barcoding and compared the identifications based on the current morphological taxonomy with three methods of species delimitation (automatic barcode gap discovery, generalized mixed Yule coalescent model, and Poisson tree processes). Sixteen potential cryptic species were recognized using these three methods, representing a net richness increase of 11.3% in the assemblage. A well-studied taxon of the genusVanessa, which has a wide geographical distribution, appeared with the potential cryptic species that had a higher genetic differentiation at the local level than at the continental level. The analyses were useful for identifying the potential cryptic species inPedaliodesandForsterinariacomplexes, which also show differentiation along altitudinal and latitudinal gradients. This genetic assessment of an entire assemblage of high Andean butterflies (Papilionoidea) provides baseline information for future research in a region characterized by high rates of endemism and population isolation.\n",
        "dimensions": [
            [
                "Biodiversity",
                "DNA-based tools",
                "Inventory methods",
                "Genetic diversity",
                "Taxonomic diversity",
                "Ecological diversity",
                "Molecular techniques",
                "Species identification",
                "Metabarcoding",
                "Phylogenetic analysis"
            ],
            [
                "location",
                "sampling method",
                "DNA extraction method",
                "DNA sequencing technology",
                "taxonomic group",
                "sampling period",
                "biodiversity metrics"
            ],
            [
                "Taxonomic Scope",
                "DNA-based Tools",
                "Geographic Scope",
                "Ecosystem Types",
                "Study Design",
                "Publication Date"
            ],
            [
                "Geographic location",
                "DNA sequencing technology",
                "Taxonomic group",
                "Sampling method",
                "Publication date",
                "DNA extraction method",
                "Bioinformatic analysis tools",
                "Environmental conditions"
            ],
            [
                "Research problem",
                "Data collection method",
                "Genetic marker",
                "Taxonomic resolution",
                "Analysis method"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Species delimitation method",
            "Taxonomic ambiguity",
            "Genetic differentiation",
            "Biodiversity assessment"
        ],
        "impr+120b": [
            "research problem",
            "data source",
            "methodology",
            "tool type",
            "domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Study organism",
            "Study location",
            "Data type",
            "Species delimitation method",
            "Findings"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R137480",
        "research_problem": "Polymeric nanoparticles for cancer treatment",
        "orkg_properties": "['Polymer', 'keywords', 'research problem', 'Animal model', 'location of study', 'Journal', 'Biodegredable or inorganic polymer', 'Nanoparticle visualisation', 'Uses drug', 'Organisations', 'Nanoparticles preparation method', 'Surfactant']",
        "nechakhin_result": "['Polymer type',\n 'Nanoparticle size and shape',\n 'Surface charge',\n 'Drug loading capacity',\n 'Drug release kinetics',\n 'Targeting ligands',\n 'Biocompatibility',\n 'In vitro characterization',\n 'In vivo studies',\n 'Toxicity',\n 'Therapeutic efficacy',\n 'Long-term stability']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "PLGA Nanoparticles Stabilized with Cationic Surfactant: Safety Studies and Application in Oral Delivery of Paclitaxel to Treat Chemical-Induced Breast Cancer in Rat",
        "abstract": "PurposeThis study was carried out to formulate poly(lactide-co-glycolide) (PLGA) nanoparticles using a quaternary ammonium salt didodecyl dimethylammonium bromide (DMAB) and checking their utility to deliver paclitaxel by oral route.MethodsParticles were prepared by emulsion solvent diffusion evaporation method. DMAB and particles stabilized with it were evaluated by MTT and LDH cytotoxicity assays. Paclitaxel was encapsulated in these nanoparticles and evaluated in a chemical carcinogenesis model in Sprague Dawley rats.ResultsMTT and LDH assays showed the surfactant to be safe toin vitrocell cultures at concentrations <33\u00a0\u03bcM. PLGA nanoparticles prepared using this stabilizer were also found to be non-toxic to cell lines for the duration of the study. When administered orally to rats bearing chemically induced breast cancer, nanoparticles were equally effective/better than intravenous paclitaxel in cremophor EL at 50% lower dose.ConclusionsThis study proves the safety and utility of DMAB in stabilizing preformed polymers like PLGA resulting in nanoparticles. This preliminary data provides a proof of concept of enabling oral chemotherapy by efficacy enhancement for paclitaxel.\nPLGA is a biodegradable and biocompatible polymer, and products based on this polymer are already approved by the United States Food & Drug Administrations (US FDA) for human use. In 1999, the US FDA approved a PLGA microsphere formulation, Nutropin Depot, as a once-a-month alternative to daily injections of human growth hormone. To increase the physical stability of nanoparticles, surfactants or stabilizers are used. Reports on the positive surface charge of DMAB (25) provided the incentive to aid the delivery of paclitaxel, since it is expected to ensure better interaction with the negatively charged cell membrane. This can result in increased retention time at the cell surface, thus increasing the chances of particle uptake. DMAB is capable of producing small and highly stable nanoparticles at 1%w/vconcentration (15). Due to the charged surface, the particle agglomeration is impeded.The particle preparation process was studied using variables like surfactant concentration, phase ratio, and homogenizer speed to understand the influence of these parameters on defining the particle characteristics. The smallest particle size with 1% surfactant might be due to better stabilization of the nanoglobules by a more comprehensive presence of the stabilizer at the interface of the two phases. Increase in specific surface area increases the surface free energy, and the decreasing particle size with increasing speed of the homogenizer reflects the transfer of higher amount of energy to the colloidal system. Similarly, a direct correlation was seen between particle size and ratio of organic-to-aqueous phase with 3:10 ratio producing particles around 70\u00a0nm (TableIII). This can be due to the surfactant effectively lining the interface between the globules and the aqueous phase when the internal phase volume is lower. In none of the parameters, we have seen a saturation effect implying that it is possible to increase or decrease the particle size beyond the obtained values, but the effect might not hold linear and would plateau outside a range. In our experience, the foremost criteria to be set is the desired particle size, and the particle preparation exercise should be carried out to then satisfy other product characteristics like drug-loading and residual surfactant concentra",
        "dimensions": [
            [
                "Polymeric composition",
                "Nanoparticle size",
                "Drug delivery",
                "Cancer treatment",
                "Biocompatibility",
                "Surface modification",
                "Targeted therapy",
                "In vivo studies",
                "Drug release kinetics"
            ],
            [
                "Polymer type",
                "Nanoparticle preparation method",
                "Drug payload",
                "Targeting ligands",
                "In vivo efficacy",
                "Biodegradability",
                "Surface charge",
                "Particle size"
            ],
            [
                "Polymer Type",
                "Drug Encapsulation",
                "Surface Functionalization",
                "Cancer Type",
                "In vivo Studies",
                "Size and Shape",
                "Release Kinetics",
                "Toxicity"
            ],
            [
                "Type of nanoparticle",
                "Polymer composition",
                "Drug payload",
                "Surface modification",
                "In vitro/in vivo studies",
                "Targeting ligands",
                "Release kinetics",
                "Biocompatibility"
            ],
            [
                "Nanoparticle type",
                "Medical application",
                "Drug delivery method",
                "Targeted therapy",
                "Biocompatibility",
                "Efficacy evaluation",
                "Toxicity assessment"
            ]
        ],
        "impr+abs": [
            "Purpose",
            "Methods",
            "Results",
            "Conclusions",
            "Polymer type",
            "Surfactant type",
            "Toxicity evaluation",
            "Drug delivery route"
        ],
        "impr+120b": [
            "research problem",
            "Nanoparticle material",
            "Therapeutic application",
            "Target disease",
            "Delivery mechanism"
        ],
        "impr+120b+abs": [
            "Formulation method",
            "Stabilizer",
            "Drug",
            "Administration route",
            "Animal model",
            "Safety assessment",
            "Efficacy evaluation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R137522",
        "research_problem": "Cervical cancer",
        "orkg_properties": "['Polymer', 'keywords', 'research problem', 'has cell line', 'location of study', 'Journal', 'Biodegredable or inorganic polymer', 'Nanoparticle visualisation', 'has  drug release studies', 'Uses drug', 'Organisations', 'Nanoparticles preparation method', 'Surfactant']",
        "nechakhin_result": "['Cancer type', 'Body part affected', 'Cancer stage', 'Treatment methods', 'Risk factors', 'Genetic factors', 'Epidemiology', 'Prevention strategies', 'Screening methods', 'Symptoms', 'Diagnosis techniques', 'Prognosis', 'Public health initiatives', 'Clinical trials']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 1,
        "nechakhin_deviation": 3,
        "title": "Paclitaxel-loaded poly(D,L-lactide-co-glycolide) nanoparticles for radiotherapy in hypoxic human tumor cells in vitro",
        "abstract": "Radioresistant hypoxic cells may contribute to the failure of radiation therapy in controlling certain tumors. Some studies have suggested the radiosensitizing effect of paclitaxel. The poly(D,L-lactide-co-glycolide)(PLGA) nanoparticles containing paclitaxel were prepared by o/w emulsification-solvent evaporation method. The physicochemical characteristics of the nanoparticles (i.e. encapsulation efficiency, particle size distribution, morphology, in vitro release) were studied. The morphology of the two human tumor cell lines: a carcinoma cervicis (HeLa) and a hepatoma (HepG2), treated with paclitaxel-loaded nanoparticles was photomicrographed. Flow cytometry was used to quantify the number of the tumor cells held in the G2/M phase of the cell cycle. The cellular uptake of nanoparticles was evaluated by transmission electronic microscopy. Cell viability was determined by the ability of single cell to form colonies in vitro. The prepared nanoparticles were spherical in shape with size between 200nm and 800nm. The encapsulation efficiency was 85.5\uff05. The release behaviour of paclitaxel from the nanoparticles exhibited a biphasic pattern characterised by a fast initial release during the first 24 h, followed by a slower and continuous release. Co-culture of the two tumor cell lines with paclitaxel-loaded nanoparticles demonstrated that the cell morphology was changed and the released paclitaxel retained its bioactivity to block cells in the G2/M phase. The cellular uptake of nanoparticles was observed. The free paclitaxel and paclitaxel-loaded nanoparticles effectively sensitized hypoxic HeLa and HepG2 cells to radiation. Under this experimental condition, the radiosensitization of paclitaxel-loaded nanoparticles was more significant than that of free paclitaxel.",
        "dimensions": [
            [
                "Risk factors",
                "Prevention",
                "Screening",
                "Treatment",
                "HPV infection",
                "Cervical cancer vaccines",
                "Epidemiology",
                "Genetics",
                "Symptoms",
                "Diagnosis",
                "Survival rates",
                "Quality of life"
            ],
            [
                "risk factors",
                "screening methods",
                "diagnostic techniques",
                "treatment options",
                "prevention strategies",
                "HPV infection",
                "cervical cancer stages",
                "patient demographics",
                "clinical trials"
            ],
            [
                "Medical domain",
                "Epidemiology",
                "Prevention and screening",
                "Treatment modalities",
                "Molecular biology and genetics",
                "Health disparities",
                "Psychosocial aspects"
            ],
            [
                "Risk factors",
                "Screening methods",
                "Treatment options",
                "Prevention strategies",
                "Epidemiology",
                "Histopathology",
                "Health disparities",
                "Molecular markers",
                "Psychosocial impact",
                "Health policy"
            ],
            [
                "Risk factors",
                "Preventive measures",
                "Screening methods",
                "Treatment options",
                "Epidemiology",
                "Genetic factors"
            ]
        ],
        "impr+abs": [
            "Nanoparticle characteristics",
            "Cellular effects",
            "Encapsulation efficiency",
            "Radiosensitization effect"
        ],
        "impr+120b": [
            "research problem",
            "Disease focus",
            "Target population",
            "Methodology",
            "Data source",
            "Study location"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Nanoparticle composition",
            "Preparation method",
            "Characterization techniques",
            "Cell models",
            "Biological assays",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R138001",
        "research_problem": "Lung cancer",
        "orkg_properties": "['Polymer', 'keywords', 'research problem', 'has cell line', 'location of study', 'Journal', 'Cytotoxicity assay', 'Biodegredable or inorganic polymer', 'Nanoparticle visualisation', 'has  drug release studies', 'Uses drug', 'Organisations', 'Nanoparticles preparation method', 'Surfactant']",
        "nechakhin_result": "['Cancer type', 'Pathogenesis', 'Risk factors', 'Symptoms', 'Diagnosis', 'Treatment', 'Prevention', 'Epidemiology']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Paclitaxel-loaded PLGA nanoparticles: preparation, physicochemical characterization and in vitro anti-tumoral activity",
        "abstract": "The main objective of this study was to develop a polymeric drug delivery system for paclitaxel, intended to be intravenously administered, capable of improving the therapeutic index of the drug and devoid of the adverse effects of Cremophor\u00aeEL. To achieve this goal paclitaxel (Ptx)-loaded poly(lactic-co-glycolic acid) (PLGA) nanoparticles (Ptx-PLGA-Nps) were prepared by the interfacial deposition method. The influence of different experimental parameters on the incorporation efficiency of paclitaxel in the nanoparticles was evaluated. Our results demonstrate that the incorporation efficiency of paclitaxel in nanoparticles was mostly affected by the method of preparation of the organic phase and also by the organic phase/aqueous phase ratio. Our data indicate that the methodology of preparation allowed the formation of spherical nanometric (<200 nm), homogeneous and negatively charged particles which are suitable for intravenous administration. The release behaviour of paclitaxel from the developed Nps exhibited a biphasic pattern characterised by an initial fast release during the first 24 h, followed by a slower and continuous release. The in vitro anti-tumoral activity of Ptx-PLGA-Nps developed in this work was assessed using a human small cell lung cancer cell line (NCI-H69 SCLC) and compared to the in vitro anti-tumoral activity of the commercial formulation Taxol\u00ae. The influence of Cremophor\u00aeEL on cell viability was also investigated. Exposure of NCI-H69 cells to 25 \u03bcg/ml Taxol\u00aeresulted in a steep decrease in cell viability. Our results demonstrate that incorporation of Ptx in nanoparticles strongly enhances the cytotoxic effect of the drug as compared to Taxol\u00ae, this effect being more relevant for prolonged incubation times.",
        "dimensions": [
            [
                "Cancer type (lung)",
                "Risk factors",
                "Genetic predisposition",
                "Environmental factors",
                "Symptoms",
                "Diagnosis",
                "Treatment options",
                "Chemotherapy",
                "Radiation therapy",
                "Immunotherapy",
                "Surgery",
                "Prognosis",
                "Survival rates",
                "Clinical trials",
                "Prevention strategies"
            ],
            [
                "type of lung cancer",
                "risk factors",
                "treatment options",
                "genetic mutations",
                "smoking history",
                "age of diagnosis",
                "gender",
                "symptoms",
                "prognosis"
            ],
            [
                "Medical Condition",
                "Risk Factors",
                "Treatment Modalities",
                "Genetic Factors",
                "Epidemiology",
                "Biomarkers",
                "Environmental Factors",
                "Palliative Care",
                "Health Policy and Public Health Interventions"
            ],
            [
                "Type of lung cancer",
                "Genetic mutations",
                "Treatment modalities",
                "Smoking history",
                "Biomarkers",
                "Stage of lung cancer",
                "Histological subtypes",
                "Environmental exposures"
            ],
            [
                "Causal factors",
                "Treatment methods",
                "Risk factors",
                "Diagnostic methods",
                "Survival rate",
                "Genetic predisposition"
            ]
        ],
        "impr+abs": [
            "Drug delivery system",
            "Nanoparticle preparation",
            "Incorporation efficiency",
            "Particle characterization",
            "Release behavior",
            "Anti-tumoral activity",
            "Cell viability",
            "Comparison with commercial formulation"
        ],
        "impr+120b": [
            "Research problem",
            "Disease focus",
            "Target population",
            "Methodology",
            "Data source",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "research objective",
            "drug delivery system",
            "nanoparticle preparation method",
            "evaluation metric",
            "particle characteristics",
            "release profile",
            "in vitro model"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R138014",
        "research_problem": "Breast cancer",
        "orkg_properties": "['Polymer', 'keywords', 'research problem', 'has cell line', 'location of study', 'Journal', 'Cytotoxicity assay', 'Biodegredable or inorganic polymer', 'Nanoparticle visualisation', 'has  drug release studies', 'Uses drug', 'Organisations', 'Nanoparticles preparation method', 'Surfactant']",
        "nechakhin_result": "['Cancer type', 'Stage of breast cancer', 'Genetic markers', 'Tumor size', 'Lymph node involvement', 'Hormone receptor status', 'Age of patient', 'Treatment options', 'Survival rate', 'Risk factors', 'Family history of breast cancer', 'Screening methods']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Nanoparticles of lipid monolayer shell and biodegradable polymer core for controlled release of paclitaxel: Effects of surfactants on particles size, characteristics and in vitro performance",
        "abstract": "This work developed a system of nanoparticles of lipid monolayer shell and biodegradable polymer core for controlled release of anticancer drugs with paclitaxel as a model drug, in which the emphasis was given to the effects of the surfactant type and the optimization of the emulsifier amount used in the single emulsion solvent evaporation/extraction process for the nanoparticle preparation on the particle size, characters andin vitroperformance. The drug loaded nanoparticles were characterized by laser light scattering (LLS) for size and size distribution, field-emission scanning electron microscopy (FESEM) for surface morphology, X-ray photoelectron spectroscopy (XPS) for surface chemistry, zetasizer for surface charge, and high performance liquid chromatography (HPLC) for drug encapsulation efficiency andin vitrodrug release kinetics. MCF-7 breast cancer cells were employed to evaluate the cellular uptake and cytotoxicity. It was found that phospholipids of short chains such as 1,2-dilauroylphosphatidylocholine (DLPC) have great advantages over the traditional emulsifier poly(vinyl alcohol) (PVA), which is used most often in the literature, in preparation of nanoparticles of biodegradable polymers such as poly(d,l-lactide-co-glycolide) (PLGA) for desired particle size, character andin vitrocellular uptake and cytotoxicity. After incubation with MCF-7 cells at 0.250mg/ml NP concentration, the coumarin-6 loaded PLGA NPs of DLPC shell showed more effective cellular uptake versus those of PVA shell. The analysis of IC50, i.e. the drug concentration at which 50% of the cells are killed, demonstrated that our DLPC shell PLGA core NP formulation of paclitaxel could be 5.88-, 5.72-, 7.27-fold effective than the commercial formulation Taxol\u00aeafter 24, 48, 72h treatment, respectively.\nLipid\u2013polymer hybrid nanoparticles (LPNs) are core\u2013shell nanoparticle structures comprising polymer cores and lipid/lipid\u2013PEG shells, which exhibit complementary characteristics of both polymeric nanoparticles and liposomes, particularly in terms of their physical stability and biocompatibility. Significantly, the LPNs have recently been demonstrated to exhibit superiorin vivocellular delivery efficacy compared to that obtained from polymeric nanoparticles and liposomes. Since their inception, the LPNs have advanced significantly in terms of their preparation strategy and scope of applications. Their preparation strategy has undergone a shift from the conceptually simple two-step method, involving preformed polymeric nanoparticles and lipid vesicles, to the more principally complex, yet easier to perform, one-step method, relying on simultaneous self-assembly of the lipid and polymer, which has resulted in better products and higher production throughput. The scope of LPNs\u2019 applications has also been extended beyond single drug delivery for anticancer therapy, to include combinatorial and active targeted drug deliveries, and deliveries of genetic materials, vaccines, and diagnostic imaging agents. This review details the current state of development for the LPNs preparation and applications from which we identify future research works needed to bring the LPNs closer to its clinical realization.",
        "dimensions": [
            [
                "Cancer type (breast)",
                "Genetic factors",
                "Hormonal factors",
                "Environmental factors",
                "Tumor markers",
                "Treatment options",
                "Survival rates",
                "Risk factors",
                "Screening methods",
                "Prevention strategies"
            ],
            [
                "type of breast cancer",
                "genetic factors",
                "hormonal factors",
                "age of diagnosis",
                "treatment type",
                "treatment outcome",
                "family history",
                "lifestyle factors",
                "ethnicity"
            ],
            [
                "Medical Domain",
                "Research Methodology",
                "Subtypes of Breast Cancer",
                "Treatment Modalities",
                "Genetic Factors",
                "Patient Outcomes",
                "Epidemiology",
                "Biomarkers and Imaging"
            ],
            [
                "Type of breast cancer",
                "Stage of breast cancer",
                "Treatment modality",
                "Genetic mutations",
                "Patient demographics",
                "Histological grade of the tumor",
                "Biomarkers",
                "Treatment response and survival rates"
            ],
            [
                "Cancer type",
                "Risk factors",
                "Treatment options",
                "Survival rate",
                "Genetic predisposition"
            ]
        ],
        "impr+abs": [
            "Drug delivery system",
            "Nanoparticle characterization",
            "Surfactant effects",
            "In vitro performance",
            "Cellular uptake",
            "Cytotoxicity analysis",
            "Hybrid nanoparticle structures",
            "Preparation strategy"
        ],
        "impr+120b": [
            "research problem",
            "disease focus",
            "target population",
            "methodology",
            "data source",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Nanoparticle composition",
            "Drug model",
            "Surfactant type",
            "Preparation method",
            "Characterization techniques",
            "In vitro evaluation metrics",
            "Cell line used"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R142246",
        "research_problem": "COVID-19 Vaccine",
        "orkg_properties": "['research problem', 'Organisations', 'Vaccine Name', 'Delivery Vehicle', 'Delivery Route']",
        "nechakhin_result": "['Vaccine type',\n'Vaccine effectiveness',\n'Vaccine development process',\n'Clinical trials',\n'Side effects',\n'Vaccine distribution',\n'Vaccine administration',\n'Vaccine safety',\n'Vaccine efficacy',\n'Vaccine ingredients',\n'Vaccine production',\n'Vaccine storage',\n'Vaccine supply chain',\n'Vaccine hesitancy',\n'Vaccine accessibility',\n'Vaccine impact on different populations',\n'Vaccine research and development']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Safety and Immunogenicity of Two RNA-Based Covid-19 Vaccine Candidates",
        "abstract": "BackgroundSevere acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infections and the resulting disease, coronavirus disease 2019 (Covid-19), have spread to millions of persons worldwide. Multiple vaccine candidates are under development, but no vaccine is currently available. Interim safety and immunogenicity data about the vaccine candidate BNT162b1 in younger adults have been reported previously from trials in Germany and the United States.MethodsIn an ongoing, placebo-controlled, observer-blinded, dose-escalation, phase 1 trial conducted in the United States, we randomly assigned healthy adults 18 to 55 years of age and those 65 to 85 years of age to receive either placebo or one of two lipid nanoparticle\u2013formulated, nucleoside-modified RNA vaccine candidates: BNT162b1, which encodes a secreted trimerized SARS-CoV-2 receptor\u2013binding domain; or BNT162b2, which encodes a membrane-anchored SARS-CoV-2 full-length spike, stabilized in the prefusion conformation. The primary outcome was safety (e.g., local and systemic reactions and adverse events); immunogenicity was a secondary outcome. Trial groups were defined according to vaccine candidate, age of the participants, and vaccine dose level (10 \u03bcg, 20 \u03bcg, 30 \u03bcg, and 100 \u03bcg). In all groups but one, participants received two doses, with a 21-day interval between doses; in one group (100 \u03bcg of BNT162b1), participants received one dose.ResultsA total of 195 participants underwent randomization. In each of 13 groups of 15 participants, 12 participants received vaccine and 3 received placebo. BNT162b2 was associated with a lower incidence and severity of systemic reactions than BNT162b1, particularly in older adults. In both younger and older adults, the two vaccine candidates elicited similar dose-dependent SARS-CoV-2\u2013neutralizing geometric mean titers, which were similar to or higher than the geometric mean titer of a panel of SARS-CoV-2 convalescent serum samples.ConclusionsThe safety and immunogenicity data from this U.S. phase 1 trial of two vaccine candidates in younger and older adults, added to earlier interim safety and immunogenicity data regarding BNT162b1 in younger adults from trials in Germany and the United States, support the selection of BNT162b2 for advancement to a pivotal phase 2\u20133 safety and efficacy evaluation. (Funded by BioNTech and Pfizer; ClinicalTrials.gov number,NCT04368728.)\n",
        "dimensions": [
            [
                "Vaccine development",
                "Coronavirus",
                "Immunization",
                "Vaccine efficacy",
                "Vaccine safety",
                "Vaccine distribution",
                "Vaccine hesitancy",
                "Vaccine side effects",
                "Vaccine technology",
                "Vaccine trials"
            ],
            [
                "Type of vaccine",
                "Vaccine platform",
                "Antigen target",
                "Clinical trial phase",
                "Efficacy",
                "Safety profile",
                "Storage requirements",
                "Dosing schedule"
            ],
            [
                "Publication Date",
                "Research Methodology",
                "Vaccine Type",
                "Efficacy and Safety",
                "Population Demographics",
                "Immunization Strategies",
                "Variants and Mutations",
                "Regulatory Approval"
            ],
            [
                "Vaccine type",
                "Clinical trial phase",
                "Efficacy and safety data",
                "Population demographics",
                "Manufacturing technology",
                "Immunization schedule",
                "Variants coverage",
                "Adverse events"
            ],
            [
                "Vaccine type",
                "Efficacy",
                "Side effects",
                "Distribution",
                "Clinical trials",
                "Manufacturing process"
            ]
        ],
        "impr+abs": [
            "Vaccine candidate",
            "Age group",
            "Outcome measure",
            "Dose level",
            "Trial design",
            "Funding source",
            "Clinical trial identifier"
        ],
        "impr+120b": [
            "research problem",
            "intervention type",
            "target disease",
            "evaluation metric",
            "study population",
            "methodology"
        ],
        "impr+120b+abs": [
            "Study location",
            "Study design",
            "Population age range",
            "Intervention",
            "Primary outcome",
            "Secondary outcome",
            "Trial phase",
            "Funding source"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R142566",
        "research_problem": "Nanocrystals for cancer treatment",
        "orkg_properties": "['research problem', 'Uses drug', 'Nanoparticles preparation method', 'Surfactant', 'Delivery Route']",
        "nechakhin_result": "['nanocrystal size', 'nanocrystal composition', 'nanocrystal surface chemistry', 'nanocrystal stability', 'drug loading efficiency', 'controlled drug release', 'biocompatibility', 'in vivo toxicity', 'cellular uptake', 'effectiveness against different types of cancer']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Formulation and antitumor activity evaluation of nanocrystalline suspensions of poorly soluble anticancer drugs",
        "abstract": "Purpose. Determine if wet milling technology could be used to formulate water insoluble antitumor agents as stabilized nanocrystalline drug suspensions that retain biological effectiveness following intravenous injection.Methods. The versatility of the approach is demonstrated by evaluation of four poorly water soluble chemotherapeutic agents that exhibit diverse chemistries and mechanisms of action. The compounds selected were: piposulfan (alkylating agent), etoposide (topoisomerase II inhibitor), camptothecin (topoisomerase I inhibitor) and paclitaxel (antimitotic agent). The agents were wet milled as a 2% w/v solids suspension containing 1 % w/v surfactant stabilizer using a low energy ball mill. The size , physical stability and efficacy of the nanocrystalline suspensions were evaluated.Results. The data show the feasibility of formulating poorly water soluble anticancer agents as physically stable aqueous nanocrystalline suspensions. The suspensions are physically stable and efficacious following intravenous injection.Conclusions. Wet milling technology is a feasible approach for formulating poorly water soluble chemotherapeutic agents that may offer a number of advantages over a more classical approach.\n",
        "dimensions": [
            [
                "Nanocrystals",
                "Cancer treatment",
                "Drug delivery",
                "Nanotechnology",
                "Biomedical applications",
                "Therapeutic nanoparticles",
                "Anticancer agents",
                "Nanomedicine",
                "Tumor targeting",
                "Biocompatibility"
            ],
            [
                "Nanocrystal composition",
                "Drug delivery method",
                "Cancer type",
                "Surface modification",
                "In vivo studies",
                "Biocompatibility"
            ],
            [
                "Nanocrystal Composition",
                "Surface Functionalization",
                "Drug Loading and Release",
                "In vitro and In vivo Studies",
                "Therapeutic Efficacy",
                "Targeted Delivery",
                "Toxicity and Biocompatibility",
                "Imaging and Diagnostic Applications"
            ],
            [
                "Nanocrystal composition",
                "Surface functionalization",
                "Drug loading and release",
                "In vivo studies",
                "Targeting strategy",
                "Therapeutic efficacy",
                "Nanocrystal synthesis method",
                "Biocompatibility and toxicity"
            ],
            [
                "Nanocrystal type",
                "Targeted therapy",
                "Drug delivery method",
                "Biological effect",
                "In vivo study",
                "Therapeutic efficacy",
                "Toxicity assessment"
            ]
        ],
        "impr+abs": [
            "Purpose",
            "Methods",
            "Chemical properties",
            "Formulation technology",
            "Stabilization method",
            "Evaluation criteria",
            "Physical stability",
            "Biological effectiveness"
        ],
        "impr+120b": [
            "research problem",
            "Material type",
            "Application domain",
            "Target disease",
            "Therapeutic strategy",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Formulation technique",
            "Drug delivery route",
            "Drug solubility",
            "Evaluation metrics",
            "Stabilizer type",
            "Compound selection"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144151",
        "research_problem": "PLGA-based nanoparticles as drug carriers",
        "orkg_properties": "['Advantages', 'Composition', 'research problem', 'Nanoparticles preparation method', 'Cargo', 'Type of nanocarrier']",
        "nechakhin_result": "['Polymer composition', 'Particle size', 'Surface charge', 'Drug-loading capacity', 'Drug release profile', 'Targeting ligands', 'Biocompatibility', 'Stability', 'In vitro drug release kinetics', 'In vivo biodistribution', 'Toxicity']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "PLGA-based nanoparticles: An overview of biomedical applications",
        "abstract": "Poly(lactic-co-glycolic acid) (PLGA) is one of the most successfully developed biodegradable polymers. Among the different polymers developed to formulate polymeric nanoparticles, PLGA has attracted considerable attention due to its attractive properties: (i) biodegradability and biocompatibility, (ii) FDA and European Medicine Agency approval in drug delivery systems for parenteral administration, (iii) well described formulations and methods of production adapted to various types of drugse.g.hydrophilic or hydrophobic small molecules or macromolecules, (iv) protection of drug from degradation, (v) possibility of sustained release, (vi) possibility to modify surface properties to provide stealthness and/or better interaction with biological materials and (vii) possibility to target nanoparticles to specific organs or cells. This review presents why PLGA has been chosen to design nanoparticles as drug delivery systems in various biomedical applications such as vaccination, cancer, inflammation and other diseases. This review focuses on the understanding of specific characteristics exploited by PLGA-based nanoparticles to target a specific organ or tissue or specific cells.\n",
        "dimensions": [
            [
                "Polymer composition",
                "Nanoparticle size",
                "Drug loading capacity",
                "Surface modification",
                "Drug release kinetics",
                "Biocompatibility",
                "Targeting ligands",
                "In vivo performance"
            ],
            [
                "Polymer composition",
                "Drug type",
                "Particle size",
                "Drug release profile",
                "Surface modification",
                "Encapsulation efficiency",
                "Biodegradability",
                "In vitro studies",
                "In vivo studies"
            ],
            [
                "Polymer Type",
                "Drug Encapsulation",
                "Targeted Drug Delivery",
                "Biodegradability",
                "Surface Modification",
                "In vitro/In vivo Studies",
                "Release Kinetics",
                "Therapeutic Area"
            ],
            [
                "Polymer composition",
                "Drug type",
                "Particle size",
                "Surface modification",
                "Drug loading method",
                "In vitro/in vivo studies",
                "Targeted delivery",
                "Release kinetics"
            ],
            [
                "Drug delivery system",
                "Polymer type",
                "Drug encapsulation method",
                "Biodegradability",
                "Drug release mechanism"
            ]
        ],
        "impr+abs": [
            "Biodegradability",
            "Biocompatibility",
            "Regulatory approval",
            "Drug delivery applications",
            "Sustained release",
            "Surface modification",
            "Targeting capabilities"
        ],
        "impr+120b": [
            "Research problem",
            "Material",
            "Nanoparticle composition",
            "Application domain",
            "Delivery mechanism",
            "Target therapeutic area"
        ],
        "impr+120b+abs": [
            "Polymer material",
            "Regulatory status",
            "Drug compatibility",
            "Release profile",
            "Surface functionalization",
            "Biomedical application"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144256",
        "research_problem": "Nanoemulsions: formation, properties and applications",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Cargo', 'Type of nanocarrier']",
        "nechakhin_result": "['Nanoemulsion formation techniques', 'Properties of nanoemulsions', 'Applications of nanoemulsions', 'Nanoemulsion stability', 'Nanoemulsion characterization', 'Surfactants and emulsifiers used in nanoemulsions', 'Effects of formulation parameters on nanoemulsion properties', 'Methodologies for evaluating nanoemulsion performance']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Nanoemulsions: formation, properties and applications",
        "abstract": "Nanoemulsions are kinetically stable liquid-in-liquid dispersions with droplet sizes on the order of 100 nm. Their small size leads to useful properties such as high surface area per unit volume, robust stability, optically transparent appearance, and tunable rheology. Nanoemulsions are finding application in diverse areas such as drug delivery, food, cosmetics, pharmaceuticals, and material synthesis. Additionally, they serve as model systems to understand nanoscale colloidal dispersions. High and low energy methods are used to prepare nanoemulsions, including high pressure homogenization, ultrasonication, phase inversion temperature and emulsion inversion point, as well as recently developed approaches such as bubble bursting method. In this review article, we summarize the major methods to prepare nanoemulsions, theories to predict droplet size, physical conditions and chemical additives which affect droplet stability, and recent applications.",
        "dimensions": [
            [
                "Nanoemulsion composition",
                "Nanoemulsion stability",
                "Nanoemulsion characterization",
                "Nanoemulsion preparation methods",
                "Nanoemulsion applications",
                "Nanoemulsion drug delivery",
                "Nanoemulsion food industry applications",
                "Nanoemulsion cosmetic industry applications"
            ],
            [
                "emulsifying agent",
                "emulsification method",
                "particle size",
                "stability",
                "rheological properties",
                "encapsulation efficiency",
                "release profile",
                "applications"
            ],
            [
                "Nanoemulsion Formation Techniques",
                "Nanoemulsion Stability",
                "Characterization Techniques",
                "Properties of Nanoemulsions",
                "Applications of Nanoemulsions",
                "Biomedical Applications",
                "Food and Beverage Applications",
                "Environmental Applications"
            ],
            [
                "Type of emulsifier",
                "Preparation method",
                "Particle size distribution",
                "Stability",
                "Applications",
                "Characterization techniques",
                "Encapsulation efficiency",
                "Stability enhancers"
            ],
            [
                "Formation method",
                "Physical properties",
                "Applications",
                "Stability factors"
            ]
        ],
        "impr+abs": [
            "Formation method",
            "Properties",
            "Applications",
            "Preparation methods",
            "Theories",
            "Physical conditions",
            "Chemical additives"
        ],
        "impr+120b": [
            "research problem",
            "Synthesis method",
            "Physical properties",
            "Application domain",
            "Stability assessment",
            "Characterization technique"
        ],
        "impr+120b+abs": [
            "Research topic",
            "Preparation methods",
            "Key physical properties",
            "Application domains",
            "Theoretical models",
            "Stability factors"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144268",
        "research_problem": "Lipid micelles as drug carriers",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Cargo', 'Type of nanocarrier']",
        "nechakhin_result": "['Lipids composition',\n 'Micelle size and shape',\n 'Drug loading capacity',\n 'Drug release profile',\n 'Biocompatibility',\n 'In vivo stability',\n 'Targeted drug delivery',\n 'Cellular uptake mechanism']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "PEG\u2013lipid micelles as drug carriers: physiochemical attributes, formulation principles and biological implication",
        "abstract": "PEG\u2013lipid micelles, primarily conjugates of polyethylene glycol (PEG) and distearyl phosphatidylethanolamine (DSPE) or PEG\u2013DSPE, have emerged as promising drug-delivery carriers to address the shortcomings associated with new molecular entities with suboptimal biopharmaceutical attributes. The flexibility in PEG\u2013DSPE design coupled with the simplicity of physical drug entrapment have distinguished PEG\u2013lipid micelles as versatile and effective drug carriers for cancer therapy. They were shown to overcome several limitations of poorly soluble drugs such as non-specific biodistribution and targeting, lack of water solubility and poor oral bioavailability. Therefore, considerable efforts have been made to exploit the full potential of these delivery systems; to entrap poorly soluble drugs and target pathological sites both passively through the enhanced permeability and retention (EPR) effect and actively by linking the terminal PEG groups with targeting ligands, which were shown to increase delivery efficiency and tissue specificity. This article reviews the current state of PEG\u2013lipid micelles as delivery carriers for poorly soluble drugs, their biological implications and recent developments in exploring their active targeting potential. In addition, this review sheds light on the physical properties of PEG\u2013lipid micelles and their relevance to the inherent advantages and applications of PEG\u2013lipid micelles for drug delivery.KeywordsActive targetingcancercytotoxicitydrug deliverydrug resistancemicellesnanotechnologyPEG\u2013lipidpoorly solublesustained releaseDeclaration of interestThe authors report no conflicts of interest.\n",
        "dimensions": [
            [
                "Lipid composition",
                "Micelle structure",
                "Drug encapsulation",
                "Drug release",
                "Biocompatibility",
                "Stability",
                "Targeting capability",
                "In vivo studies",
                "Drug delivery efficiency"
            ],
            [
                "Type of nanocarrier",
                "Micelle preparation method",
                "Lipid composition",
                "Drug type",
                "Particle size"
            ],
            [
                "drug delivery system",
                "lipid composition",
                "drug encapsulation",
                "stability and release kinetics",
                "biological interactions",
                "in vivo studies",
                "comparative studies"
            ],
            [
                "Type of nanocarrier",
                "Nanoparticle preparation method",
                "Lipid composition",
                "Drug type",
                "Particle size"
            ],
            [
                "Drug delivery system",
                "Lipid type",
                "Encapsulation efficiency",
                "Drug release mechanism"
            ]
        ],
        "impr+abs": [
            "Drug carriers",
            "Biological implications",
            "Formulation principles",
            "Active targeting",
            "Physical properties",
            "Drug delivery",
            "Sustained release"
        ],
        "impr+120b": [
            "research problem",
            "Carrier type",
            "Application domain",
            "Target therapeutic",
            "Delivery mechanism",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Nanocarrier type",
            "Therapeutic application",
            "Formulation principle",
            "Physicochemical attributes",
            "Passive targeting mechanism",
            "Active targeting strategy",
            "Release profile"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144328",
        "research_problem": "Liposomes as drug carriers",
        "orkg_properties": "['Composition', 'research problem', 'Nanoparticles preparation method', 'Type of nanocarrier']",
        "nechakhin_result": "['Drug delivery systems', 'Lipid-based drug delivery', 'Nanotechnology', 'Biomaterials', 'Pharmaceutical science', 'Pharmacokinetics', 'Formulation development', 'Drug encapsulation', 'Biocompatibility', 'Drug release kinetics', 'Targeted drug delivery', 'Stability of liposomes']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Preparation, Biodistribution and Neurotoxicity of Liposomal Cisplatin following Convection Enhanced Delivery in Normal and F98 Glioma Bearing Rats",
        "abstract": "The purpose of this study was to evaluate two novel liposomal formulations of cisplatin as potential therapeutic agents for treatment of the F98 rat glioma. The first was a commercially produced agent, Lipoplatin\u2122, which currently is in a Phase III clinical trial for treatment of non-small cell lung cancer (NSCLC). The second, produced in our laboratory, was based on the ability of cisplatin to form coordination complexes with lipid cholesteryl hemisuccinate (CHEMS). Thein vitrotumoricidal activity of the former previously has been described in detail by other investigators. The CHEMS liposomal formulation had a Pt loading efficiency of 25% and showed more potentin vitrocytotoxicity against F98 glioma cells than free cisplatin at 24 h.In vivoCHEMS liposomes showed high retention at 24 h after intracerebral (i.c.) convection enhanced delivery (CED) to F98 glioma bearing rats. Neurotoxicologic studies were carried out in non-tumor bearing Fischer rats following i.c. CED of Lipoplatin\u2122 or CHEMS liposomes or their \u201chollow\u201d counterparts. Unexpectedly, Lipoplatin\u2122 was highly neurotoxic when given i.c. by CED and resulted in death immediately following or within a few days after administration. Similarly \u201chollow\u201d Lipoplatin\u2122 liposomes showed similar neurotoxicity indicating that this was due to the liposomes themselves rather than the cisplatin. This was particularly surprising since Lipoplatin\u2122 has been well tolerated when administered intravenously. In contrast, CHEMS liposomes and their \u201chollow\u201d counterparts were clinically well tolerated. However, a variety of dose dependent neuropathologic changes from none to severe were seen at either 10 or 14 d following their administration. These findings suggest that further refinements in the design and formulation of cisplatin containing liposomes will be required before they can be administered i.c. by CED for the treatment of brain tumors and that a formulation that may be safe when given systemically may be highly neurotoxic when administered directly into the brain.",
        "dimensions": [
            [
                "Drug delivery",
                "Nanotechnology",
                "Pharmacokinetics",
                "Biocompatibility",
                "Encapsulation efficiency",
                "Surface modification",
                "Targeted drug delivery",
                "Lipid bilayer",
                "In vivo studies",
                "In vitro studies",
                "Therapeutic efficacy",
                "Toxicity",
                "Stability",
                "Formulation",
                "Biological barriers",
                "Drug release kinetics"
            ],
            [
                "Type of nanocarrier",
                "Nanoparticle preparation method",
                "Lipid composition",
                "Drug type",
                "Particle size"
            ],
            [
                "Drug delivery mechanisms",
                "Liposome composition",
                "Drug loading and release",
                "Therapeutic applications",
                "In vitro and in vivo studies",
                "Comparative studies",
                "Clinical translation"
            ],
            [
                "Type of nanocarrier",
                "Nanoparticle preparation method",
                "Lipid composition",
                "Drug type",
                "Particle size"
            ],
            [
                "Drug delivery system",
                "Lipid composition",
                "Encapsulation efficiency",
                "Targeting mechanism",
                "Release kinetics"
            ]
        ],
        "impr+abs": [
            "Liposomal formulation",
            "Therapeutic agent",
            "In vitro cytotoxicity",
            "Neurotoxicity",
            "Drug delivery method"
        ],
        "impr+120b": [
            "research problem",
            "Carrier type",
            "Therapeutic application",
            "Delivery mechanism",
            "Target disease",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Animal model",
            "Therapeutic agent",
            "Formulation type",
            "Delivery technique",
            "Administration route",
            "Evaluation endpoint",
            "Toxicity assessment"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144336",
        "research_problem": "Solid lipid nanoparticles as drug carriers",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Type of nanocarrier']",
        "nechakhin_result": "['Nanoparticle composition', 'Drug encapsulation method', 'Drug release kinetics', 'Particle size', 'Surface charge', 'Biocompatibility', 'Drug loading capacity', 'Drug stability', 'In vitro drug release profiles', 'In vivo performance', 'Targeting ability', 'Drug efficacy', 'Toxicity', 'Biodegradability']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Solid Lipid Nanoparticles: Emerging Colloidal Nano Drug Delivery Systems",
        "abstract": "Solid lipid nanoparticles (SLNs) are nanocarriers developed as substitute colloidal drug delivery systems parallel to liposomes, lipid emulsions, polymeric nanoparticles, and so forth. Owing to their unique size dependent properties and ability to incorporate drugs, SLNs present an opportunity to build up new therapeutic prototypes for drug delivery and targeting. SLNs hold great potential for attaining the goal of targeted and controlled drug delivery, which currently draws the interest of researchers worldwide. The present review sheds light on different aspects of SLNs including fabrication and characterization techniques, formulation variables, routes of administration, surface modifications, toxicity, and biomedical applications.",
        "dimensions": [
            [
                "Nanoparticles",
                "Drug delivery",
                "Solid lipid",
                "Drug carriers",
                "Biocompatibility",
                "Encapsulation efficiency",
                "Release kinetics",
                "Surface modification",
                "Biodegradability",
                "In vivo studies"
            ],
            [
                "Type of nanocarrier",
                "Lipid composition",
                "Drug type",
                "Particle size",
                "Preparation method",
                "Encapsulation efficiency",
                "Release profile",
                "Stability"
            ],
            [
                "drug delivery system",
                "nanotechnology",
                "pharmacology",
                "formulation and characterization",
                "biomedical applications",
                "in vitro and in vivo studies",
                "comparative studies",
                "clinical trials"
            ],
            [
                "Type of nanocarrier",
                "Lipid composition",
                "Drug type",
                "Particle size",
                "Surface modification"
            ],
            [
                "Drug delivery system",
                "Nanoparticle composition",
                "Drug encapsulation efficiency",
                "In vitro release profile"
            ]
        ],
        "impr+abs": [
            "Drug delivery system",
            "Nanoparticle type",
            "Formulation variables",
            "Routes of administration",
            "Surface modifications",
            "Toxicity",
            "Biomedical applications"
        ],
        "impr+120b": [
            "research problem",
            "nanoparticle type",
            "drug delivery method",
            "target application",
            "formulation technique"
        ],
        "impr+120b+abs": [
            "Nanoparticle type",
            "Fabrication techniques",
            "Characterization techniques",
            "Formulation variables",
            "Routes of administration",
            "Surface modifications",
            "Toxicity assessment",
            "Biomedical applications"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144341",
        "research_problem": "Lipid nanocapsules as drug carriers",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Type of nanocarrier']",
        "nechakhin_result": "['Lipid composition', 'Nanocapsule size', 'Drug encapsulation efficiency', 'Surface charge', 'Drug release kinetics', 'Biocompatibility', 'Targeting ligands', 'Stability', 'Overcoming biological barriers', 'Drug loading capacity', 'Administration route', 'In vivo biodistribution', 'Toxicity', 'Pharmacokinetics', 'Therapeutic efficacy']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Lipid nanocapsules: A new platform for nanomedicine",
        "abstract": "Nanomedicine, an emerging new field created by the fusion of nanotechnology and medicine, is one of the most promising pathways for the development of effective targeted therapies with oncology being the earlier and the most notable beneficiary to date. Indeed, drug-loaded nanoparticles provide an ideal solution to overcome the low selectivity of the anticancer drugs towards the cancer cells in regards to normal cells and the induced severe side-effects, thanks to their passive and/or active targeting to cancer tissues. Liposome-based systems encapsulating drugs are already used in some cancer therapies (e.g. Myocet, Daunoxome, Doxil). But liposomes have some important drawbacks: they have a low capacity to encapsulate lipophilic drugs (even though it exists), they are manufactured through processes involving organic solvents, and they are leaky, unstable in biological fluids and more generally in aqueous solutions for being commercialized as such. We have developed new nano-cargos, the lipid nanocapsules, with sizes below the endothelium fenestration (\u03d5<100nm), that solve these disadvantages. They are prepared according to a solvent-free process and they are stable for at least one year in suspension ready for injection, which should reduce considerably the cost and convenience for treatment. Moreover, these new nano-cargos have the ability to encapsulate efficiently lipophilic drugs, offering a pharmaceutical solution for their intravenous administration.The lipid nanocapsules (LNCs) have been prepared according to an original method based on a phase-inversion temperature process recently developed and patented. Their structure is a hybrid between polymeric nanocapsules and liposomes because of their oily core which is surrounded by a tensioactive rigid membrane. They have a lipoprotein-like structure. Their size can be adjusted below 100nm with a narrow distribution. Importantly, these properties confer great stability to the structure (physical stability>18 months). Blank or drug-loaded LNCs can be prepared, with or without PEG (polyethyleneglycol)ylation that is a key parameter that affects the vascular residence time of the nano-cargos. Other hydrophilic tails can also be grafted. Different anticancer drugs (paclitaxel, docetaxel, etoposide, hydroxytamoxifen, doxorubicin, etc.) have been encapsulated. They all are released according to a sustained pattern. Preclinical studies on cell cultures and animal models of tumors have been performed, showing promising results.\nNanomedicine, an emerging new field created by the fusion of nanotechnology and medicine, is one of the most promising pathways for the development of effective targeted therapies with oncology being the earlier and the most notable beneficiary to date. Indeed, drug-loaded nanoparticles provide an ideal solution to overcome the low selectivity of the anticancer drugs towards the cancer cells in regards to normal cells and the induced severe side-effects, thanks to their passive and/or active targeting to cancer tissues. Liposome-based systems encapsulating drugs are already used in some cancer therapies (e.g. Myocet, Daunoxome, Doxil). But liposomes have some important drawbacks: they have a low capacity to encapsulate lipophilic drugs (even though it exists), they are manufactured through processes involving organic solvents, and they are leaky, unstable in biological fluids and more generally in aqueous solutions for being commercialized as such. We have developed n",
        "dimensions": [
            [
                "Nanotechnology",
                "Drug delivery",
                "Lipid-based drug carriers",
                "Nanocapsules",
                "Biomedical applications",
                "Pharmacokinetics",
                "Encapsulation efficiency",
                "Surface modification",
                "Biocompatibility",
                "Targeted drug delivery"
            ],
            [
                "Type of nanocarrier",
                "Nanoparticle preparation method",
                "Lipid composition",
                "Drug type",
                "Particle size"
            ],
            [
                "Drug Delivery Systems",
                "Nanotechnology",
                "Pharmaceutical Chemistry",
                "Biomedical Engineering",
                "Pharmacology",
                "Biophysics",
                "Biocompatibility",
                "Therapeutics"
            ],
            [
                "Type of nanocarrier",
                "Nanoparticle preparation method",
                "Lipid composition",
                "Drug type",
                "Particle size"
            ],
            [
                "Drug delivery system",
                "Nanotechnology",
                "Encapsulation method",
                "Drug release mechanism"
            ]
        ],
        "impr+abs": [
            "Nanomedicine",
            "Drug delivery system",
            "Encapsulation method",
            "Therapeutic application",
            "Stability",
            "Particle size",
            "Preclinical studies"
        ],
        "impr+120b": [
            "research problem",
            "Carrier type",
            "Material composition",
            "Application domain",
            "Target therapeutic area",
            "Delivery mechanism"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Nanocarrier type",
            "Preparation method",
            "Physical stability",
            "Drug encapsulation capability",
            "Surface modification",
            "Preclinical evaluation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144346",
        "research_problem": "Nano-lipid carrier for drug delivery",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Type of nanocarrier']",
        "nechakhin_result": "['nanotechnology',\n 'lipid carrier',\n 'drug delivery',\n 'nanoparticles',\n 'nanomedicine',\n 'drug release',\n 'targeted therapy',\n 'encapsulation',\n 'biocompatibility',\n 'pharmacokinetics',\n 'toxicity',\n 'efficiency']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Recent strategies and advances in the fabrication of nano lipid carriers and their application towards brain targeting",
        "abstract": "In last two decades, the lipidnanocarriershave been extensively investigated for theirdrug targetingefficiency towards the critical areas of the human body like CNS, cardiac region, tumor cells,etc.Owing to the flexibility andbiocompatibility, the lipid-based nanocarriers, includingnanoemulsion,liposomes, SLN, NLCetc.have gained much attention among various other nanocarrier systems for brain targeting of bioactives. Across different lipid nanocarriers, NLC remains to be the safest, stable, biocompatible and cost-effective drug carrier system with high encapsulation efficiency. Drug delivery to the brain always remains a challenging issue for scientists due to the complex structure and various barrier mechanisms surrounding the brain. The application of a suitable nanocarrier system and the use of any alternative route of drug administration like nose-to-brain drug delivery could overcome the hurdle and improves the therapeutic efficiency of CNS acting drugs thereof. NLC, a second-generation lipid nanocarrier, upsurges the drug permeation across the BBB due to its unique structural properties. The biocompatible lipid matrix and nano-size make it an ideal drug carrier for brain targeting. It offers many advantages over other drug carrier systems, including ease of manufacturing and scale-up to industrial level, higher drug targeting, high drug loading, control drug release, compatibility with a wide range of drug substances, non-toxic and non-irritant behavior. This review highlights recent progresses towards the development of NLC for brain targeting of bioactives with particular reference to its surface modifications, formulations aspects,pharmacokineticbehavior and efficacy towards the treatment of variousneurological disorderslike AD, PD, schizophrenia, epilepsy, brain cancer,CNS infection(viral and fungal),multiple sclerosis,cerebral ischemia, andcerebral malaria. This work describes in detail the role and application of NLC, along with its different fabrication techniques and associated limitations. Specific emphasis is given to compile a summary and graphical data on the area explored by scientists and researchers worldwide towards the treatment of neurological disorders with or without NLC. The article also highlights a brief insight into two prime approaches for brain targeting, including drug delivery across BBB and direct nose-to-brain drug delivery along with the current global status of specific neurological disorders.",
        "dimensions": [
            [
                "Nanotechnology",
                "Lipid carriers",
                "Drug delivery",
                "Nanoparticles",
                "Encapsulation",
                "Biocompatibility",
                "Targeted delivery",
                "Drug release",
                "Surface modification",
                "Therapeutic efficacy"
            ],
            [
                "Type of nanocarrier",
                "Lipid composition",
                "Drug type",
                "Particle size",
                "Drug delivery method",
                "Encapsulation efficiency",
                "Surface charge",
                "In vivo performance"
            ],
            [
                "drug delivery system",
                "drug type",
                "biomedical application",
                "lipid composition",
                "encapsulation efficiency",
                "stability and shelf-life",
                "in vitro/in vivo studies",
                "surface modification",
                "targeted delivery",
                "toxicity and biocompatibility"
            ],
            [
                "Type of nanocarrier",
                "Nanoparticle preparation method",
                "Lipid composition",
                "Drug type",
                "Particle size"
            ],
            [
                "Drug delivery system",
                "Nanotechnology",
                "Carrier type",
                "Drug encapsulation",
                "Targeted delivery",
                "Biocompatibility",
                "Release kinetics"
            ]
        ],
        "impr+abs": [
            "Drug carrier system",
            "Targeted area",
            "Encapsulation efficiency",
            "Fabrication techniques",
            "Pharmacokinetic behavior",
            "Neurological disorders",
            "Therapeutic efficiency",
            "Global status"
        ],
        "impr+120b": [
            "research problem",
            "Carrier type",
            "Delivery mechanism",
            "Target application",
            "Formulation method",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Target organ",
            "Nanocarrier type",
            "Delivery route",
            "Therapeutic indication",
            "Fabrication technique",
            "Surface modification",
            "Key advantages"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144353",
        "research_problem": "Extracellular vesicles as drug carrier",
        "orkg_properties": "['research problem', 'Nanoparticles preparation method', 'Cargo', 'Type of nanocarrier']",
        "nechakhin_result": "['Extracellular vesicles', 'Drug delivery', 'Nanoparticles', 'Biological membranes', 'Targeted drug delivery', 'Therapeutic cargo', 'Cellular communication', 'Drug release kinetics', 'Biocompatibility', 'Drug encapsulation', 'In vivo studies', 'Cellular uptake', 'Drug loading efficiency']",
        "nechakhin_mappings": 3,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Exosome-based nanocarriers as bio-inspired and versatile vehicles for drug delivery: recent advances and challenges",
        "abstract": "Recent decades have witnessed the fast and impressive development of nanocarriers as a drug delivery system. Considering the safety, delivery efficiency and stability of nanocarriers, there are many obstacles in accomplishing successful clinical translation of these nanocarrier-based drug delivery systems. The gap has urged drug delivery scientists to develop innovative nanocarriers with high compatibility, stability and longer circulation time. Exosomes are nanometer-sized, lipid-bilayer-enclosed extracellular vesicles secreted by many types of cells. Exosomes serving as versatile drug vehicles have attracted increasing attention due to their inherent ability of shuttling proteins, lipids and genes among cells and their natural affinity to target cells. Attractive features of exosomes, such as nanoscopic size, low immunogenicity, high biocompatibility, encapsulation of various cargoes and the ability to overcome biological barriers, distinguish them from other nanocarriers. To date, exosome-based nanocarriers delivering small molecule drugs as well as bioactive macromolecules have been developed for the treatment of many prevalent and obstinate diseases including cancer, CNS disorders and some other degenerative diseases. Exosome-based nanocarriers have a huge prospect in overcoming many hindrances encountered in drug and gene delivery. This review highlights the advances as well as challenges of exosome-based nanocarriers as drug vehicles. Special focus has been placed on the advantages of exosomes in delivering various cargoes and in treating obstinate diseases, aiming to offer new insights for exploring exosomes in the field of drug delivery.",
        "dimensions": [
            [
                "Extracellular vesicles",
                "Drug delivery",
                "Nanoparticles",
                "Biological membranes",
                "Cellular communication",
                "Therapeutic applications",
                "Biological transport",
                "Drug targeting",
                "Pharmacokinetics",
                "Biocompatibility"
            ],
            [
                "Type of extracellular vesicles",
                "Isolation method",
                "Cargo loading method",
                "Drug type",
                "Particle size",
                "Surface modification",
                "Targeting ligands",
                "In vivo distribution"
            ],
            [
                "Type of Extracellular Vesicles",
                "Drug Payload",
                "Targeted Disease or Condition",
                "Loading and Release Mechanisms",
                "Biological Source",
                "In vitro or In vivo Studies",
                "Characterization Techniques",
                "Therapeutic Outcome"
            ],
            [
                "Type of extracellular vesicles",
                "Source of extracellular vesicles",
                "Cargo loading method",
                "Targeting ligands",
                "In vitro/in vivo models",
                "Therapeutic payload",
                "Characterization techniques",
                "Biological effects"
            ],
            [
                "Drug delivery system",
                "Extracellular vesicles source",
                "Cargo selection",
                "Targeting mechanism",
                "Biological barriers",
                "Therapeutic payload",
                "Release kinetics"
            ]
        ],
        "impr+abs": [
            "Drug delivery system",
            "Nanocarrier properties",
            "Exosome characteristics",
            "Disease treatment"
        ],
        "impr+120b": [
            "research problem",
            "carrier type",
            "application domain",
            "target therapeutic area",
            "delivery mechanism",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Nanocarrier type",
            "Cargo type",
            "Target disease area",
            "Advantages",
            "Challenges"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139645",
        "research_problem": "Search for the sequestration and preservation of organic material on Mars",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Mars', 'Sequestration', 'Preservation', 'Organic material']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Clay minerals in delta deposits and organic preservation potential on Mars",
        "abstract": "Clay-rich sedimentary deposits are often sites of organic matter preservation1,2, and have therefore been sought in Mars exploration3. However, regional deposits of hydrous minerals, including phyllosilicates and sulphates4,5, are not typically associated with valley networks and layered sediments that provide geomorphic evidence of surface water transport on early Mars6,7,8. The Compact Reconnaissance Imaging Spectrometer for Mars (CRISM)9has recently identified phyllosilicates10within three lake basins with fans or deltas that indicate sustained sediment deposition: Eberswalde crater7,11,12, Holden crater12,13and Jezero crater14. Here we use high-resolution data from the Mars Reconnaissance Orbiter (MRO) to identify clay-rich fluvial\u2013lacustrine sediments within Jezero crater, which has a diameter of 45\u2009km. The crater is an open lake basin on Mars with sedimentary deposits of hydrous minerals sourced from a smectite-rich catchment in the Nili\u00a0Fossae region. We find that the two deltas and the lowest observed stratigraphic layer within the crater host iron\u2013magnesium smectite clay. Jezero crater holds sediments that record multiple episodes of aqueous activity on early Mars. We suggest that this depositional setting and the smectite mineralogy make these deltaic deposits well suited for the sequestration and preservation of organic material.",
        "dimensions": [
            [
                "Mars",
                "Sequestration",
                "Preservation",
                "Organic material",
                "Astrobiology",
                "Planetary science",
                "Extraterrestrial life",
                "Mars rovers",
                "Mars missions",
                "Geological processes on Mars"
            ],
            [
                "sequestration method",
                "preservation method",
                "organic material type",
                "Mars environment conditions",
                "duration of preservation",
                "detection method"
            ],
            [
                "Research Problem",
                "Keywords",
                "Space Missions",
                "Geological Features",
                "Microbial Life",
                "Analytical Techniques",
                "Planetary Protection",
                "Climate and Environmental Conditions",
                "Astrobiology",
                "Remote Sensing"
            ],
            [
                "Location",
                "Mars mission",
                "Organic material type",
                "Environmental conditions",
                "Preservation methods",
                "Sequestration techniques",
                "Mars analog environments",
                "Instrumentation"
            ],
            [
                "Research problem",
                "Organic material",
                "Preservation method",
                "Location",
                "Sequestration method",
                "Analysis technique"
            ]
        ],
        "impr+abs": [
            "Study location",
            "Mineral identification",
            "Sediment type",
            "Mars exploration",
            "Organic preservation potential"
        ],
        "impr+120b": [
            "Research problem",
            "Study location",
            "Target material",
            "Process investigated",
            "Objective"
        ],
        "impr+120b+abs": [
            "Study location",
            "Planetary body",
            "Instrument",
            "Data used",
            "Research problem",
            "Methodology",
            "Target mineral",
            "Depositional environment"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139649",
        "research_problem": "Search for hydrated silicate minerals on Mars using orbital remote sensing",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Mars', 'hydrated silicate minerals', 'orbital remote sensing']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Hydrated silicate minerals on Mars observed by the Mars Reconnaissance Orbiter CRISM instrument",
        "abstract": "Phyllosilicates, a class of hydrous mineral first definitively identified on Mars by the OMEGA (Observatoire pour la Mineralogie, L\u2019Eau, les Glaces et l\u2019Activiti\u00e9) instrument1,2, preserve a record of the interaction of water with rocks on Mars. Global mapping showed that phyllosilicates are widespread but are apparently restricted to ancient terrains and a relatively narrow range of mineralogy (Fe/Mg and Al smectite clays). This was interpreted to indicate that phyllosilicate formation occurred during the Noachian (the earliest geological era of Mars), and that the conditions necessary for phyllosilicate formation (moderate to high pH and high water activity3) were specific to surface environments during the earliest era of Mars\u2019s history4. Here we report results from the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM)4of phyllosilicate-rich regions. We expand the diversity of phyllosilicate mineralogy with the identification of kaolinite, chlorite and illite or muscovite, and a new class of hydrated silicate (hydrated silica). We observe diverse Fe/Mg-OH phyllosilicates and find that smectites such as nontronite and saponite are the most common, but chlorites are also present in some locations. Stratigraphic relationships in the Nili Fossae region show olivine-rich materials overlying phyllosilicate-bearing units, indicating the cessation of aqueous alteration before emplacement of the olivine-bearing unit. Hundreds of detections of Fe/Mg phyllosilicate in rims, ejecta and central peaks of craters in the southern highland Noachian cratered terrain indicate excavation of altered crust from depth. We also find phyllosilicate in sedimentary deposits clearly laid by water. These results point to a rich diversity of Noachian environments conducive to habitability.\n",
        "dimensions": [
            [
                "Mineralogy",
                "Hydrated silicate minerals",
                "Mars",
                "Orbital remote sensing",
                "Planetary geology",
                "Spectroscopy",
                "Mineral identification",
                "Geological mapping"
            ],
            [
                "mineral composition",
                "orbital remote sensing technique",
                "spectral signature",
                "geographical location",
                "depth of hydration",
                "mineral abundance"
            ],
            [
                "Mineral Composition",
                "Spectral Signatures",
                "Orbital Remote Sensing Techniques",
                "Martian Geology",
                "Data Processing and Analysis",
                "Instrumentation",
                "Environmental Conditions"
            ],
            [
                "Type of mineral",
                "Orbital remote sensing technique",
                "Location on Mars",
                "Spectral signature",
                "Instrumentation",
                "Data processing techniques",
                "Geological context",
                "Previous research findings"
            ],
            [
                "Research problem",
                "Study location",
                "Data source",
                "Remote sensing method",
                "Mineral type",
                "Data analysis method"
            ]
        ],
        "impr+abs": [
            "Mineral identification",
            "Geological era",
            "Mineral diversity",
            "Surface environment",
            "Stratigraphic relationships",
            "Habitability indication",
            "Instrument used",
            "Detection locations"
        ],
        "impr+120b": [
            "research problem",
            "study location",
            "target material",
            "data source",
            "methodology",
            "instrumentation"
        ],
        "impr+120b+abs": [
            "Instrument",
            "Study location",
            "Mineral types identified",
            "Geological era",
            "Spectroscopic method",
            "Scientific implications"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139653",
        "research_problem": "To estimate mineralogical diversity in Mawrth Vallis region on Mars",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Mawrth Vallis region', 'Mars', 'mineralogical diversity']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Phyllosilicate Diversity and Past Aqueous Activity Revealed at Mawrth Vallis, Mars",
        "abstract": "Observations by the Mars Reconnaissance Orbiter/Compact Reconnaissance Imaging Spectrometer for Mars in the Mawrth Vallis region show several phyllosilicate species, indicating a wide range of past aqueous activity. Iron/magnesium (Fe/Mg)\u2013smectite is observed in light-toned outcrops that probably formed via aqueous alteration of basalt of the ancient cratered terrain. This unit is overlain by rocks rich in hydrated silica, montmorillonite, and kaolinite that may have formed via subsequent leaching of Fe and Mg through extended aqueous events or a change in aqueous chemistry. A spectral feature attributed to an Fe2+phase is present in many locations in the Mawrth Vallis region at the transition from Fe/Mg-smectite to aluminum/silicon (Al/Si)\u2013rich units. Fe2+-bearing materials in terrestrial sediments are typically associated with microorganisms or changes in pH or cations and could be explained here by hydrothermal activity. The stratigraphy of Fe/Mg-smectite overlain by a ferrous phase, hydrated silica, and then Al-phyllosilicates implies a complex aqueous history.\n",
        "dimensions": [
            [
                "Mawrth Vallis region",
                "mineralogical diversity",
                "Mars",
                "geological composition",
                "remote sensing",
                "spectroscopy",
                "mineral identification",
                "planetary geology"
            ],
            [
                "Mawrth Vallis region",
                "mineralogical composition",
                "mineral types",
                "mineral abundance",
                "spectral data",
                "geological formations",
                "Mars",
                "remote sensing",
                "mineral identification techniques"
            ],
            [
                "geographic location",
                "mineralogical composition",
                "geological history",
                "environmental conditions",
                "remote sensing techniques"
            ],
            [
                "Location",
                "Mineralogical composition",
                "Geological history",
                "Remote sensing data",
                "Spectral analysis techniques",
                "Environmental conditions",
                "Geological features",
                "Data sources"
            ],
            [
                "Research problem",
                "Study location",
                "Estimation method",
                "Data source"
            ]
        ],
        "impr+abs": [
            "Observation method",
            "Mineral composition",
            "Geological location",
            "Aqueous activity"
        ],
        "impr+120b": [
            "research problem",
            "objective",
            "study location",
            "target region",
            "subject of study",
            "estimation approach"
        ],
        "impr+120b+abs": [
            "Study location",
            "Instrument",
            "Data type",
            "Research problem",
            "Analysis method",
            "Minerals identified",
            "Interpretation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139657",
        "research_problem": "Search for carbonates on Mars despite the acidic weathering",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['carbonates', 'Mars', 'acidic weathering']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Orbital Identification of Carbonate-Bearing Rocks on Mars",
        "abstract": "Geochemical models for Mars predict carbonate formation during aqueous alteration. Carbonate-bearing rocks had not previously been detected on Mars' surface, but Mars Reconnaissance Orbiter mapping reveals a regional rock layer with near-infrared spectral characteristics that are consistent with the presence of magnesium carbonate in the Nili Fossae region. The carbonate is closely associated with both phyllosilicate-bearing and olivine-rich rock units and probably formed during the Noachian or early Hesperian era from the alteration of olivine by either hydrothermal fluids or near-surface water. The presence of carbonate as well as accompanying clays suggests that waters were neutral to alkaline at the time of its formation and that acidic weathering, proposed to be characteristic of Hesperian Mars, did not destroy these carbonates and thus did not dominate all aqueous environments.\n",
        "dimensions": [
            [
                "Mars",
                "carbonates",
                "acidic weathering",
                "planetary geology",
                "mineralogy",
                "astrobiology",
                "geochemistry",
                "remote sensing",
                "rover missions"
            ],
            [
                "carbonates",
                "Mars",
                "acidic weathering",
                "geological formations",
                "spectral analysis",
                "rover data",
                "mineral composition",
                "weathering processes"
            ],
            [
                "Mineralogy",
                "Mars Environment",
                "Remote Sensing",
                "Geochemistry",
                "Astrobiology",
                "Planetary Geology",
                "Space Missions"
            ],
            [
                "Type of mineral",
                "Martian environment",
                "Remote sensing techniques",
                "Geological formations",
                "Instrumentation",
                "Previous missions",
                "Acidic weathering effects",
                "Data analysis methods",
                "Astrobiology implications"
            ],
            [
                "Research problem",
                "Study location",
                "Data source",
                "Weathering condition"
            ]
        ],
        "impr+abs": [
            "Geochemical models",
            "Study location",
            "Rock characteristics",
            "Formation process",
            "Time period",
            "Aqueous environment"
        ],
        "impr+120b": [
            "research problem",
            "Study location",
            "Target compound",
            "Environmental condition",
            "Detection method"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data used",
            "Study location",
            "Geological period",
            "Mineral detection method",
            "Interpretation of aqueous conditions"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139661",
        "research_problem": "To determine the mineralogy of Gale crater on Mars",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Mineral composition', 'Gale crater', 'Mars', 'Martian mineralogy', 'Remote sensing', 'Geological composition', 'Chemical analysis', 'Spectroscopy', 'Mineral identification']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Mineralogy of the MSL Curiosity landing site in Gale crater as observed by MRO/CRISM",
        "abstract": "Orbital data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) and High Resolution Imaging Science Experiment instruments on the Mars Reconnaissance Orbiter (MRO) provide a synoptic view of compositional stratigraphy on the floor of Gale crater surrounding the area where the Mars Science Laboratory (MSL) Curiosity landed. Fractured, light-toned material exhibits a 2.2 \u00b5m absorption consistent with enrichment in hydroxylated silica. This material may be distal sediment from the Peace Vallis fan, with cement and fracture fill containing the silica. This unit is overlain by more basaltic material, which has 1 \u00b5m and 2 \u00b5m absorptions due to pyroxene that are typical of Martian basaltic materials. Both materials are partially obscured by aeolian dust and basaltic sand. Dunes to the southeast exhibit differences in mafic mineral signatures, with barchan dunes enhanced in olivine relative to pyroxene-containing longitudinal dunes. This compositional difference may be related to aeolian grain sorting.",
        "dimensions": [
            [
                "Mars",
                "Gale crater",
                "mineralogy",
                "planetary science",
                "Martian geology",
                "crater analysis",
                "remote sensing",
                "spectroscopy",
                "rover missions"
            ],
            [
                "location",
                "mineralogy",
                "Gale crater",
                "Mars",
                "spectroscopy",
                "rover",
                "geological features",
                "rock samples"
            ],
            [
                "Location",
                "Mars Mission",
                "Mineralogy",
                "Remote Sensing",
                "Geological Features",
                "Instrumentation",
                "Data Analysis"
            ],
            [
                "Location",
                "Mars mission",
                "Mineralogical analysis techniques",
                "Geological context",
                "Instrumentation"
            ],
            [
                "Study location",
                "Research objective",
                "Methodology",
                "Data source"
            ]
        ],
        "impr+abs": [
            "Data source",
            "Study location",
            "Mineral composition",
            "Material characteristics",
            "Geological features",
            "Aeolian activity",
            "Instrumentation",
            "Observation method"
        ],
        "impr+120b": [
            "research problem",
            "study location",
            "target body",
            "objective",
            "analysis technique"
        ],
        "impr+120b+abs": [
            "Study location",
            "Instruments",
            "Data type",
            "Analysis technique",
            "Target minerals",
            "Key observations",
            "Research objective"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139664",
        "research_problem": "Estimation of seasonal impact on Martian gully formation from spectroscopic studies",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['Martian gully formation', 'spectroscopic studies', 'estimation', 'seasonal impact']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "New insights into gully formation on Mars: Constraints from composition as seen by MRO/CRISM",
        "abstract": "Over 100 Martian gully sites were analyzed using orbital data collected by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) and High Resolution Imaging Science Experiment on the Mars Reconnaissance Orbiter (MRO). Most gullies are spectrally indistinct from their surroundings, due to mantling by dust. Where spectral information on gully sediments was obtained, a variety of mineralogies were identified. Their relationship to the source rock suggests that gully-forming processes transported underlying material downslope. There is no evidence for specific compositions being more likely to be associated with gullies or with the formation of hydrated minerals in situ as a result of recent liquid water activity. Seasonal CO2and H2O frosts were observed in gullies at middle to high latitudes, consistent with seasonal frost-driven processes playing important roles in the evolution of gullies. Our results do not clearly indicate a role for long-lived liquid water in gully formation and evolution.",
        "dimensions": [
            [
                "Martian gully formation",
                "Spectroscopic studies",
                "Seasonal impact",
                "Mars",
                "Planetary geology",
                "Climate variations",
                "Surface features",
                "Mineral composition",
                "Remote sensing",
                "Geological processes"
            ],
            [
                "Martian gully formation",
                "seasonal impact",
                "spectroscopic studies",
                "Martian surface composition",
                "seasonal variation",
                "spectral analysis",
                "gully morphology",
                "Martian atmosphere",
                "spectral resolution"
            ],
            [
                "Mars Geography",
                "Spectroscopic Studies",
                "Seasonal Variation",
                "Gully Formation",
                "Remote Sensing",
                "Comparative Planetary Studies",
                "Data Analysis and Modeling"
            ],
            [
                "Spectroscopic technique",
                "Seasonal variation",
                "Gully characteristics",
                "Martian geography",
                "Spectral features",
                "Data collection method",
                "Geological context",
                "Publication date"
            ],
            [
                "Research problem",
                "Data source",
                "Analysis method",
                "Study location",
                "Temporal factor"
            ]
        ],
        "impr+abs": [
            "Study location",
            "Data collection method",
            "Mineralogical composition",
            "Formation process",
            "Seasonal activity",
            "Liquid water evidence",
            "Evolution process"
        ],
        "impr+120b": [
            "research problem",
            "study location",
            "phenomenon",
            "temporal scope",
            "data source",
            "analysis method"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data sources",
            "Study location",
            "Analysis method",
            "Key observations",
            "Conclusions"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139667",
        "research_problem": "Analysis of spectroscopic data for Mud volcanism on Mars",
        "orkg_properties": "['research problem', 'References', 'Study Area', 'Instrument', 'Processing']",
        "nechakhin_result": "['spectroscopic technique used', 'type of mud volcanism', 'analysis parameters', 'features observed in the spectral data', 'location on Mars', 'time period of data collection', 'research methodology']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Insights into Mars mud volcanism using visible and near-infrared spectroscopy",
        "abstract": "Mudvolcanism(MV) has been a proposed formation mechanism for positive-relieflandformsin thelowland, equatorial, andhighland regionsof Mars. While visible and near-infrared (VNIR) spectroscopy has been used in a few cases to argue for the presence of MV on the surface of Mars, data from the CompactReconnaissanceImaging Spectrometer for Mars (CRISM) remain underutilized. We conducted a global examination of proposed Mars MV features using CRISM VNIR data. We observe variable hydration states and place constraints on the composition of these features from orbit. We do not confidently identifyphyllosilicates, carbonates, or sulfates associated with suggested Martianmud volcanoes. However, specific structures in Valles Marineris exhibitVNIR signaturesconsistent with unaltered hydrated glass of a volcanic origin and high-Capyroxene. CRISM visible data from MV features reveal consistent nanophaseferric oxidesignatures on a global scale, although these signatures are not unique to Mars MV materials. Limitations in specific mineral detection are likely due to the fine grain size and/or textural characteristics of putative MV features. While we do not argue in favor of a specific proposed MV site in the context of future robotic or human missions, the insights of this study could be used as a guide for Mars surface exploration.\nMud volcanism has been proposed to explain the formation of morphologically diverse edifices across the surface of Mars. Previous global compositional analysis of surface features on Mars, such as pitted cones and knobs, interpreted as mud volcanoes using Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) targeted data revealed consistent water hydration signatures associated with these landforms (Dapremont and Wray, 2021b). However, no definitive identifications of specific minerals known to comprise mud volcanism materials on Earth (e.g., phyllosilicates, carbonates, sulfates) were identified. Here, we take advantage of an alternative methodology involving a quantitative color band ratio technique using blue-green (BG), red (RED), and infrared (IR) reflectance data sourced from the High Resolution Imaging Science Experiment (HiRISE) camera to determine if any additional mineralogical information about proposed Martian mud volcanoes could be acquired. Across Mars in the three distinct study regions of Valles Marineris, Terra Sirenum, and Arabia Terra, proposed mud materials consistently plot with ferric laboratory and Mars surface reference minerals. In Terra Sirenum, HiRISE color data point clusters acquired from mounds attributed to mud volcanism plot overtop the Fe-smectite reference mineral nontronite. We show that HiRISE color data can be used as a complementary mineralogical investigation tool applied to the study of surface materials on Mars and demonstrate the value of HiRISE color ratios for comparative planetology.\nHighlights\u2022Proposed mud volcanism materials on Mars exhibit variable hydration states.\u2022Valles Marineris mud features exhibit unaltered hydrated glass and HCP signatures.\u2022Consistent nanophase ferric oxide signatures are not unique to Mars mud volcanoes\u2022Mars mud volcanism mineral detection limitations include grain size and/or texture.\nHighlights\u2022Proposed mud volcanism materials on Mars exhibit variable hydration states.\u2022Valles Marineris mud features exhibit unaltered hydrated glass and HCP signatures.\u2022Consistent nanophase ferric oxide signatures are not unique to Mars mud volcanoes\u2022Mars mud v",
        "dimensions": [
            [
                "Spectroscopic techniques",
                "Mud volcanism",
                "Mars",
                "Planetary geology",
                "Remote sensing",
                "Mineralogy",
                "Volcanic processes",
                "Geological features",
                "Space exploration",
                "Astrobiology"
            ],
            [
                "spectroscopic technique",
                "data collection method",
                "spectral range",
                "geological features",
                "Mars location",
                "comparative terrestrial analogs",
                "data processing software",
                "research team",
                "publication date"
            ],
            [
                "Spectroscopic Techniques",
                "Mud Volcanism",
                "Mars Geology",
                "Planetary Spectroscopy",
                "Remote Sensing",
                "Comparative Planetology"
            ],
            [
                "Spectroscopic technique",
                "Mars location",
                "Mud composition",
                "Geological context",
                "Data processing methods",
                "Instrumentation",
                "Publication date"
            ],
            [
                "Data source",
                "Spectroscopic technique",
                "Study location",
                "Data analysis method",
                "Research problem",
                "Findings",
                "Data interpretation"
            ]
        ],
        "impr+abs": [
            "Study location",
            "Data source",
            "Mineral composition",
            "Surface features",
            "Limitations",
            "Exploration implications"
        ],
        "impr+120b": [
            "research problem",
            "data type",
            "study location",
            "phenomenon",
            "analysis method"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data used",
            "Methodology",
            "Study region",
            "Findings",
            "Limitations",
            "Implications"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140690",
        "research_problem": "Integration of hyperspectral and multispectral thermal imagery for geological mapping at Cuprite, Nevada",
        "orkg_properties": "['Data used', 'research problem', 'Analysis', 'Processing']",
        "nechakhin_result": "['Imagery type (hyperspectral, multispectral thermal)',\n 'Geological mapping',\n 'Location (Cuprite, Nevada)']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Integrating visible, near-infrared and short-wave infrared hyperspectral and multispectral thermal imagery for geological mapping at Cuprite, Nevada",
        "abstract": "This study investigated the potential value of integrating hyperspectral visible, near-infrared, and short-wave infrared imagery with multispectral thermal data for geological mapping. Two coregistered aerial data sets of Cuprite, Nevada were used: Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) hyperspectral data, and MODIS/ASTER Airborne Simulator (MASTER) multispectral thermal data. Four classification methods were each applied to AVIRIS, MASTER, and a combined set. Confusion matrices were used to assess the classification accuracy. The assessment showed, in terms of kappa coefficient, that most classification methods applied to the combined data achieved a marked improvement compared to the results using either AVIRIS or MASTER thermal infrared (TIR) data alone. Spectral angle mapper (SAM) showed the best overall classification performance. Minimum distance classification had the second best accuracy, followed by spectral feature fitting (SFF) and maximum likelihood classification. The results of the study showed that SFF applied to the combination of AVIRIS with MASTER TIR data are especially valuable for identification of silicified alteration and quartzite, both of which exhibit distinctive features in the TIR region. SAM showed some advantages over SFF in dealing with multispectral TIR data, obtaining higher accuracy in discriminating low albedo volcanic rocks and limestone which do not have unique, distinguishing features in the TIR region.\nThe spectral angle mapper (SAM), as a spectral matching method, has been widely used in lithological type identification and mapping using hyperspectral data. The SAM quantifies the spectral similarity between an image pixel spectrum and a reference spectrum with known components. In most existing studies a mean reflectance spectrum has been used as the reference spectrum for a specific lithological class. However, this conventional use of SAM does not take into account the spectral variability, which is an inherent property of many rocks and is further magnified in remote sensing data acquisition process. In this study, two methods of determining reference spectra used in SAM are proposed for the improved lithological mapping. In first method the mean of spectral derivatives was combined with the mean of original spectra, i.e., the mean spectrum and the mean spectral derivative were jointly used in SAM classification, to improve the class separability. The second method is the use of multiple reference spectra in SAM to accommodate the spectral variability. The proposed methods were evaluated in lithological mapping using EO-1 Hyperion hyperspectral data of two arid areas. The spectral variability and separability of the rock types under investigation were also examined and compared using spectral data alone and using both spectral data and first derivatives. The experimental results indicated that spectral variability significantly affected the identification of lithological classes with the conventional SAM method using a mean reference spectrum. The proposed methods achieved significant improvement in the accuracy of lithological mapping, outperforming the conventional use of SAM with a mean spectrum as the reference spectrum, and the matching filtering, a widely used spectral mapping method.\nThis study investigated the potential value of integrating hyperspectral visible, near-infrared, and short-wave infrared imagery with multispectral thermal data for geological mapping. Two c",
        "dimensions": [
            [
                "Imaging technology (hyperspectral, multispectral thermal)",
                "Geological mapping",
                "Cuprite, Nevada",
                "Remote sensing",
                "Data fusion",
                "Spectral analysis",
                "Mineral identification",
                "Geological surveying"
            ],
            [
                "imaging technology",
                "geological mapping",
                "Cuprite, Nevada",
                "spectral resolution",
                "spatial resolution",
                "data fusion method",
                "feature extraction",
                "mineral identification"
            ],
            [
                "Imagery Type",
                "Geological Mapping",
                "Location",
                "Remote Sensing Techniques",
                "Geological Features",
                "Data Integration",
                "Mineralogy and Spectral Signatures",
                "Machine Learning and Classification"
            ],
            [
                "Type of imagery",
                "Geological location",
                "Geological mapping application",
                "Integration method",
                "Geological features of interest"
            ],
            [
                "Data fusion",
                "Geological mapping",
                "Imaging techniques",
                "Study location"
            ]
        ],
        "impr+abs": [
            "Data used",
            "research problem",
            "Classification methods",
            "Evaluation metric",
            "Mapping method",
            "Spectral analysis method",
            "Reference spectra method"
        ],
        "impr+120b": [
            "research problem",
            "data modalities",
            "study location",
            "application domain",
            "methodology",
            "integration approach"
        ],
        "impr+120b+abs": [
            "Study location",
            "Data used",
            "Research problem",
            "Classification approach",
            "Evaluation metric",
            "Spectral variability handling",
            "Reference spectra method"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140694",
        "research_problem": "Mineralogy of Cuprite hills in Nevada of Unitd states from airborne and spaceborne imageries",
        "orkg_properties": "['research problem', 'Analysis', 'Processing']",
        "nechakhin_result": "['Mineral composition', 'Cuprite hills', 'Nevada', 'United States', 'Airborne imagery', 'Spaceborne imagery']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 3,
        "title": "Comparison of airborne hyperspectral data and eo-1 hyperion for mineral mapping",
        "abstract": ":Airborne hyperspectral data have been available to researchers since the early 1980s and their use for geologic applications is well documented. The launch of the National Aeronautics and Space Administration Earth Observing 1 Hyperion sensor in November 2000 marked the establishment of a test bed for spaceborne hyperspectral capabilities. Hyperion covers the 0.4-2.5-/spl mu/m range with 242 spectral bands at approximately 10-nm spectral resolution and 30-m spatial resolution. Analytical Imaging and Geophysics LLC and the Commonwealth Scientific and Industrial Research Organisation have been involved in efforts to evaluate, validate, and demonstrate Hyperions's utility for geologic mapping in a variety of sites in the United States and around the world. Initial results over several sites with established ground truth and years of airborne hyperspectral data show that Hyperion data from the shortwave infrared spectrometer can be used to produce useful geologic (mineralogic) information. Minerals mapped include carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite. Hyperion data collected under optimum conditions (summer season, bright targets, well-exposed geology) indicate that Hyperion data meet prelaunch specifications and allow subtle distinctions such as determining the difference between calcite and dolomite and mapping solid solution differences in micas caused by substitution in octahedral molecular sites. Comparison of airborne hyperspectral data [from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)] to the Hyperion data establishes that Hyperion provides similar basic mineralogic information, with the principal limitation being limited mapping of fine spectral detail under less-than-optimum acquisition conditions (winter season, dark targets) based on lower signal-to-noise ratios. Case histories demonstrate the analysis methodologies and level of information available from the Hyperion data...Show More",
        "dimensions": [
            [
                "Mineralogy",
                "Cuprite hills",
                "Nevada",
                "United States",
                "airborne imagery",
                "spaceborne imagery"
            ],
            [
                "location",
                "mineralogy",
                "Cuprite hills",
                "Nevada",
                "United States",
                "airborne imagery",
                "spaceborne imagery"
            ],
            [
                "Geographic Location",
                "Remote Sensing Data",
                "Mineralogical Composition",
                "Spectral Analysis Techniques",
                "Geological Context",
                "Data Processing Methods",
                "Research Objectives"
            ],
            [
                "Location",
                "Imaging technology",
                "Mineral composition",
                "Spectral bands",
                "Data processing techniques",
                "Geological context",
                "Remote sensing instruments"
            ],
            [
                "Study location",
                "Data source",
                "Imaging technology",
                "Mineralogy analysis"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Study location",
            "Minerals mapped",
            "Comparison method",
            "Analysis methodology"
        ],
        "impr+120b": [
            "research problem",
            "Study location",
            "Subject of study",
            "Data source",
            "Data type"
        ],
        "impr+120b+abs": [
            "Data sources",
            "Sensor specifications",
            "Study locations",
            "Target minerals",
            "Evaluation criteria",
            "Analysis methodology"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140698",
        "research_problem": "Application of ASTER data for hydrothermal geological mapping at Cuprite, Nevada",
        "orkg_properties": "['Data used', 'research problem', 'Analysis', 'Processing']",
        "nechakhin_result": "['Remote sensing data sources',\n 'ASTER data',\n 'Hydrothermal geological mapping',\n 'Cuprite, Nevada',\n 'Geographical location',\n 'Geological features',\n 'Mineral exploration',\n 'Spectral bands',\n 'Spectral resolution',\n 'Spatial resolution',\n 'Temporal resolution',\n 'Data preprocessing techniques',\n 'Classification algorithms',\n 'Image enhancement techniques']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Mapping Hydrothermally Altered Rocks at Cuprite, Nevada, Using the Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), a New Satellite-Imaging System",
        "abstract": "The Advanced Spaceborne Thermal Emission and Reflection  Radiometer (ASTER) is a 14-band multispectral instrument on board the Earth  Observing System (EOS), TERRA. The three bands between 0.52 and 0.86\u03bcm and  the six bands from 1.60 and 2.43\u03bcm, which have 15- and 30-m spatial  resolution, respectively, were selected primarily for making remote  mineralogical determinations.The Cuprite, Nevada, mining district comprises two  hydrothermal alteration centers where Tertiary volcanic rocks have been  hydrothermally altered mainly to bleached silicified rocks and opalized  rocks, with a marginal zone of limonitic argillized rocks. Country rocks are  mainly Cambrian phyllitic siltstone and limestone.Evaluation of an ASTER image of the Cuprite district  shows that spectral reflectance differences in the nine bands in the 0.52 to  2.43\u03bcm region provide a basis for identifying and mapping mineralogical  components which characterize the main hydrothermal alteration zones: opal  is the spectrally dominant mineral in the silicified zone; whereas, alunite  and kaolinite are dominant in the opalized zone. In addition, the  distribution of unaltered country rocks was mapped because of the presence  of spectrally dominant muscovite in the siltstone and calcite in limestone,  and the tuffaceous rocks and playa deposits were distinguishable due to  their relatively flat spectra and weak absorption features at 2.33 and 2.20\u03bcm, respectively.An Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)  image of the study area was processed using a similar methodology used with  the ASTER data. Comparison of the ASTER and AVIRIS results shows that the  results are generally similar, but the higher spectral resolution of AVIRIS  (224 bands) permits identification of more individual minerals, including  certain polymorphs. However, ASTER has recorded images of more than 90  percent of the Earth\u2019s  land surface with less than 20 percent cloud cover, and these data are  available at nominal or no cost. Landsat TM images have a similar spatial  resolution to ASTER images, but TM has fewer bands, which limits its  usefulness for making mineral determinations.\nArticle Navigation",
        "dimensions": [
            [
                "Remote sensing data",
                "ASTER data",
                "Hydrothermal geological mapping",
                "Cuprite, Nevada",
                "Geological mapping",
                "Hydrothermal systems",
                "Geological remote sensing",
                "Mineral exploration",
                "Geological features",
                "Geological analysis"
            ],
            [
                "Remote sensing data source",
                "Geological mapping",
                "Hydrothermal features",
                "Cuprite, Nevada",
                "ASTER data processing",
                "Spectral bands",
                "Spatial resolution",
                "Temporal resolution",
                "Data acquisition date"
            ],
            [
                "Remote Sensing Data",
                "Geological Mapping Techniques",
                "Study Area",
                "Hydrothermal Systems",
                "Geological Features",
                "Data Processing and Analysis",
                "Geological Signatures"
            ],
            [
                "Satellite data source",
                "Geological features of interest",
                "Data processing techniques",
                "Spectral bands used",
                "Validation methods",
                "Geological mapping scale",
                "Geological mapping accuracy",
                "Comparison with ground truth data"
            ],
            [
                "Data source",
                "Geological mapping method",
                "Study location",
                "Data analysis technique"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Study location",
            "Imaging system",
            "Mineral identification",
            "Comparison method",
            "Spatial resolution",
            "Cost",
            "Remote sensing technology"
        ],
        "impr+120b": [
            "data source",
            "study location",
            "research problem",
            "methodology",
            "application domain"
        ],
        "impr+120b+abs": [
            "Study location",
            "Remote sensing instrument",
            "Spectral resolution",
            "Data sources",
            "Target minerals",
            "Methodology",
            "Comparison with other sensors"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140706",
        "research_problem": "Lithologic discrimination and mapping at Nevada using ASTER multispectral data",
        "orkg_properties": "['Data used', 'research problem', 'Analysis']",
        "nechakhin_result": "['Lithologic composition',\n 'Mapping techniques',\n 'Nevada region',\n 'ASTER multispectral data']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "Spectral indices for lithologic discrimination and mapping by using the ASTER SWIR bands",
        "abstract": "The Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) is a research facility instrument launched on NASA's Terra spacecraft in December 1999. Spectral indices, a kind of orthogonal transformation in the five-dimensional space formed by the five ASTER short-wave-infrared (SWIR) bands, were proposed for discrimination and mapping of surface rock types. These include Alunite Index, Kaolinite Index, Calcite Index, and Montmorillonite Index, and can be calculated by linear combination of reflectance values of the five SWIR bands. The transform coefficients were determined so as to direct transform axes to the average spectral pattern of the typical minerals. The spectral indices were applied to the simulated ASTER dataset of Cuprite, Nevada, USA after converting its digital numbers to surface reflectance. The resultant spectral index images were useful for lithologic mapping and were easy to interpret geologically. An advantage of this method is that we can use the pre-determined transform coefficients, as long as image data are converted to surface reflectance.\n",
        "dimensions": [
            [
                "Remote sensing",
                "ASTER multispectral data",
                "Lithologic discrimination",
                "Mapping",
                "Nevada",
                "Geology",
                "Spectral analysis"
            ],
            [
                "Lithology",
                "Discrimination method",
                "Mapping technique",
                "Nevada region",
                "ASTER multispectral data"
            ],
            [
                "Geographic Location",
                "Remote Sensing Data",
                "Lithologic Discrimination Techniques",
                "Geological Context",
                "ASTER Data Processing",
                "Geological Mapping",
                "Mineralogy and Petrology"
            ],
            [
                "Geographic location",
                "ASTER multispectral data",
                "Lithologic discrimination methods",
                "Geological formations",
                "Remote sensing techniques",
                "Spectral bands",
                "Data processing and analysis",
                "Accuracy assessment"
            ],
            [
                "Study location",
                "Data used",
                "Mapping technique",
                "Remote sensing method",
                "Geological feature",
                "Mineral identification",
                "Spectral bands",
                "Accuracy assessment"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Spectral indices",
            "Mapping method",
            "Advantages"
        ],
        "impr+120b": [
            "research problem",
            "study location",
            "data used",
            "methodology",
            "analysis technique",
            "mapping approach"
        ],
        "impr+120b+abs": [
            "Instrument",
            "Data source",
            "Methodology",
            "Derived spectral indices",
            "Study location",
            "Result"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140710",
        "research_problem": "Mineral mapping at Nevada using diagnostic absorption feature in SWIR wavelength range",
        "orkg_properties": "['Data used', 'research problem', 'Analysis', 'Processing', 'reference']",
        "nechakhin_result": "['geographic location', 'mineral mapping technique', 'SWIR wavelength range', 'absorption features', 'Nevada']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Simple mineral mapping algorithm based on multitype spectral diagnostic absorption features: a case study at Cuprite, Nevada",
        "abstract": "Hyperspectral remote sensing has been widely used in mineral identification using the particularly useful short-wave infrared (SWIR) wavelengths (1.0 to2.5\u03bcm2.5\u03bcm). Current mineral mapping methods are easily limited by the sensor\u2019s radiometric sensitivity and atmospheric effects. Therefore, a simple mineral mapping algorithm (SMMA) based on the combined application with multitype diagnostic SWIR absorption features for hyperspectral data is proposed. A total of nine absorption features are calculated, respectively, from the airborne visible/infrared imaging spectrometer data, the Hyperion hyperspectral data, and the ground reference spectra data collected from the United States Geological Survey (USGS) spectral library. Based on spectral analysis and statistics, a mineral mapping decision-tree model for the Cuprite mining district in Nevada, USA, is constructed. Then, the SMMA algorithm is used to perform mineral mapping experiments. The mineral map from the USGS (USGS map) in the Cuprite area is selected for validation purposes. Results showed that the SMMA algorithm is able to identify most minerals with high coincidence with USGS map results. Compared with Hyperion data (overall accuracy=74.54%), AVIRIS data showed overall better mineral mapping results (overall accuracy=94.82%) due to low signal-to-noise ratio and high spatial resolution.",
        "dimensions": [
            [
                "Mineral mapping",
                "Nevada",
                "SWIR wavelength range",
                "Absorption features",
                "Remote sensing",
                "Geological mapping",
                "Spectroscopy",
                "Hyperspectral imaging"
            ],
            [
                "location",
                "mineral type",
                "spectral wavelength range",
                "mapping method",
                "absorption feature",
                "remote sensing technology"
            ],
            [
                "Geographic Location",
                "Spectral Wavelength Range",
                "Remote Sensing Techniques",
                "Mineralogy and Geology",
                "Data Analysis and Processing",
                "Machine Learning and Classification",
                "Environmental Applications"
            ],
            [
                "Geographic location",
                "Spectral wavelength range",
                "Remote sensing technology",
                "Mineral types",
                "Data processing techniques",
                "Spectral resolution",
                "Validation methods",
                "Environmental conditions"
            ],
            [
                "Study location",
                "Mineral type",
                "Spectral feature",
                "Data collection method",
                "Analysis technique"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Mineral identification method",
            "Study location",
            "Accuracy assessment",
            "Comparison method"
        ],
        "impr+120b": [
            "research problem",
            "Study location",
            "Methodology",
            "Spectral range",
            "Data type",
            "Analysis technique"
        ],
        "impr+120b+abs": [
            "Data used",
            "Study location",
            "Algorithm",
            "Methodology",
            "Validation data",
            "Evaluation metric",
            "Results"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140808",
        "research_problem": "Hyperion data for mineralogcal analysis of Lonar crater in India",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Software', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Hyperion data', 'mineralogical analysis', 'Lonar crater', 'India']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Exploring the Mineralogy at Lonar Crater with Hyperspectral Remote Sensing",
        "abstract": "The earth crust is made up of variety of minerals. These minerals are having very significant applications in our day today life. The various studies, characterizing physical, chemical, electrical, structural properties, have been carried out on the Lonar crater for studying mineralogy, surface morphology and geology but has not been done by remote sensing technology. So, the proposed work focused on exploring the mineralogy at the Lonar crater by using high resolution hyperspectral imageries. The spectral reflectance of minerals was characterized by using FieldSpec4 spectroradiometer. The minerals at Lonar crater were explored by performing preprocessing and spectral analysis. The techniques used in the work are Spectral Angle Mapper and Spectral Feature Fitting. The results of the work marked the presence of pigeonite and augite at Lonar crater which indicates that this crater is the result of extrusive volcanic activity. Also, the presence of augite underneath basaltic igneous rocks as the rock type of Lonar crater. The salinity of the Lonar lake is proved by the presence of mirabilite and salt. Thus, the important results of this work are presence of minerals quartz, actinol, jarosite, pigeonite, augite, albite, mirabilite and scolecite. The significance of these minerals related to the crater is discussed here. It also validated the existence of these minerals which were identified through the previous geological, chemical, physical, electrical and magnetic studies and the minerals pyrite, chlorite, richter, illite, limonite, allanite, amphibolite and margarite were also identified.",
        "dimensions": [
            [
                "Hyperion data",
                "mineralogical analysis",
                "Lonar crater",
                "India"
            ],
            [
                "Type of data",
                "Location",
                "Mineralogical analysis method",
                "Time period",
                "Data source",
                "Mineral composition",
                "Data resolution"
            ],
            [
                "Geographic Location",
                "Mineralogical Analysis Techniques",
                "Impact Crater Formation",
                "Hyperion Data Analysis",
                "Mineralogical Composition",
                "Geological Context",
                "Remote Sensing and Satellite Data"
            ],
            [
                "Location",
                "Mineralogical composition",
                "Hyperion data parameters",
                "Geological context",
                "Remote sensing techniques",
                "Mineral identification methods",
                "Geological implications"
            ],
            [
                "Data source",
                "Mineralogical analysis",
                "Study location",
                "Research problem"
            ]
        ],
        "impr+abs": [
            "Study location",
            "Data collection method",
            "Minerals identified",
            "Geological significance",
            "Remote sensing technology",
            "Analysis technique"
        ],
        "impr+120b": [
            "Data used",
            "Study location",
            "Analysis method",
            "Research problem",
            "Remote sensing platform"
        ],
        "impr+120b+abs": [
            "Study location",
            "Data type",
            "Instrument",
            "Analysis method",
            "Target minerals",
            "Research objective"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140812",
        "research_problem": "Hyperion data for characterization of hematite ore mineral classes",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Hyperion data', 'characterization', 'hematite ore', 'mineral classes']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Characterization and mapping of hematite ore mineral classes using hyperspectral remote sensing technique: a case study from Bailadila iron ore mining region",
        "abstract": "The study demonstrates a methodology for mapping various hematite ore classes based on their reflectance and absorption spectra, using Hyperion satellite imagery. Substantial validation is carried out, using the spectral feature fitting technique, with the field spectra measured over the Bailadila hill range in Chhattisgarh State in India. The results of the study showed a good correlation between the concentration of iron oxide with the depth of the near-infrared absorption feature (R2=\u20090.843) and the width of the near-infrared absorption feature (R2=\u20090.812) through different empirical models, with a root-mean-square error (RMSE) between\u2009<\u20090.317 and\u2009<\u20090.409. The overall accuracy of the study is 88.2% with a Kappa coefficient value of 0.81. Geochemical analysis and X-ray fluorescence (XRF) of field ore samples are performed to ensure different classes of hematite ore minerals. Results showed a high content of Fe\u2009>\u200960\u00a0wt% in most of the hematite ore samples, except banded hematite quartzite (BHQ) (<\u200947\u00a0wt%).\nOpen AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visithttp://creativecommons.org/licenses/by/4.0/.Reprints and permissions\nThis research work illustrates the practicability of using Hyperion imagery for mapping various types of hematite ores in hilly and inaccessible locations. The finest processed image spectra assisted in identifying the prominent absorption features (750\u20131100\u00a0nm) related to iron oxide minerals. Matching between image and field spectra through spectral feature fitting technique showed the best correlation. Prominent spectral absorption features of iron ore at a particular wavelength, location, and geochemical analysis of iron ore samples showed a good association. The depth and width of the absorption features of spectra are increasing with the increasing amount of iron content. The depth of absorption is increasing significantly, even a minute increase in the concentration of iron content in the sample. SFF is a robust method to compare the similarity between field and image spectra of Hyperion data. Further, the classified map of Hyperion data resulted from the spectral angle mapper algorithm. RMSE values show the exceptional matching between image and field spectra which leads to an increase in the classification accuracy. The confusion matrix and the value of the Kappa coefficient show the accuracy of the classified output.\nThe study demonstrates a methodology for mapping various hematite ore classes based on their reflectance and absorption spectra, using Hyperion satellite imagery. Substantial validation is carried out, using the spectral feature fitting technique, with the field spectra measured over the Bailadila hill range in Chhattisgarh State in India. The results of t",
        "dimensions": [
            [
                "Hyperion data",
                "characterization",
                "hematite ore",
                "mineral classes"
            ],
            [
                "type of hyperspectral sensor",
                "wavelength range",
                "data preprocessing method",
                "mineral class identification method",
                "spectral resolution",
                "spatial resolution",
                "mineral abundance estimation method"
            ],
            [
                "Mineralogy",
                "Spectroscopy",
                "Remote Sensing",
                "Geology and Ore Deposits",
                "Machine Learning and Data Analysis",
                "Mining and Mineral Processing",
                "Environmental Impact"
            ],
            [
                "Type of hyperspectral imaging technique",
                "Mineral classes of interest",
                "Data preprocessing methods",
                "Spectral resolution",
                "Machine learning algorithms",
                "Geological context",
                "Spectral libraries or references",
                "Validation methods"
            ],
            [
                "Data source",
                "Mineral type",
                "Characterization method",
                "Analysis technique"
            ]
        ],
        "impr+abs": [
            "Mapping methodology",
            "Spectral analysis",
            "Validation technique",
            "Geochemical analysis",
            "Accuracy assessment",
            "Mineral concentration",
            "Satellite imagery",
            "Study location"
        ],
        "impr+120b": [
            "Data source",
            "Research objective",
            "Target material",
            "Analysis method",
            "Application domain"
        ],
        "impr+120b+abs": [
            "Data used",
            "Study location",
            "Methodology",
            "Validation technique",
            "Evaluation metric",
            "Analysis technique",
            "Mineral classes"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140815",
        "research_problem": "Hyperion data for for hydrothermal alteration and base metal minerals",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Software', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Data Source', 'Hyperion Satellite', 'Hydrothermal Alteration', 'Base Metal Minerals']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Mapping Regolith and Gossan for Mineral Exploration in the Eastern Kumaon Himalaya, India using hyperion data and object oriented image classification",
        "abstract": "Crystalline in the KumaonHimalaya, India near Askot area is a prominent site of the base metal mineralization and gossanised surface. This area is hosted by the sulphides and sulphates of Cu, Pb, Zn and Au and Ag mineralization with the altered rocks like sericite chlorite schist,gneissetc. Due to the deep weathering this area is also a good illustration site of the gossanised outcrop.Regolithmapping through the multispectral remotely sensed data using different parametric and nonparametric classification algorithm has been used for many years. In recent years, object oriented classification for classification of object rather than pixel has gained a good success within the geospatial community. On the other hand, space borne hyperspectralremote sensinghas gained a great success in identification of the minerals from space. The narrow contiguous bands of this hyperspectral remote sensing data can provide more information of the chemical content of the different minerals.In this study EO1 (Earth Observation) Hyperion hyperspectral sensor data has been evaluated for regolith andgossanmapping using the object orientedimage classificationtechnique. The efficacy of the hyperion data is evaluated in and around the Askot base metalmineral depositsforhydrothermal alterationand base metal minerals. Three sites were selected by regolith mapping using object oriented image classification method and were evaluated by the aid of hyperspectral data for alteration minerals. During the field verification it was found that the mineralization within these sites was associated with the dolomite, gneiss and schists. Stronghydrothermal activityand shearing were dominated at these sites. Minerals like carbonates, sulphates and sulphides of copper, lead, zinc and silver, magnetite along with the altered minerals like, mica, chlorite, talc etc. were found at the target sites. eCognition\u2122 and ENVI\u00ae software\u2019s were used for the object based classification and for the hyperspectral data processing, respectively. It was concluded from this study that object oriented classification is a suitable classification algorithm for regolith and gossan mapping as it uses the spatial, spectral and textural information of the remote sensing dataset. Target areas suggested by the object oriented classification method were examined by the hyperspectral data analysis. Results of the hyperspectral analysis were in good confirmation of the minerals found in the field as well as in the lab. Though the signal to noise ratio of the scene was low but still hyperion data was able to highlight themineralogyof the Askot and nearby areas. It was also concluded by this study that the spectral analysis was very helpful for identification of the gossan and limonite over the conventionalpetrographyanalysis.\nAirborne particulate matter (PM) has been a major threat to air quality and public health in major cities in China for more than a decade. Green space has been deemed to be effective in mitigating PM pollution; however, few studies have examined its effectiveness at the neighborhood scale. In this study, the authors probe the contributions from different landscape components in the green space (i.e., tree, grass), as well as the spatial scale of planning on fine PM (PM2.5) concentrations in urban neighborhoods. PM2.5data including 37 samples from five megacities were collected from the National Environmental Monitoring Centre in China. Results showed that, neighborhood green space greatly ",
        "dimensions": [
            [
                "Geological data",
                "Hydrothermal alteration",
                "Base metal minerals",
                "Remote sensing data",
                "Mineral exploration",
                "Geospatial analysis"
            ],
            [
                "Type of hyperspectral data",
                "Data acquisition date",
                "Spatial resolution",
                "Spectral resolution",
                "Hydrothermal alteration minerals",
                "Base metal minerals",
                "Data preprocessing method"
            ],
            [
                "Geological Context",
                "Data Collection Methods",
                "Mineralogical Composition",
                "Spectral Signatures",
                "Hydrothermal Processes",
                "Spatial Resolution",
                "Temporal Aspects"
            ],
            [
                "Type of hyperspectral data",
                "Study area",
                "Mineral identification methods",
                "Hydrothermal alteration indicators",
                "Base metal mineral detection techniques",
                "Geological context",
                "Data preprocessing and correction",
                "Spectral resolution and range"
            ],
            [
                "Data used",
                "Hydrothermal alteration",
                "Base metal minerals",
                "Analysis"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Study location",
            "Analysis",
            "Methodology",
            "Minerals studied",
            "Software used",
            "Research problem"
        ],
        "impr+120b": [
            "Data source",
            "Research problem",
            "Target minerals",
            "Geological process",
            "Data type"
        ],
        "impr+120b+abs": [
            "Study location",
            "Sensor type",
            "Data type",
            "Classification method",
            "Software used",
            "Evaluation approach",
            "Research objective"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140819",
        "research_problem": "Limestone mineral identification using Hyperion imagery",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Software', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Mineralogy', 'Limestone', 'Hyperion imagery', 'Identification']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Enhancement of limestone mineral identification using Hyperion imagery: a case study from Tirunelveli District, Tamil Nadu, South India",
        "abstract": "Hyperspectral remote sensing consolidates imaging and spectroscopy in a solitary system which frequently comprises big datasets and necessitates the novel processing methods. In the present study, Cheranmadevi Block of Tirunelveli District in Tamil Nadu is selected to extract the abundant limestone mineral. Hyperion is one of the freely available hyperspectral imagery containing 242 spectral bands with 10-nm intervals in the wavelength between 400 and 2500\u00a0nm. The main objectives of the present research work are to enhance the imagery visualization, end member extraction, and classification, and estimate the abundant limestone quantity by removing the striping error in Hyperion imagery. The scanning electron microscope with energy-dispersive X-ray spectroscopy analysis is performed to identify the chemical composition of limestone mineral. The spectral reflectance of limestone is characterized using analytical spectral devices like a field spectroradiometer. Limestone has deep absorption in the short-wave infrared region (1900\u20132500\u00a0nm) around 2320\u20132340\u00a0nm due to their calcite composition (CaCO3). The feature extraction in Hyperion data is performed using various preprocessing steps like bad bands removal, vertical strip removal, and radiance and reflectance creation. To improve the classification accuracy, vertical strip removal process is performed using a local destriping algorithm. The absolute reflectance is achieved by the atmospheric correction module using Fast Line-of-sight Atmospheric Analysis of Hypercubes. The acquired reflectance image spectra are compared with the spectral libraries of USGS, JPL, and field spectra. Destriping enhances qualities of Hyperion data interims of the spectral profile, radiance, reflectance, and data reduction methods. The present research work focused on the local destriping algorithm to increase the quality and quantity of limestone deposit extraction.\nAdam E, Mutanga O, Rugege D (2010) Multispectral and hyperspectral remote sensing for identification and mapping of wetland vegetation: a review. Wetl Ecol Manag 18(3):281\u2013296",
        "dimensions": [
            [
                "Remote sensing",
                "Hyperion imagery",
                "Limestone mineral",
                "Mineral identification",
                "Spectral analysis",
                "Geological mapping"
            ],
            [
                "mineral identification",
                "Hyperion imagery",
                "spectral bands",
                "spectral resolution",
                "spatial resolution",
                "image processing techniques",
                "machine learning algorithms",
                "geological features"
            ],
            [
                "Remote Sensing",
                "Mineral Identification",
                "Limestone Geology",
                "Hyperspectral Imaging",
                "Machine Learning and Data Analysis",
                "Environmental Monitoring",
                "Spectral Signature Analysis"
            ],
            [
                "Spectral bands",
                "Spatial resolution",
                "Geological context",
                "Image processing techniques",
                "Study area",
                "Spectral signature analysis",
                "Remote sensing methods",
                "Accuracy assessment"
            ],
            [
                "Mineral type",
                "Imaging technology",
                "Data source",
                "Analysis method"
            ]
        ],
        "impr+abs": [
            "Study location",
            "Data used",
            "Mineral identification method",
            "Analysis method",
            "Chemical composition analysis",
            "Data preprocessing",
            "Data enhancement",
            "Reference"
        ],
        "impr+120b": [
            "research problem",
            "data source",
            "imaging sensor",
            "target mineral",
            "methodology"
        ],
        "impr+120b+abs": [
            "Study location",
            "Data source",
            "Methodology",
            "Preprocessing technique",
            "Instrumentation",
            "Evaluation metric",
            "Research objective"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140823",
        "research_problem": "Mapping the spatial distribution of altered minerals in rocks and soils in the Gadag Schist Belt (GSB) using Hyperion data",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Spatial distribution', 'Altered minerals', 'Rocks', 'Soils', 'Gadag Schist Belt', 'Hyperion data']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Spatial distribution of altered minerals in the Gadag Schist Belt (GSB) of Karnataka, Southern India using hyperspectral remote sensing data",
        "abstract": "Spatial distribution of altered minerals in rocks and soils in the Gadag Schist Belt (GSB) is carried out using Hyperion data of March 2013. The entire spectral range is processed with emphasis on VNIR (0.4\u20131.0\u00a0\u03bcm) and SWIR regions (2.0\u20132.4\u00a0\u03bcm). Processing methodology includes Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes correction, minimum noise fraction transformation, spectral feature fitting (SFF) and spectral angle mapper (SAM) in conjunction with spectra collected, using an analytical spectral device spectroradiometer. A total of 155 bands were analysed to identify and map the major altered minerals by studying the absorption bands between the 0.4\u20131.0-\u03bcm and 2.0\u20132.3-\u03bcm wavelength regions. The most important and diagnostic spectral absorption features occur at 0.6\u20130.7\u00a0\u03bcm, 0.86 and at 0.9\u00a0\u03bcm in the VNIR region due to charge transfer of crystal field effect in the transition elements, whereas absorption near 2.1, 2.2, 2.25 and 2.33\u00a0\u03bcm in the SWIR region is related to the bending and stretching of the bonds in hydrous minerals (Al-OH, Fe-OH and Mg-OH), particularly in clay minerals. SAM and SFF techniques are implemented to identify the minerals present. A score of 0.33\u20131 was assigned for both SAM and SFF, where a value of 1 indicates the exact mineral type. However, endmember spectra were compared with United States Geological Survey and John Hopkins University spectral libraries for minerals and soils. Five minerals, i.e. kaolinite-5, kaolinite-2, muscovite, haematite, kaosmec and one soil, i.e. greyish brown loam have been identified. Greyish brown loam and kaosmec have been mapped as the major weathering/altered products present in soils and rocks of the GSB. This was followed by haematite and kaolinite. The SAM classifier was then applied on a Hyperion image to produce a mineral map. The dominant lithology of the area included greywacke, argillite and granite gneiss.",
        "dimensions": [
            [
                "Spatial distribution",
                "Altered minerals",
                "Rocks",
                "Soils",
                "Gadag Schist Belt (GSB)",
                "Hyperion data"
            ],
            [
                "Spatial distribution",
                "Altered minerals",
                "Rocks",
                "Soils",
                "Gadag Schist Belt (GSB)",
                "Hyperion data"
            ],
            [
                "Geographic Location",
                "Remote Sensing Data",
                "Geological Formation",
                "Mineralogy and Petrology",
                "Spatial Analysis",
                "Environmental Factors",
                "Geological Mapping"
            ],
            [
                "Geographic location",
                "Hyperion data",
                "Rock and soil types",
                "Mineral alteration types",
                "Spectral bands",
                "Data processing techniques",
                "Geological mapping",
                "Remote sensing techniques"
            ],
            [
                "Spatial scale",
                "Data source",
                "Study location",
                "Mineral type",
                "Analysis method"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Study location",
            "Methodology",
            "Mineral identification"
        ],
        "impr+120b": [
            "research problem",
            "study location",
            "data used",
            "sensor type",
            "methodology",
            "analysis type"
        ],
        "impr+120b+abs": [
            "Data used",
            "Study location",
            "Methodology",
            "Target minerals",
            "Classification method",
            "Evaluation metric",
            "Spectral libraries"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R140827",
        "research_problem": "Sub-pixel mineral investigation using Hyperion data in Rajasthan, India",
        "orkg_properties": "['Data used', 'research problem', 'Study Area', 'Software', 'Analysis', 'Processing', 'Field instrument', 'reference']",
        "nechakhin_result": "['Mineral type', 'Spectral reflectance', 'Hyperion data', 'Sub-pixel analysis', 'Rajasthan, India']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Sub-pixel mineral mapping using EO-1 Hyperion hyperspectral data",
        "abstract": ".This study describes the utility of Earth Observation (EO)-1 Hyperion data for sub-pixel mineral investigation using Mixture Tuned Target Constrained Interference Minimized Filter (MTTCIMF) algorithm in hostile mountainous terrain of Rajsamand district of Rajasthan, which hosts economic mineralization such as lead, zinc, and copper etc. The study encompasses pre-processing, data reduction, Pixel Purity Index (PPI) and endmember extraction from reflectance image of surface minerals such as illite, montmorillonite, phlogopite, dolomite and chlorite. These endmembers were then assessed with USGS mineral spectral library and lab spectra of rock samples collected from field for spectral inspection. Subsequently, MTTCIMF algorithm was implemented on processed image to obtain mineral distribution map of each detected mineral. A virtual verification method has been adopted to evaluate the classified image, which uses directly image information to evaluate the result and confirm the overall accuracy and kappa coefficient of 68 % and 0.6 respectively. The sub-pixel level mineral information with reasonable accuracy could be a valuable guide to geological and exploration community for expensive ground and/or lab experiments to discover economic deposits. Thus, the study demonstrates the feasibility of Hyperion data for sub-pixel mineral mapping using MTTCIMF algorithm with cost and time effective approach.\n",
        "dimensions": [
            [
                "Remote sensing",
                "Hyperion data",
                "Sub-pixel analysis",
                "Mineral investigation",
                "Rajasthan",
                "India"
            ],
            [
                "remote sensing",
                "Hyperion data",
                "mineral investigation",
                "sub-pixel analysis",
                "Rajasthan, India"
            ],
            [
                "Geographic Location",
                "Remote Sensing Data",
                "Mineralogy and Geology",
                "Spectral Analysis Techniques",
                "Environmental Conditions"
            ],
            [
                "Geographic location",
                "Hyperspectral sensor",
                "Mineral types",
                "Data processing techniques",
                "Spectral analysis methods",
                "Geological context",
                "Remote sensing applications",
                "Research methodology"
            ],
            [
                "Data source",
                "Study location",
                "Analysis method",
                "Mineral type",
                "Resolution level"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Study location",
            "Minerals investigated",
            "Methodology",
            "Evaluation metric"
        ],
        "impr+120b": [
            "research problem",
            "Data used",
            "Study location",
            "Analysis scale",
            "Methodology"
        ],
        "impr+120b+abs": [
            "Data source",
            "Study location",
            "Methodology",
            "Algorithm",
            "Evaluation metric",
            "Target minerals",
            "Spectral library"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143607",
        "research_problem": "Applications of Hyperspectral remote sensing for mineral exploartion",
        "orkg_properties": "['research problem', 'Preprocessing', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['Hyperspectral remote sensing techniques', 'Mineral exploration', 'Spectral signatures', 'Mineral mapping', 'Geological features', 'Spectral libraries', 'Image classification', 'Data preprocessing', 'Spectral unmixing', 'Feature extraction', 'Machine learning algorithms', 'Image fusion', 'Data fusion', 'Data interpretation', 'Accuracy assessment']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Multi- and hyperspectral geologic remote sensing: A review",
        "abstract": "Geologists have used remote sensing data since the advent of the technology for regional mapping, structural interpretation and to aid in prospecting for ores and hydrocarbons. This paper provides a review of multispectral and hyperspectral remote sensing data, products and applications in geology. During the early days of Landsat Multispectral scanner and Thematic Mapper, geologists developed band ratio techniques and selective principal component analysis to produce iron oxide and hydroxyl images that could be related to hydrothermal alteration. The advent of the Advanced Spaceborne Thermal Emission and Reflectance Radiometer (ASTER) with six channels in the shortwave infrared and five channels in the thermal region allowed to produce qualitative surface mineral maps of clay minerals (kaolinite, illite), sulfate minerals (alunite), carbonate minerals (calcite, dolomite), iron oxides (hematite, goethite), and silica (quartz) which allowed to map alteration facies (propylitic, argillic etc.). The step toward quantitative and validated (subpixel) surface mineralogic mapping was made with the advent of high spectral resolution hyperspectral remote sensing. This led to a wealth of techniques to match image pixel spectra to library and field spectra and to unravel mixed pixel spectra to pure endmember spectra to derive subpixel surface compositional information. These products have found their way to the mining industry and are to a lesser extent taken up by the oil and gas sector. The main threat for geologic remote sensing lies in the lack of (satellite) data continuity. There is however a unique opportunity to develop standardized protocols leading to validated and reproducible products from satellite remote sensing for the geology community. By focusing on geologic mapping products such as mineral and lithologic maps,geochemistry, P-T paths, fluid pathways etc. the geologic remote sensing community can bridge the gap with the geosciences community. Increasingly workflows should be multidisciplinary and remote sensing data should be integrated with field observations and subsurface geophysical data to monitor and understand geologic processes.Highlights\u25ba Multispectral RS allows geologists a qualitative assessment of surface composition. \u25ba Hyperspectralremote sensingallows mapping surfacemineralogy. \u25ba Data integration (hyperspectral,geophysics, geochemistry) reveals proxies to P-T trajectories and fluid pathways in alteration systems. \u25ba Standards and protocols are lacking in geologic remote sensing. \u25ba Data continuity is essential in long-term monitoring ofgeological processes.\nKnowledge of tree species distribution is important worldwide for sustainable forest management and resource evaluation. The accuracy and information content of species maps produced using remote sensing images vary with scale, sensor (optical, microwave, LiDAR), classification algorithm, verification design and natural conditions like tree age, forest structure and density. Imaging spectroscopy reduces the inaccuracies making use of the detailed spectral response. However, the scale effect still has a strong influence and cannot be neglected. This study aims to bridge the knowledge gap in understanding the scale effect in imaging spectroscopy when moving from 4 to 30\u00a0m pixel size for tree species mapping, keeping in mind that most current and future hyperspectral satellite based sensors work with spatial resolution around 30\u00a0m or more.Two airborne (HyMAP) and one s",
        "dimensions": [
            [
                "Remote sensing techniques",
                "Hyperspectral imaging",
                "Mineral exploration",
                "Geological mapping",
                "Spectral analysis",
                "Data processing",
                "Feature extraction",
                "Machine learning",
                "Geospatial analysis"
            ],
            [
                "remote sensing technology",
                "hyperspectral imaging",
                "mineral exploration",
                "spectral resolution",
                "spatial resolution",
                "mineral identification",
                "data processing techniques",
                "geological features",
                "spectral libraries"
            ],
            [
                "Remote Sensing Technology",
                "Mineral Exploration Techniques",
                "Geographic Location",
                "Spectral Data Analysis Methods",
                "Mineral Types",
                "Data Processing and Interpretation",
                "Geological Context",
                "Sensor Specifications"
            ],
            [
                "Spectral resolution",
                "Spatial resolution",
                "Mineral composition",
                "Data processing techniques",
                "Geological setting",
                "Sensor platform",
                "Case studies or study areas"
            ],
            [
                "Application",
                "Remote sensing technology",
                "Mineral exploration",
                "Data analysis",
                "Spectral bands",
                "Geological mapping"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Research problem",
            "Application",
            "Resolution methods",
            "Data integration",
            "Standards and protocols",
            "Data continuity",
            "Scale effect"
        ],
        "impr+120b": [
            "research problem",
            "technology",
            "application domain",
            "data type",
            "target industry",
            "methodology"
        ],
        "impr+120b+abs": [
            "Remote sensing data type",
            "Geological application",
            "Mapping products",
            "Methodology",
            "Challenges",
            "Opportunities",
            "Data integration approach"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143755",
        "research_problem": "Vegetation stress through Hyperspectral remote sensing",
        "orkg_properties": "['research problem', 'Study Area', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['Vegetation type', 'Vegetation growth stage', 'Hyperspectral sensor', 'Spectral bands', 'Spectral resolution', 'Feature extraction methods', 'Classification algorithms', 'Vegetation indices', 'Temporal resolution', 'Spatial resolution', 'Data preprocessing steps']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Hyper-spectral remote sensing to monitor vegetation stress",
        "abstract": "Background, aim, and scopeVegetation stress diagnoses based on plant sampling and physiochemical analysis using traditional methods are commonly time-consuming, destructive and expensive. The measurement of field spectral reflectance is one basis of airborne or spaceborne remote sensing monitoring.Materials and methodsIn this study, paddy plants were grown in the barrels evenly filled with 10.0\u00a0kg soil that was mixed respectively with 0, 2.5\u2009\u00d7\u2009207.2 and 5.0\u2009\u00d7\u2009207.2\u00a0mg Pb per 1,000\u00a0g soil. Rice canopy spectra were gathered by mobile hyper-spectral radiometer (ASD FieldSpec Pro FR, USA). Meanwhile, canopy leaves in the field-of-view (FOV) of spectroradiometer were collected and then prepared in the laboratory, (1) for chlorophyll measurement by Model 721 spectrophotometer, and (2) for Pb determination by atomic absorption spectrophotometer (SpectraAA-220FS).Results and discussionCanopy spectral reflectance in the region of visible-to-near-infrared light (VNIR) increased, because ascended Pb concentration caused the decrease of canopy chlorophyll content. In the agro-ecosystem, however, heavy metal contamination is presented typically as mixture and their interactions strongly affect actually occurring effects. Normalized spectral absorption depth (Dn), and shifting distance (DS) of red edge position (REPs) revealed the differences in Pb concentration for canopy leaves, especially at the early tillering stage. Due to insufficient biomass of rice plants, the 30th day was not reliable enough for the selection of crucial growth stages. Some special sensitive bands might be omitted at the same time because of limited sample sets.ConclusionsOur initial experiments are still too few in the amounts of both metals and plants neither to build accurate prediction models nor to discuss the transformation from ground to air/spaceborne remote sensing. However, we are pleased to communicate that ground remote sensing measurements would provide reliable information for the estimation of Pb concentration in rice plants at the early tillering stage when proper features (such as DS andDn) of reflectance spectra are applied.Recommendations and perspectivesHyper-spectral remote sensing is a potential and promising technology for monitoring environmental stresses on agricultural vegetation. Further ground remote sensing experiments are necessary to evaluate the possibility of hyper-spectral reflectance spectroscopy in monitoring different kinds of metals\u2019 stress on various plants.\nREPs and DSs towards longer or shorter wavelength are summarized in Table1. It shows that REPs of Pb2 paddy rice plants shifted towards shorter wavelength (blue light) by 22\u00a0nm on the 30th day, but DSs were equal to zero when paddy rice plants are on the 50th and 65th day with more and more exuberant vegetation, and then REPs of Pb2 shifted towards longer wavelength (red light) by 6\u00a0nm and 12\u00a0nm. In contrast, DSs of Pb1 were much less and not obvious.Table\u00a01 REPs and DS (nm) in 1st derivative spectra in all growth stagesFull size tableThe beginning and the terminal points were determined at 530 and 740\u00a0nm, respectively. Pb contamination also made the depth of absorption at 680\u00a0nm different between paddy rice plants of Pb0, Pb1 and Pb2. On the 50th day (Fig.1a),Dncurve of Pb0 enveloped those of Pb1 and Pb2 from 530 to 740\u00a0nm, while on the 65th day (Fig.1b) all threeDncurves went consistently, and then on the 80th day (Fig.1c). TheDncurve of Pb0 was enveloped by both of Pb1 and Pb2 fro",
        "dimensions": [
            [
                "Hyperspectral remote sensing",
                "Vegetation",
                "Stress detection",
                "Plant physiology",
                "Spectral reflectance",
                "Chlorophyll content",
                "Water stress",
                "Drought monitoring",
                "Feature extraction",
                "Data analysis"
            ],
            [
                "vegetation type",
                "stress indicators",
                "hyperspectral sensor",
                "remote sensing platform",
                "spatial resolution",
                "spectral resolution",
                "data preprocessing",
                "feature extraction method",
                "vegetation index",
                "time of acquisition"
            ],
            [
                "Hyperspectral Remote Sensing",
                "Vegetation Health",
                "Spectral Reflectance",
                "Environmental Stressors",
                "Machine Learning and Data Analysis",
                "Vegetation Types",
                "Spatial and Temporal Analysis"
            ],
            [
                "Vegetation type",
                "Hyperspectral remote sensing technique",
                "Stress indicators",
                "Geographic location",
                "Time period",
                "Data processing techniques",
                "Environmental conditions",
                "Spectral bands used"
            ],
            [
                "Vegetation type",
                "Remote sensing method",
                "Stress indicator",
                "Data collection method"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Methodology",
            "Data collection",
            "Analysis",
            "Recommendations",
            "Study location",
            "Technology used"
        ],
        "impr+120b": [
            "research problem",
            "Data source",
            "Sensing modality",
            "Analysis technique",
            "Target domain",
            "Outcome measure"
        ],
        "impr+120b+abs": [
            "Study organism",
            "Contaminant",
            "Measurement instrument",
            "Spectral analysis technique",
            "Experimental treatment levels",
            "Growth stage",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143763",
        "research_problem": "Spectral library for remote sensing of urban materials",
        "orkg_properties": "['research problem', 'Study Area', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['spectral resolution',\n 'spatial resolution',\n 'spectral range',\n 'material composition',\n 'urban environment',\n 'remote sensing techniques',\n 'data acquisition',\n 'data preprocessing',\n 'data interpretation',\n 'classification algorithms',\n 'data fusion',\n 'sample size',\n 'sensor characteristics',\n 'atmospheric correction']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Development and utilization of urban spectral library for remote sensing of urban environment",
        "abstract": null,
        "dimensions": [
            [
                "Remote sensing",
                "Spectral library",
                "Urban materials",
                "Spectroscopy",
                "Material identification",
                "Urban environment",
                "Hyperspectral imaging"
            ],
            [
                "spectral resolution",
                "urban materials",
                "remote sensing platform",
                "library size",
                "spectral range",
                "data format",
                "geographic location",
                "sensor type"
            ],
            [
                "Remote Sensing Techniques",
                "Urban Materials",
                "Spectral Resolution",
                "Data Collection Methods",
                "Data Analysis Techniques",
                "Geographic Location",
                "Application Areas"
            ],
            [
                "Spectral resolution",
                "Urban material types",
                "Sensor type",
                "Geographic location",
                "Spectral range",
                "Data collection method",
                "Spectral library size",
                "Open access availability"
            ],
            [
                "Spectral library",
                "Remote sensing application",
                "Urban materials",
                "Data collection method",
                "Spectral resolution",
                "Data analysis technique"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Spectral library development",
            "Remote sensing application",
            "Urban environment"
        ],
        "impr+120b": [
            "Research problem",
            "Data type",
            "Application domain",
            "Target objects",
            "Methodology",
            "Data source"
        ],
        "impr+120b+abs": [
            "Application domain",
            "Data type",
            "Methodology",
            "Target environment",
            "Research objective"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143778",
        "research_problem": "Effects of mining areas on vegetation through Hyperspectral remote sensing",
        "orkg_properties": "['research problem', 'Study Area', 'Preprocessing', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['mining areas',\n 'vegetation',\n 'effects',\n 'Hyperspectral remote sensing']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 3,
        "title": "Application of hyperspectral remote sensing for environment monitoring in mining areas",
        "abstract": "Environmental problems caused by extraction of minerals have long been a focus on environmental earth sciences. Vegetation growing conditions are an indirect indicator of the environmental problem in mining areas. A growing number of studies in recent years made substantial efforts to better utilize remote sensing for dynamic monitoring of vegetation growth conditions and the environment in mining areas. In this article, airborne and satellite hypersectral remote sensing data\u2014HyMap and Hyperion images are used in the Mount Lyell mining area in Australia and Dexing copper mining area in China, respectively. Based on the analyses of biogeochemical effect of dominant minerals, the vegetation spectrum and vegetation indices, two hyperspectral indices: vegetation inferiority index (VII) and water absorption disrelated index (WDI) are employed to monitor the environment in the mining area. Experimental results indicate that VII can effectively distinguish the stressed and unstressed vegetation growth situation in mining areas. The sensitivity of VII to the vegetation growth condition is shown to be superior to the traditional vegetation index\u2014NDVI. The other index, WDI, is capable of informing whether the target vegetation is affected by a certain mineral. It is an important index that can effectively distinguish the hematite areas that are covered with sparse vegetation. The successful applications of VII and WDI show that hyperspectral remote sensing provides a good method to effectively monitor and evaluate the vegetation and its ecological environment in mining areas.",
        "dimensions": [
            [
                "Mining areas",
                "Vegetation",
                "Hyperspectral remote sensing",
                "Environmental impact",
                "Ecosystem",
                "Land use",
                "Biodiversity",
                "Remote sensing techniques",
                "Spectral analysis",
                "Geospatial analysis"
            ],
            [
                "mining area",
                "vegetation",
                "hyperspectral remote sensing",
                "environmental impact",
                "vegetation health",
                "spectral signature",
                "land cover change",
                "mining activity intensity"
            ],
            [
                "Hyperspectral Remote Sensing",
                "Mining Activities",
                "Vegetation Analysis",
                "Environmental Impact",
                "Geographic Location",
                "Spectral Analysis",
                "Restoration and Rehabilitation"
            ],
            [
                "Type of mining",
                "Vegetation type",
                "Hyperspectral remote sensing parameters",
                "Environmental factors",
                "Vegetation health indices",
                "Geographic location",
                "Time period",
                "Mining activity intensity"
            ],
            [
                "Study location",
                "Vegetation type",
                "Remote sensing technology",
                "Environmental impact",
                "Data collection method"
            ]
        ],
        "impr+abs": [
            "Study location",
            "Data used",
            "Analysis",
            "Vegetation monitoring method",
            "Environmental impact",
            "Remote sensing technology",
            "Experimental results"
        ],
        "impr+120b": [
            "research problem",
            "Study area",
            "Data type",
            "Methodology",
            "Analysis technique",
            "Sensor modality"
        ],
        "impr+120b+abs": [
            "Study location",
            "Data used",
            "Methodology",
            "Developed indices",
            "Evaluation metric",
            "Comparison baseline"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143827",
        "research_problem": "Application of Hyperspectral remote sensing to Agriculture",
        "orkg_properties": "['research problem', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['Hyperspectral remote sensing technology', 'Agriculture', 'Crop monitoring', 'Plant health assessment', 'Yield prediction', 'Water stress detection', 'Disease detection', 'Crop classification', 'Crop mapping', 'Nutrient management', 'Precision agriculture', 'Data analysis techniques']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "A short survey of hyperspectral remote sensing applications in agriculture",
        "abstract": ":Hyperspectral sensors are devices that acquire images over hundreds of spectral bands, thereby enabling the extraction of spectral signatures for objects or materials observed. Hyperspectral remote sensing has been used over a wide range of applications, such as agriculture, forestry, geology, ecological monitoring and disaster monitoring. In this paper, the specific application of hyperspectral remote sensing to agriculture is examined. The technological development of agricultural methods is of critical importance as the world's population is anticipated to continuously rise much beyond the current number of 7 billion. One area upon which hyperspectral sensing can yield considerable impact is that of precision agriculture - the use of observations to optimize the use of resources and management of farming practices. For example, hyperspectral image processing is used in the monitoring of plant diseases, insect pests and invasive plant species; the estimation of crop yield; and the fine classification of crop distributions. This paper also presents a detailed overview of hyperspectral data processing techniques and suggestions for advancing the agricultural applications of hyperspectral technologies in Turkey.",
        "dimensions": [
            [
                "Hyperspectral remote sensing",
                "Agriculture",
                "Crop monitoring",
                "Vegetation health",
                "Spectral reflectance",
                "Precision agriculture",
                "Hyperspectral imaging",
                "Plant disease detection",
                "Soil analysis",
                "Yield prediction"
            ],
            [
                "spectral resolution",
                "spatial resolution",
                "hyperspectral sensor",
                "vegetation indices",
                "crop monitoring",
                "soil analysis",
                "water stress detection",
                "crop disease detection",
                "yield estimation"
            ],
            [
                "Remote Sensing Technology",
                "Agricultural Practices",
                "Crop Types",
                "Data Analysis Techniques",
                "Environmental Factors",
                "Geographical Location",
                "Yield Monitoring and Prediction"
            ],
            [
                "Spectral Resolution",
                "Spatial Resolution",
                "Crop Type",
                "Data Preprocessing Techniques",
                "Vegetation Indices",
                "Machine Learning Algorithms",
                "Environmental Factors",
                "Crop Health Monitoring"
            ],
            [
                "Application domain",
                "Remote sensing technology",
                "Agricultural focus",
                "Data analysis method"
            ]
        ],
        "impr+abs": [
            "Application domain",
            "Technology description",
            "Impact area",
            "Data processing techniques"
        ],
        "impr+120b": [
            "research problem",
            "technology used",
            "data modality",
            "application domain",
            "methodology"
        ],
        "impr+120b+abs": [
            "Application area",
            "Sensor technology",
            "Data processing methods",
            "Use cases",
            "Geographic focus",
            "Research motivation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143846",
        "research_problem": "Use of hyperspectral remote sensing technology to geological applications",
        "orkg_properties": "['research problem', 'Outcome', 'Sensors', 'Techniques/Analysis']",
        "nechakhin_result": "['Hyperspectral remote sensing technology', \n 'Geological applications']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "A review on the geological applications of hyperspectral remote sensing technology",
        "abstract": ":Based on the progress of hyperspectral data acquisition, information processing and geological requirements, the current status and trends of hyperspectral remote sensing...Show MoreMetadata",
        "dimensions": [
            [
                "Hyperspectral remote sensing technology",
                "Geological applications",
                "Spectral resolution",
                "Spatial resolution",
                "Data preprocessing techniques",
                "Feature extraction methods",
                "Classification algorithms",
                "Mineral identification",
                "Geological mapping",
                "Spectral libraries",
                "Field spectroscopy",
                "Data fusion techniques"
            ],
            [
                "remote sensing technology",
                "hyperspectral imaging",
                "geological applications",
                "spectral resolution",
                "spatial resolution",
                "data preprocessing",
                "feature extraction",
                "classification algorithms",
                "mineral identification",
                "geological mapping"
            ],
            [
                "Remote Sensing Technology",
                "Geological Applications",
                "Hyperspectral Imaging",
                "Geological Features",
                "Data Analysis and Processing",
                "Case Studies and Applications"
            ],
            [
                "Spectral Resolution",
                "Geological Application",
                "Sensor Platform",
                "Data Processing Techniques",
                "Case Studies",
                "Spectral Libraries",
                "Spatial Resolution",
                "Data Fusion"
            ],
            [
                "Technology used",
                "Application domain",
                "Data source",
                "Analysis method"
            ]
        ],
        "impr+abs": [
            "Data acquisition method",
            "Information processing technique",
            "Geological application",
            "Technology trend"
        ],
        "impr+120b": [
            "technology",
            "application domain",
            "research problem",
            "data type",
            "methodology"
        ],
        "impr+120b+abs": [
            "Research area",
            "Technology used",
            "Application domain",
            "Data acquisition",
            "Data processing",
            "Current status",
            "Future trends"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143971",
        "research_problem": "Polarized raman scattering for calcium alumino silicate glasses",
        "orkg_properties": "['Techniques', 'research problem', 'Minerals in consideration', 'Instruments', 'Raman laser used']",
        "nechakhin_result": "['Raman spectroscopy', 'Polarization', 'Calcium alumino silicate glasses']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Infrared and Raman spectra of calcium alumino\u2013silicate glasses",
        "abstract": "Calcium alumino\u2013silicate glasses were investigated by infrared reflectivity and polarized Raman scattering. Vibrational modes were assigned to different types of atomic motion in the glass network. The nature of the low frequency Raman peak or Boson Peak is discussed in terms of existing theories. Kramers\u2013Kronig analysis was performed on IR reflectivity data to extract longitudinal and transverse optic phonon frequencies.\nThe purpose of this review is to provide non-specialists with a basic understanding of the information micro-Raman Spectroscopy (\u03bcRS) may yield when this characterization tool is applied to nanomaterials, a generic term for describing nano-sized crystals and bulk homogeneous materials with a structural disorder at the nanoscale \u2013 typically nanoceramics, nanocomposites, glassy materials and relaxor ferroelectrics. The selected materials include advanced and ancient ceramics, semiconductors and polymers developed in the form of dots, wires, films, fibres or composites for applications in the energy, electronic and aeronautics\u2013aerospace industries. The text is divided into five sections:\u2022Section 1 is a general introduction.\u2022Section 2 outlines the principles of conventional \u03bcRS.\u2022Section 3 introduces the main effects for nanomaterials, with special emphasis on two models that connect Raman spectra features to \u201cgrain size\u201d, namely the Phonon Confinement Model (PCM) and the Elastic Sphere Model (ESM).\u2022Section 4 presents the experimental versatility of \u03bcRS applied to nanomaterials (phase identification, phase transition monitoring, grain size determination, defect concentration assessment, etc.).\u2022Section 5 deals with the micro-mechanical aspects of \u03bcRS (\u201cRaman extensometry\u201d). Special emphasis is placed on the relationship between the stress-related coefficientsS\u025b/\u03c3and the macroscopic response of the materials to the applied stress.\nCalcium alumino\u2013silicate glasses were investigated by infrared reflectivity and polarized Raman scattering. Vibrational modes were assigned to different types of atomic motion in the glass network. The nature of the low frequency Raman peak or Boson Peak is discussed in terms of existing theories. Kramers\u2013Kronig analysis was performed on IR reflectivity data to extract longitudinal and transverse optic phonon frequencies.\nThe structure and properties of glasses and melts in the CaO\u2013Al2O3\u2013SiO2(CAS) system play an important role in earth and material sciences. Aluminum has a crucial role in this ternary system, and its environment is still questioned. In this paper, we present new results using Raman spectroscopy and27Al Nuclear Magnetic Resonance on CAS glasses obtained by classic and rapid quenching methods. We propose an Al/Si tetrahedral distribution in the glass network in different Qnspecies. In this system, we show that Al and Si are mainly in Q4species along the joinR=\u00a0CaO/Al2O3=\u00a01, and in depolymerized Q2and Q3units at high CaO content for other joins (R=\u00a01.57 and 3). Five- ([5]Al) and six-fold ([6]Al) coordinated aluminum can be detected in the peraluminous glasses (R<\u00a01) in agreement with the deficit of charge compensators, Ca2+ions, near Al atoms. Unexpectedly, between 5% and 8% of[5]Al is also observed for percalcic glasses (R>\u00a01), except for glasses with low silica and high CaO content. The presence of[5]Al is related to viscous flow mechanisms while, in highly depolymerized glasses, the absence of[5]Al may indicate different mechanisms for melts to flow. This systematic study on the CAS",
        "dimensions": [
            [
                "Raman scattering",
                "Polarization",
                "Calcium alumino silicate glasses",
                "Glass composition",
                "Material characterization",
                "Spectroscopy",
                "Vibrational modes",
                "Crystal structure"
            ],
            [
                "Raman scattering technique",
                "Material composition",
                "Glass type",
                "Calcium content",
                "Alumina content",
                "Silicate content",
                "Polarization state",
                "Excitation wavelength",
                "Temperature"
            ],
            [
                "Chemical Composition",
                "Raman Spectroscopy Parameters",
                "Glass Structure and Properties",
                "Raman Spectra Analysis Techniques",
                "Applications and Implications",
                "Publication Date"
            ],
            [
                "Material composition",
                "Raman scattering technique",
                "Polarization of incident light",
                "Glass structure",
                "Spectral range",
                "Temperature",
                "Pressure",
                "Sample preparation"
            ],
            [
                "Material composition",
                "Experimental technique",
                "Analysis method",
                "Research area"
            ]
        ],
        "impr+abs": [
            "Analysis method",
            "Material studied",
            "Research purpose",
            "Experimental techniques"
        ],
        "impr+120b": [
            "research problem",
            "material studied",
            "analytical technique",
            "measurement mode",
            "spectral analysis",
            "sample composition"
        ],
        "impr+120b+abs": [
            "Spectroscopic techniques",
            "Material system",
            "Sample preparation method",
            "Structural analysis method",
            "Research focus",
            "Coordination environment characterization",
            "Field of study"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143983",
        "research_problem": "Advantage of integrated spectral analysis for Actinolite mineral",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['Spectral resolution', 'Spectral range', 'Spectral sensitivity', 'Spectral enhancement techniques', 'Data preprocessing', 'Data acquisition', 'Signal-to-noise ratio', 'Dimensionality reduction', 'Feature selection', 'Pattern recognition algorithms', 'Classification algorithms']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "EPR, optical, infrared and Raman spectral studies of Actinolite mineral",
        "abstract": "Electron paramagnetic resonance (EPR), optical, infrared and Raman spectral studies have been performed on a natural Actinolite mineral. The room temperature EPR spectrum reveals the presence of Mn2+and Fe3+ions giving rise to two resonance signals atg=2.0 and 4.3, respectively. The resonance signal atg=2.0 exhibits a six line hyperfine structure characteristic of Mn2+ions. EPR spectra have been studied at different temperatures from 123 to 433K. The number of spins (N) participating in the resonance atg=2.0 has been calculated at different temperatures. A linear relationship is observed between logNand 1/Tin accordance with Boltzmann law and the activation energy was calculated. The paramagnetic susceptibility (\u03c7) has been calculated at different temperatures and is found to be increasing with decreasing temperature as expected from Curie\u2019s law. From the graph of 1/\u03c7versusT, the Curie constant and Curie temperature have been evaluated. The optical absorption spectrum exhibits bands characteristic of Fe2+and Fe3+ions. The crystal field parameterDqand the Racah parametersBandChave been evaluated from the optical absorption spectrum. The infrared spectral studies reveal the formation of Fe3+\ue5f8OH complexes due to the presence of higher amount of iron in this mineral. The Raman spectrum exhibits bands characteristic of Si\ue5f8O\ue5f8Si stretching and Mg\ue5f8OH translation modes.\nThis paper presents a new aqueous precipitation method to prepare silicon-substituted hydroxyapatites Ca10(PO4)6\u2212y(SiO4)y(OH)2\u2212y(VOH)y(SiHAs) and details the characterization of powders with varying Si content up toy=\u00a01.25\u00a0mol\u00a0molSiHA\u22121. X-ray diffraction, transmission electron microscopy, solid-state nuclear magnetic resonance and Fourier transform infrared spectroscopy were used to accurately characterize samples calcined at 400\u00a0\u00b0C for 2\u00a0h and 1000\u00a0\u00b0C for 15\u00a0h. This method allows the synthesis of monophasic SiHAs with controlled stoichiometry. The theoretical maximum limit of incorporation of Si into the hexagonal apatitic structure isy<\u00a01.5. This limit depends on the OH content in the channel, which is a function of the Si content, temperature and atmosphere of calcination. These results, particularly those from infrared spectroscopy, raise serious reservations about the phase purity of previously prepared and biologically evaluated SiHA powders, pellets and scaffolds in the literature.",
        "dimensions": [
            [
                "Spectral analysis techniques",
                "Actinolite mineral properties",
                "Advantages of integrated spectral analysis",
                "Mineral identification methods",
                "Spectroscopy applications in mineralogy"
            ],
            [
                "spectral analysis method",
                "Actinolite mineral composition",
                "advantages",
                "spectral range",
                "data processing technique",
                "instrumentation"
            ],
            [
                "Mineralogy",
                "Spectral Analysis Techniques",
                "Data Integration",
                "Geological Context",
                "Applications of Spectral Analysis",
                "Instrumentation",
                "Data Processing and Interpretation"
            ],
            [
                "Mineral identification method",
                "Spectral range",
                "Data integration techniques",
                "Sample preparation",
                "Spectral resolution",
                "Environmental conditions"
            ],
            [
                "Advantages",
                "Spectral analysis technique",
                "Mineral identification",
                "Data interpretation"
            ]
        ],
        "impr+abs": [
            "Spectral analysis",
            "Element identification",
            "Temperature dependence",
            "Material synthesis"
        ],
        "impr+120b": [
            "Research problem",
            "Methodology",
            "Material studied",
            "Analysis technique",
            "Benefit type",
            "Application domain"
        ],
        "impr+120b+abs": [
            "Material studied",
            "Spectroscopic techniques",
            "Temperature range",
            "Calculated parameters",
            "Synthesis method",
            "Characterization techniques",
            "Research focus"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R143998",
        "research_problem": "Advantage of integrated spectral analysis for clinochlore mineral",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['spectral analysis technique',\n 'integrated approach',\n 'clinochlore mineral',\n 'advantages']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Optical absorption, EPR, infrared and Raman spectral studies of clinochlore mineral",
        "abstract": "Optical absorption, EPR, Infrared and Raman spectral studies have been carried out on natural clinochlore mineral. The optical absorption spectrum exhibits bands characteristic of Fe2+and Fe3+ions. A band observed in the NIR region is attributed to an intervalence charge transfer (Fe2+\u2013Fe3+) band. The room temperature EPR spectrum of single crystal of clinochlore mineral reveals the dominance of Fe3+ion exhibiting resonance signals atg=2.66; 3.68 and 4.31 besides one isotropic resonance signal atg=2.0. The EPR studies have been carried out for a polycrystalline sample in the temperature range from 103 to 443K and for a single crystal of clinochlore mineral in the temperature range 123\u2013297K. The number of spins (N) participating in resonance atg=4.3 signal of the single crystal of clinochlore mineral has been calculated at different temperatures. The paramagnetic susceptibility (\u03c7) is calculated from the EPR data at different temperatures for single crystal of clinochlore mineral. The Curie constant and Curie temperature values are evaluated from 1/\u03c7versusTgraph. The infrared spectral studies reveal the formation of Fe3+\u2013OH complexes due to the presence of higher amount of iron in this mineral. The Raman spectrum exhibits bands characteristic of Si\u2013O\u2013Si stretching and Si\u2013O bending modes.\nThe efficacy and the interface interactions of fluoride on laterite were investigated using batch methods; under various ionic strengths, pH, fluoride loading and diverse spectroscopy along with surface complexation modeling. The laterite used in this study was rich in iron (40%) and aluminum (30%). Proton binding sites were characterized by potentiometric titrations yielding pHZPCaround pH 8.72. Adsorption of fluoride on laterite is strongly pH dependent showing a maximum adsorption at pH <5, though not affected by the electrolyte concentration. Experimental data were quantified with a 2pKgeneralized diffused layer model considering two different surface binding sites for both protons and anions, using reaction stoichiometries. Surface complexation modeling showed that both Fe and Al sites of the laterite surface contributes to fluoride adsorption via inner-sphere complexation forming monodentate mononuclear interaction with laterite. Fluoride adsorption followed the Freundlich isotherm, indicating multi-site complexation on the laterite surface. FT-IR spectroscopic data provides an evidence for increased hydrogen bonding, indicated by the broadening of the OH stretch features around 3300\u00a0cm\u22121.\nLizardites in Yuanjiang laterite ore were characterized using X-ray diffraction (XRD), X-ray energy dispersive spectroscopy (EDS) and Fourier transform infrared spectroscopy (FTIR). Their leaching behaviour in sulphuric acid was investigated. XRD patterns show that there are two different lizardite polytypes in Yuanjiang laterite ore: lizardite-1T and lizardite-1M. The crystal order of the lizardites between soil and rock samples was different, which was demonstrated by XRD and FTIR spectrum analyses. The obvious change of the crystal cell volume was observed in the lizardite-1T of soil sample, with higher iron content, whereas the crystal cell varied little for the lizardite-1T of rock sample, with higher aluminum content. FTIR spectrum, EDS analyses and the leaching experiments show substitution of octahedral nickel, iron and aluminum, as well as tetrahedral aluminum. Iron occurred as ferrous and ferric iron in the lizardite-1T of rock sample, mostly as ferrous",
        "dimensions": [
            [
                "Mineralogy",
                "Spectral analysis",
                "Integrated spectral analysis",
                "Clinochlore",
                "Mineral identification",
                "Mineral characterization",
                "Spectroscopy",
                "Infrared spectroscopy",
                "Raman spectroscopy",
                "X-ray diffraction",
                "Mineral composition",
                "Mineral properties"
            ],
            [
                "spectral analysis method",
                "clinochlore mineral",
                "integrated analysis",
                "advantages",
                "mineral identification",
                "spectral resolution",
                "data interpretation",
                "sample preparation"
            ],
            [
                "Mineral Composition",
                "Spectral Analysis Techniques",
                "Advantages of Integrated Analysis",
                "Geological Context",
                "Applications",
                "Data Interpretation",
                "Comparative Studies"
            ],
            [
                "Type of spectral analysis technique",
                "Mineral composition",
                "Sample preparation method",
                "Data processing and analysis",
                "Environmental conditions"
            ],
            [
                "Advantages",
                "Spectral analysis method",
                "Mineral type",
                "Data integration",
                "Accuracy",
                "Application field"
            ]
        ],
        "impr+abs": [
            "Spectral analysis",
            "Mineral characterization",
            "Surface interaction",
            "Leaching behavior",
            "Crystallographic analysis",
            "Elemental analysis",
            "Adsorption behavior",
            "Substitution analysis"
        ],
        "impr+120b": [
            "research problem",
            "material studied",
            "analytical method",
            "integration approach",
            "evaluation metric",
            "application domain"
        ],
        "impr+120b+abs": [
            "Mineral studied",
            "Spectroscopic techniques",
            "Temperature range",
            "Surface complexation modeling",
            "Adsorption isotherm model",
            "Characterization methods",
            "Sample composition"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144015",
        "research_problem": "Raman spectral analysis for humite minerals",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['Raman spectroscopy', 'Spectral analysis', 'Humite minerals', 'Mineralogy', 'Chemical composition', 'Crystal structure', 'Raman spectrum', 'Spectral features', 'Peak positions', 'Peak intensities', 'Peak widths', 'Vibrational modes', 'Spectral database', 'Raman peaks classification', 'Peak assignments']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "A Raman spectroscopic study of humite minerals",
        "abstract": "Raman spectroscopy has been used to study the structure of the humite mineral group ((A2SiO4)n\u2013A(OH, F)2wherenrepresents the number of olivine and brucite layers in the structure and is 1, 2, 3 or 4 and A2+is Mg, Mn, Fe or some mix of these cations). The humite group of minerals forms a morphotropic series with the minerals olivine and brucite. The members of the humite group contain layers of the olivine structure that alternate with layers of the brucite-like sheets. The minerals are characterized by a complex set of bands in the 800\u20131000 cm\u22121region attributed to the stretching vibrations of the olivine (SiO4)4\u2212units. The number of bands in this region is influenced by the number of olivine layers. Characteristic bending modes of the (SiO4)4\u2212units are observed in the 500\u2013650 cm\u22121region. The brucite sheets are characterized by the OH stretching vibrations in the 3475\u20133625 cm\u22121wavenumber region. The position of the OH stretching vibrations is determined by the strength of the hydrogen bond formed between the brucite-like OH units and the olivine silica layer. The number of olivine sheets and not the chemical composition determines the strength of the hydrogen bonds. Copyright \u00a9 2006 John Wiley & Sons, Ltd.",
        "dimensions": [
            [
                "Raman spectroscopy",
                "Mineral analysis",
                "Humite minerals",
                "Spectral analysis",
                "Raman scattering",
                "Crystal structure",
                "Vibrational modes",
                "Mineral identification"
            ],
            [
                "Raman spectroscopy parameters",
                "Humite mineral composition",
                "Sample preparation method",
                "Spectral resolution",
                "Excitation wavelength",
                "Data processing method"
            ],
            [
                "Mineral Composition",
                "Raman Spectral Features",
                "Sample Preparation Methods",
                "Instrumentation and Parameters",
                "Geological Origin and Occurrence",
                "Data Analysis and Interpretation Methods",
                "Applications and Significance",
                "Publication Date and Journal"
            ],
            [
                "Mineral composition",
                "Raman spectroscopy parameters",
                "Sample preparation method",
                "Spectral analysis techniques",
                "Geological origin",
                "Instrumentation",
                "Environmental conditions"
            ],
            [
                "Spectral analysis",
                "Mineral type",
                "Raman spectroscopy",
                "Data collection method",
                "Analysis technique"
            ]
        ],
        "impr+abs": [
            "Mineral group",
            "Structure",
            "Raman bands",
            "Vibrational modes",
            "Chemical composition"
        ],
        "impr+120b": [
            "research problem",
            "analytical technique",
            "target material",
            "spectral data",
            "instrumentation"
        ],
        "impr+120b+abs": [
            "Analytical technique",
            "Sample type",
            "Structural focus",
            "Spectral region",
            "Vibrational mode",
            "Composition variable"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144024",
        "research_problem": "Raman spectral analysis for borosilicate mineral ferroaxinite",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['Raman spectroscopy', 'minerals', 'borosilicate minerals', 'ferroaxinite', 'spectral analysis']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Raman spectroscopy of the borosilicate mineral ferroaxinite",
        "abstract": "Raman spectroscopy, complemented by infrared spectroscopy, has been used to characterise the ferroaxinite minerals of the theoretical formula Ca2Fe2+Al2BSi4O15(OH), a ferrous aluminium borosilicate. The Raman spectra are complex but are subdivided into sections on the basis of the vibrating units. The Raman spectra are interpreted in terms of the addition of borate and silicate spectra. Three characteristic bands of ferroaxinite are observed at 1082, 1056 and 1025 cm\u22121and are attributed to BO4stretching vibrations. Bands at 1003, 991, 980 and 963 cm\u22121are assigned to SiO4stretching vibrations. Bands are found in these positions for each of the ferroaxinites studied. No Raman bands were found above 1100 cm\u22121showing that ferroaxinites contain only tetrahedral boron. The hydroxyl stretching region of ferroaxinites is characterised by a single Raman band between 3368 and 3376 cm\u22121, the position of which is sample-dependent. Bands for ferroaxinite at 678, 643, 618, 609, 588, 572, 546 cm\u22121may be attributed to the \u03bd4bending modes and the three bands at 484, 444 and 428 cm\u22121may be attributed to the \u03bd2bending modes of the (SiO4)2\u2212. Copyright \u00a9 2006 John Wiley & Sons, Ltd.",
        "dimensions": [
            [
                "Raman spectroscopy",
                "Borosilicate minerals",
                "Ferroaxinite",
                "Spectral analysis",
                "Mineralogy"
            ],
            [
                "Raman spectral analysis",
                "borosilicate mineral",
                "ferroaxinite",
                "spectral peaks",
                "peak intensities",
                "peak positions",
                "crystal structure",
                "chemical composition",
                "sample preparation"
            ],
            [
                "Mineral Composition",
                "Raman Spectral Features",
                "Analytical Techniques",
                "Geological Context",
                "Chemical Composition",
                "Crystal Structure",
                "Applications"
            ],
            [
                "Mineral composition",
                "Raman spectral analysis parameters",
                "Sample preparation method",
                "Spectral peaks and bands",
                "Instrumentation",
                "Environmental conditions",
                "Data analysis methods"
            ],
            [
                "Spectroscopy technique",
                "Mineral composition",
                "Analysis method",
                "Research problem"
            ]
        ],
        "impr+abs": [
            "Analysis method",
            "Mineral composition",
            "Spectroscopy technique",
            "Copyright information"
        ],
        "impr+120b": [
            "Research problem",
            "Sample material",
            "Analysis method",
            "Target mineral",
            "Data modality"
        ],
        "impr+120b+abs": [
            "Analytical technique",
            "Sample material",
            "Spectral range",
            "Characteristic Raman bands",
            "Vibrational mode assignment",
            "Research focus"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R144034",
        "research_problem": "Raman spectral analysis for joaquinite minerals",
        "orkg_properties": "['Techniques', 'research problem', 'Study Area', 'Minerals in consideration', 'Instruments', 'Raman laser used', 'Supplements']",
        "nechakhin_result": "['Mineral composition and characteristics', 'Raman spectral profile', 'Spectral peak positions and intensities', 'Crystal structure', 'Chemical formula', 'Sampling method', 'Spectrometer type and settings', 'Data pre-processing techniques', 'Spectral databases used for comparison', 'Analytical techniques and methodologies used', 'Similarities in spectral features', 'Geological or mineralogical context']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Raman spectroscopy of the joaquinite minerals",
        "abstract": "Selected joaquinite minerals have been studied by Raman spectroscopy. The minerals are categorised into two groups depending upon whether bands occur in the 3250 to 3450 cm\u22121region and in the 3450 to 3600 cm\u22121region, or in the latter region only. The first set of bands is attributed to water stretching vibrations and the second set to OH stretching bands. In the literature, X-ray diffraction could not identify the presence of OH units in the structure of joaquinite. Raman spectroscopy proves that the joaquinite mineral group contains OH units in their structure, and in some cases both water and OH units. A series of bands at 1123, 1062, 1031, 971, 912 and 892 cm\u22121are assigned to SiO stretching vibrations. Bands above 1000 cm\u22121are attributable to the \u03bdasmodes of the (SiO4)4\u2212and (Si2O7)6\u2212units. Bands that are observed at 738, around 700, 682 and around 668, 621 and 602 cm\u22121are attributed toO\uf8ffSi\uf8ffO bending modes. The patterns do not appear to match the published infrared spectral patterns of either (SiO4)4\u2212or (Si2O7)6\u2212units. The reason is attributed to the actual formulation of the joaquinite mineral, in which significant amounts of Ti or Nb and Fe are found. Copyright \u00a9 2007 John Wiley & Sons, Ltd.",
        "dimensions": [
            [
                "Raman spectroscopy",
                "Mineral analysis",
                "Joaquinite minerals",
                "Spectral analysis",
                "Raman scattering",
                "Crystallography"
            ],
            [
                "Raman spectroscopy",
                "mineral analysis",
                "Joaquinite minerals",
                "spectral peaks",
                "sample preparation",
                "spectral resolution",
                "spectral interpretation"
            ],
            [
                "Mineralogy",
                "Raman Spectroscopy Techniques",
                "Geological Context",
                "Spectral Analysis",
                "Chemical Composition",
                "Crystal Structure",
                "Spectral Database"
            ],
            [
                "Mineral type",
                "Raman spectroscopy parameters",
                "Sample preparation method",
                "Spectral interpretation",
                "Geological origin",
                "Instrumentation",
                "Environmental conditions",
                "Chemical composition"
            ],
            [
                "Spectroscopy technique",
                "Mineral type",
                "Analysis method",
                "Data interpretation"
            ]
        ],
        "impr+abs": [
            "Mineral composition",
            "Spectroscopy technique",
            "Chemical bonds",
            "Element presence"
        ],
        "impr+120b": [
            "Research problem",
            "Analytical technique",
            "Material studied",
            "Data type",
            "Instrumentation"
        ],
        "impr+120b+abs": [
            "Sample type",
            "Analytical technique",
            "Spectral range",
            "Classification criteria",
            "Key findings",
            "Chemical composition",
            "Research objective"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R139324",
        "research_problem": "Chemical sensors",
        "orkg_properties": "['Analyte', 'research problem', 'Sensing material', 'Sensing environment', 'Architecture']",
        "nechakhin_result": "['Chemical properties', 'Detection method', 'Sensing material', 'Analytes', 'Sensitivity', 'Selectivity', 'Response time', 'Stability', 'Cost', 'Fabrication method']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Graphene Nanomesh As Highly Sensitive Chemiresistor Gas Sensor",
        "abstract": "Click to copy section linkSection link copied!High Resolution ImageDownload MS PowerPoint SlideGraphene is a one atom thick carbon allotrope with all surface atoms that has attracted significant attention as a promising material as the conduction channel of a field-effect transistor and chemical field-effect transistor sensors. However, the zero bandgap of semimetal graphene still limits its application for these devices. In this work, ethanol-chemical vapor deposition (CVD) of a grown p-type semiconducting large-area monolayer graphene film was patterned into a nanomesh by the combination of nanosphere lithography and reactive ion etching and evaluated as a field-effect transistor and chemiresistor gas sensors. The resulting neck-width of the synthesized nanomesh was about \u223c20 nm and was comprised of the gap between polystyrene (PS) spheres that was formed during the reactive ion etching (RIE) process. The neck-width and the periodicities of the graphene nanomesh (GNM) could be easily controlled depending on the duration/power of the RIE and the size of the PS nanospheres. The fabricated GNM transistor device exhibited promising electronic properties featuring a high drive current and anION/IOFFratio of about 6, significantly higher than its film counterpart. Similarly, when applied as a chemiresistor gas sensor at room temperature, the graphene nanomesh sensor showed excellent sensitivity toward NO2and NH3, significantly higher than their film counterparts. The ethanol-based graphene nanomesh sensors exhibited sensitivities of about 4.32%/ppm in NO2and 0.71%/ppm in NH3with limits of detection of 15 and 160 ppb, respectively. Our demonstrated studies on controlling the neck width of the nanomesh would lead to further improvement of graphene-based transistors and sensors.This publication is licensed under the terms of your\n                            institutional subscription.Request reuse permissions.",
        "dimensions": [
            [
                "Chemical composition",
                "Sensing mechanism",
                "Detection limit",
                "Response time",
                "Selectivity",
                "Sensitivity",
                "Operating conditions",
                "Material type",
                "Sensor design",
                "Application area"
            ],
            [
                "type of sensor",
                "sensing mechanism",
                "target analyte",
                "detection limit",
                "response time",
                "selectivity",
                "stability",
                "operating conditions"
            ],
            [
                "Chemical Composition",
                "Sensing Mechanism",
                "Application Area",
                "Sensitivity and Selectivity",
                "Material and Fabrication Techniques",
                "Integration and Miniaturization",
                "Environmental Conditions",
                "Data Analysis and Interpretation"
            ],
            [
                "Type of sensor",
                "Sensing mechanism",
                "Target analytes",
                "Substrate/material",
                "Application area",
                "Sensitivity and selectivity",
                "Fabrication method",
                "Signal transduction"
            ],
            [
                "Sensing technology",
                "Target analyte",
                "Detection method",
                "Application field"
            ]
        ],
        "impr+abs": [
            "Material composition",
            "Fabrication method",
            "Device performance",
            "Gas sensitivity"
        ],
        "impr+120b": [
            "research problem",
            "sensor type",
            "target analyte",
            "detection principle",
            "application domain",
            "performance metric"
        ],
        "impr+120b+abs": [
            "Material",
            "Fabrication technique",
            "Device architecture",
            "Target analytes",
            "Performance metric",
            "Operating condition",
            "Sensor type"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R141661",
        "research_problem": "Nanothermometer",
        "orkg_properties": "['precursors', 'research problem', 'Method of nanomaterial synthesis', 'Nanomaterial', 'Has a unit']",
        "nechakhin_result": "['Nanotechnology', 'Temperature measurement', 'Thermometers', 'Nanostructures', 'Materials science', 'Nanoparticles']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "Fluorescent N-Doped Carbon Dots as in Vitro and in Vivo Nanothermometer",
        "abstract": "Click to copy section linkSection link copied!High Resolution ImageDownload MS PowerPoint SlideThe fluorescent N-doped carbon dots (N-CDs) obtained from C3N4emit strong blue fluorescence, which is stable with different ionic strengths and time. The fluorescence intensity of N-CDs decreases with the temperature increasing, while it can recover to the initial one with the temperature decreasing. It is an accurate linear response of fluorescence intensity to temperature, which may be attributed to the synergistic effect of abundant oxygen-containing functional groups and hydrogen bonds. Further experiments also demonstrate that N-CDs can serve as effectivein vitroandin vivofluorescence-based nanothermometer.This publication is licensed under the terms of your\n                            institutional subscription.Request reuse permissions.",
        "dimensions": [
            [
                "Nanotechnology",
                "Temperature measurement",
                "Nanoparticles",
                "Thermometry",
                "Biomedical applications",
                "Sensing technology"
            ],
            [
                "Nanoparticle type",
                "Temperature sensitivity",
                "Detection method",
                "Biocompatibility",
                "Application field"
            ],
            [
                "Nanotechnology",
                "Thermometry",
                "Biomedical Applications",
                "Material Science",
                "Nanoparticles",
                "Optical Properties",
                "Chemical Sensors",
                "Nanofabrication Techniques"
            ],
            [
                "Type of nanothermometer",
                "Nanoparticle composition",
                "Temperature sensing mechanism",
                "Calibration method",
                "Biomedical applications",
                "Environmental applications",
                "Synthesis method",
                "Size and shape of nanoparticles"
            ],
            [
                "Nanoparticle type",
                "Measurement method",
                "Temperature range",
                "Application field"
            ]
        ],
        "impr+abs": [
            "Material composition",
            "Fluorescence properties",
            "Application",
            "Stability"
        ],
        "impr+120b": [
            "research problem",
            "device type",
            "measurement principle",
            "target application",
            "material used",
            "performance metric"
        ],
        "impr+120b+abs": [
            "Nanomaterial type",
            "Synthesis precursor",
            "Optical property",
            "Temperature sensing mechanism",
            "Stability condition",
            "Application"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R195342",
        "research_problem": "Ontology integration",
        "orkg_properties": "['Implemented technologies', 'research problem']",
        "nechakhin_result": "['domain',\n 'ontology',\n 'integration approach',\n 'integration method',\n 'semantic web',\n 'knowledge representation',\n 'mapping techniques',\n 'ontology mapping',\n 'ontology alignment',\n 'ontology merging',\n 'ontology mediation',\n 'ontology mediation',\n 'ontology interoperability',\n 'ontology versioning',\n 'ontology evolution',\n 'ontology similarity',\n 'ontology matching',\n 'ontology heterogeneity']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Constructing Cooking Ontology for Live Streams",
        "abstract": "Constructing Cooking Ontology for Live StreamsShanlin ChangSan-Yih HwangYu-Chen YangAffiliation:National Sun Yat-sen University, TWCloseAffiliation:National Sun Yat-sen University, TWCloseAffiliation:National Sun Yat-sen University, TWCloseChapter from the book: Australasian Conference on Information Systems,  . 2018.Australasian Conference on Information Systems 2018.Return to BookDownloadPDFWe build a cooking domain knowledge by using an ontology schema that reflects natural language processing and enhances ontology instances with semantic query. Our research helps audiences to better understand live streaming, especially when they just switch to a show. The practical contribution of our research is to use cooking ontology, so we may map clips of cooking live stream video and instructions of recipes. The architecture of our study presents three sections: ontology construction, ontology enhancement, and mapping cooking video to cooking ontology. Also, our preliminary evaluations consist of three hierarchies\u2014nodes, ordered-pairs, and 3-tuples\u2014that we use to referee (1) ontology enhancement performance for our first experiment evaluation and (2) the accuracy ratio of mapping between video clips and cooking ontology for our second experiment evaluation. Our results indicate that ontology enhancement is effective and heightens accuracy ratios on matching pairs with cooking ontology and video clips.Disciplines:ComputingInformation technology",
        "dimensions": [
            [
                "Ontology",
                "Integration",
                "Semantic",
                "Data",
                "Knowledge",
                "Mapping",
                "Alignment",
                "Interoperability",
                "Semantic Web",
                "Information Retrieval"
            ],
            [
                "ontology type",
                "integration method",
                "ontology format",
                "domain",
                "mapping approach",
                "semantic similarity measure",
                "ontology alignment",
                "knowledge representation",
                "ontology evaluation"
            ],
            [
                "Domain",
                "Ontology Types",
                "Integration Techniques",
                "Evaluation Metrics",
                "Challenges and Solutions",
                "Application Scenarios",
                "Semantic Web Standards"
            ],
            [
                "Ontology type",
                "Integration approach",
                "Integration tools",
                "Integration challenges",
                "Integration evaluation",
                "Application domain",
                "Integration standards"
            ],
            [
                "Integration method",
                "Data sources",
                "Ontology type",
                "Mapping approach",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Ontology construction",
            "Ontology enhancement",
            "Mapping methodology",
            "Evaluation metrics"
        ],
        "impr+120b": [
            "research problem",
            "integration approach",
            "target ontology language",
            "source ontologies",
            "evaluation metric",
            "tooling"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Data source",
            "Evaluation metric",
            "Application domain",
            "Ontology type",
            "Experiment evaluation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R195387",
        "research_problem": "Inspection of interrelationship between foods and biomarkers",
        "orkg_properties": "['description', 'research problem', 'Implemented technologies', 'type of software']",
        "nechakhin_result": "['Foods',\n 'Biomarkers',\n 'Interrelationship',\n 'Nutrition',\n 'Diet',\n 'Metabolism',\n 'Health',\n 'Biochemical markers',\n 'Food analysis',\n 'Dietary patterns']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "FOBI: an ontology to represent food intake data and associate it with metabolomic data",
        "abstract": "Nutrition research can be conducted by using two complementary approaches: (i) traditional self-reporting methods or (ii) via metabolomics techniques to analyze food intake biomarkers in biofluids. However, the complexity and heterogeneity of these two very different types of data often hinder their analysis and integration. To manage this challenge, we have developed a novel ontology that describes food and their associated metabolite entities in a hierarchical way. This ontology uses a formal naming system, category definitions, properties and relations between both types of data. The ontology presented is called FOBI (Food-Biomarker Ontology) and it is composed of two interconnected sub-ontologies. One is a \u2019Food Ontology\u2019 consisting of raw foods and \u2018multi-component foods\u2019 while the second is a \u2018Biomarker Ontology\u2019 containing food intake biomarkers classified by their chemical classes. These two sub-ontologies are conceptually independent but interconnected by different properties. This allows data and information regarding foods and food biomarkers to be visualized in a bidirectional way, going from metabolomics to nutritional data or vice versa. Potential applications of this ontology include the annotation of foods and biomarkers using a well-defined and consistent nomenclature, the standardized reporting of metabolomics workflows (e.g. metabolite identification, experimental design) or the application of different enrichment analysis approaches to analyze nutrimetabolomic data.Availability: FOBI is freely available in both OWL (Web Ontology Language) and OBO (Open Biomedical Ontologies) formats at the project\u2019s Github repository (https://github.com/pcastellanoescuder/FoodBiomarkerOntology) and FOBI visualization tool is available inhttps://polcastellano.shinyapps.io/FOBI_Visualization_Tool/.",
        "dimensions": [
            [
                "Foods",
                "Biomarkers",
                "Nutrition",
                "Diet",
                "Health",
                "Metabolism",
                "Biochemical markers",
                "Nutritional assessment",
                "Disease risk",
                "Nutritional epidemiology"
            ],
            [
                "type of food",
                "biomarker type",
                "study design",
                "sample size",
                "statistical analysis method",
                "time of biomarker measurement",
                "nutrient content of food",
                "biomarker measurement technique"
            ],
            [
                "Food Types",
                "Biomarkers",
                "Study Design",
                "Population",
                "Nutrient Composition",
                "Time Frame",
                "Outcome Measures"
            ],
            [
                "Food types",
                "Biomarkers",
                "Nutrient composition",
                "Dietary patterns",
                "Meal timing",
                "Cooking methods",
                "Population demographics",
                "Intervention studies"
            ],
            [
                "Food types",
                "Biomarkers",
                "Correlation analysis",
                "Study population",
                "Data collection method"
            ]
        ],
        "impr+abs": [
            "Ontology name",
            "Ontology components",
            "Data integration",
            "Availability",
            "Application",
            "Data format"
        ],
        "impr+120b": [
            "Research problem",
            "Study domain",
            "Data type",
            "Variables of interest",
            "Analysis method",
            "Outcome metric"
        ],
        "impr+120b+abs": [
            "Ontology name",
            "Integrated data types",
            "Ontology structure",
            "Potential applications",
            "Resource availability",
            "Implementation format"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R195718",
        "research_problem": "Food recommendation",
        "orkg_properties": "['description', 'research problem', 'Implemented technologies', 'type of software']",
        "nechakhin_result": "['Cuisine', 'Ingredients', 'Dietary restrictions', 'Flavors', 'Nutritional value', 'Cooking methods', 'Meal type', 'Food preferences']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "NAct: The Nutrition &amp; Activity Ontology for Healthy Living",
        "abstract": "This paper presents the NAct (Nutrition & Activity) Ontology, designed to drive personalised nutritional and physical activity recommendations and effectively support healthy living, through a reasoning-based AI decision support system. NAct coalesces nutritional, medical, behavioural and lifestyle indicators with potential dietary and physical activity directives. The paper presents the first version of the ontology, including its co-design and engineering methodology, along with usage examples in supporting healthy nutritional and physical activity choices. Lastly, the plan for future improvements and extensions is discussed.",
        "dimensions": [
            [
                "Cuisine type",
                "Dietary restrictions",
                "Flavor profile",
                "Ingredient availability",
                "Nutritional content",
                "Cooking method",
                "Cultural preferences",
                "Allergies",
                "Budget",
                "Health goals"
            ],
            [
                "type of cuisine",
                "dietary restrictions",
                "meal type",
                "ingredient preferences",
                "cooking time",
                "nutritional requirements",
                "cultural preferences"
            ],
            [
                "Cuisine",
                "Dietary Restrictions",
                "Nutritional Content",
                "User Preferences",
                "Cultural Context",
                "Meal Type",
                "Cooking Skill Level"
            ],
            [
                "Type of cuisine",
                "Dietary restrictions",
                "Meal type",
                "Ingredients",
                "Cultural preferences",
                "Nutritional preferences",
                "Location",
                "Cooking time",
                "Allergies"
            ],
            [
                "User preferences",
                "Dietary restrictions",
                "Cuisine type",
                "Nutritional requirements",
                "Allergies",
                "Meal time",
                "Health goals"
            ]
        ],
        "impr+abs": [
            "Ontology name",
            "Purpose",
            "Components",
            "Methodology",
            "Usage examples",
            "Future improvements"
        ],
        "impr+120b": [
            "research problem",
            "application domain",
            "methodology",
            "data source",
            "evaluation metric",
            "user group"
        ],
        "impr+120b+abs": [
            "research problem",
            "artifact type",
            "methodology",
            "application domain",
            "decision support approach",
            "future work"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R198240",
        "research_problem": "Monitoring people behavior",
        "orkg_properties": "['description', 'research problem', 'name', 'purpose']",
        "nechakhin_result": "['Surveillance techniques',\n 'Data collection methods',\n 'Analysis techniques',\n 'Ethical considerations',\n 'Privacy concerns',\n 'Context of behavior (e.g., online, physical)',\n 'Demographic factors',\n 'Temporal factors',\n 'Spatial factors',\n 'Cultural factors',\n 'Psychological factors',\n 'Societal factors']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "HeLiS: An Ontology for Supporting Healthy Lifestyles",
        "abstract": "The use of knowledge resources in the digital health domain is a trending activity significantly grown in the last decade. In this paper, we presentHeLiS: an ontology aiming to provide in tandem a representation of both the food and physical activity domains and the definition of concepts enabling the monitoring of users\u2019 actions and of their unhealthy behaviors. We describe the construction process, the plan for its maintenance, and how this ontology has been used into a real-world system with a focus on \u201cKey to Health\u201d: a project for promoting healthy lifestyles on workplaces.\nTheHeLiSontology is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0Footnote14and it can be downloaded from thePerKAppproject website at the link reported at the beginning of the paper. The rational behind the CC BY-NC-SA 4.0 is that the Trentino Healthcare Department, that funds the project in which theHeLiSontology has been developed, was not in favor of releasing this ontology for business purposes. Hence, they force the adoption of this type of license for releasing the ontology. TheHeLiSontology can be downloaded in two different modalities: (i) the conceptual model only, where the user can download a light version of the ontology that does not contain any individual, or (ii) the full package, where the ontology is populated with all the individuals we have already modeled. TheHeLiSontology is constantly updated due to the project activities using the ontology as core component.The ontology is available also as web service. Detailed instructions are provided on the ontology website. Briefly, the service exposes a set ofinformativemethods enabling the access to a JSON representation of the individuals included into the ontology.The reusability aspect of theHeLiSontology can be seen from three perspectives. First, theHeLiSontology contains structured supervised knowledge about the food and physical activity domains. Provided information has a high granularity and the consistency of the modeled data have been validated by domain experts. Hence, the ontology represents a valuable artifact for the digital health domain. Second, the ontology model represents a relevant resource of medical knowledge. In particular, the ontology contains a set of rules, modeling good practices, related to what a person should eat and which physical activities a person should do for maintaining a good health, and the relationships between food\u2019s allergies or intolerances and specific food categories. This kind of knowledge is presented, for the first time, in a structured way and it can be reused in several third-party applications for different purposes. Third, theHeLiSontology enables the construction of health-based (but not limited to this domain) applications exploiting the whole content of the ontology as well as the sole conceptual model.\nThe topic area of theHeLiSontology is timely for researchers and developers working on digital health applications (including mobile) who are now exploring the use of Semantic Web technology in the form of knowledge graphs and rules to analyze the nutrition intake over time, activities, and their association with health risks and symptoms related to chronic diseases.TheHeLiSontology can be integrated within applications going beyond the mere access to the resource for informative purposes. Indeed, theMonitoringbranch of the ontology can be populated to properly respond to the needs of the solutions integrating the",
        "dimensions": [
            [
                "Surveillance",
                "Behavioral analysis",
                "Data collection",
                "Privacy concerns",
                "Ethical considerations",
                "Technology",
                "Social implications",
                "Legal regulations"
            ],
            [
                "type of behavior",
                "location of monitoring",
                "method of monitoring",
                "duration of monitoring",
                "ethical considerations",
                "privacy concerns",
                "data collection techniques",
                "analysis tools"
            ],
            [
                "Research Problem",
                "Methodology",
                "Types of Behavior",
                "Technology",
                "Ethical and Legal Considerations",
                "Application Domains",
                "Impact and Outcomes"
            ],
            [
                "Type of Monitoring",
                "Technology Used",
                "Ethical Considerations",
                "Application Domain",
                "Data Analysis Techniques",
                "Impact of Monitoring",
                "Legal Framework",
                "Cultural Differences"
            ],
            [
                "Behavioral data",
                "Data collection method",
                "Privacy considerations",
                "Ethical implications"
            ]
        ],
        "impr+abs": [
            "Ontology name",
            "License type",
            "Data access method",
            "Reusability aspect",
            "Domain focus",
            "Maintenance plan",
            "Integration capability"
        ],
        "impr+120b": [
            "research problem",
            "target population",
            "data source",
            "methodology",
            "evaluation metric",
            "ethical considerations"
        ],
        "impr+120b+abs": [
            "Domain area",
            "License type",
            "Distribution modality",
            "Access interface",
            "Maintenance strategy",
            "Use case scenario"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R201361",
        "research_problem": "Constraint Question Answering over Food Knowledge Graph",
        "orkg_properties": "['method', 'description', 'research problem', 'Domain', 'question answering type of system', 'Question Answering category', 'name']",
        "nechakhin_result": "['Food domain', 'Knowledge graph', 'Constraint', 'Question answering']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph",
        "abstract": "Food recommendation has become an important means to help guide users to adopt healthy dietary habits. Previous works on food recommendation either i) fail to consider users' explicit requirements, ii) ignore crucial health factors (e.g., allergies and nutrition needs), or iii) do not utilize the rich food knowledge for recommending healthy recipes. To address these limitations, we propose a novel problem formulation for food recommendation, modeling this task as constrained question answering over a large-scale food knowledge base/graph (KBQA). Besides the requirements from the user query, personalized requirements from the user's dietary preferences and health guidelines are handled in a unified way as additional constraints to the QA system. To validate this idea, we create a QA style dataset for personalized food recommendation based on a large-scale food knowledge graph and health guidelines. Furthermore, we propose a KBQA-based personalized food recommendation framework which is equipped with novel techniques for handling negations and numerical comparisons in the queries. Experimental results on the benchmark show that our approach significantly outperforms non-personalized counterparts (average 59.7% absolute improvement across various evaluation metrics), and is able to recommend more relevant and healthier recipes.",
        "dimensions": [
            [
                "Natural Language Processing",
                "Knowledge Graph",
                "Question Answering",
                "Food",
                "Constraint",
                "Semantic Web",
                "Graph-based Learning",
                "Information Retrieval"
            ],
            [
                "question type",
                "knowledge graph structure",
                "constraint representation",
                "semantic parsing",
                "natural language processing",
                "food domain",
                "query formulation",
                "answer generation"
            ],
            [
                "Domain",
                "Natural Language Processing (NLP)",
                "Constraint-based Systems",
                "Food and Nutrition",
                "Graph Databases and Querying",
                "Semantic Web",
                "Machine Learning",
                "Evaluation Metrics"
            ],
            [
                "Task",
                "Knowledge Graph",
                "Food Domain",
                "Question Answering",
                "Natural Language Processing",
                "Constraint Handling",
                "Evaluation Metrics",
                "Dataset"
            ],
            [
                "Research problem",
                "Knowledge graph type",
                "Constraint type",
                "Question type",
                "Answering method"
            ]
        ],
        "impr+abs": [
            "Food recommendation",
            "User requirements",
            "Health factors",
            "Knowledge base",
            "Personalized recommendation",
            "Experimental results",
            "Evaluation metrics"
        ],
        "impr+120b": [
            "research problem",
            "domain",
            "knowledge graph type",
            "task type",
            "methodology",
            "application area"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Dataset",
            "Evaluation metric",
            "Knowledge source",
            "Personalization approach"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R217480",
        "research_problem": "Question Answering ",
        "orkg_properties": "['method', 'description', 'data source', 'research problem', 'Implemented technologies', 'Domain', 'Question Answering category']",
        "nechakhin_result": "['Natural language processing', 'Information retrieval', 'Text comprehension', 'Machine learning', 'Relevance ranking']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Food safety Knowledge Graph and Question Answering System",
        "abstract": "The issue of food safety in recent years has always been the focus of public opinion. Every time there are unqualified foods, it will cause widespread panic and rumor spread, which has a great impact on social stability. Therefore, this paper crawled the data of unqualified foods officially released in recent years from the network, and designed the extraction algorithm of food general entities, food domain entities and relationships between entities for these data. The extracted entity pairs were stored in the gStore database. In order to solve the problem of association of knowledge in knowledge graph, this paper also designed the food safety ontology which organized the concepts, classifications and relationships about food production and food inspection. Finally, this paper also built an intelligent question answering system by means of gStore's http service to help person grasp the unqualified food information through natural language.",
        "dimensions": [
            [
                "Natural Language Processing",
                "Information Retrieval",
                "Machine Learning",
                "Text Mining",
                "Semantic Search",
                "Knowledge Graphs",
                "Deep Learning",
                "Information Extraction",
                "Question Generation",
                "Text Summarization"
            ],
            [
                "question type",
                "answer type",
                "dataset",
                "language",
                "model architecture",
                "training method",
                "evaluation metric",
                "domain"
            ],
            [
                "Topic",
                "Keywords",
                "Publication Date",
                "Authors",
                "Methodology",
                "Citations",
                "Publication Venue",
                "Data Sources"
            ],
            [
                "Task",
                "Model architecture",
                "Dataset",
                "Evaluation metric",
                "Training technique",
                "Domain",
                "Language",
                "Publication venue"
            ],
            [
                "research problem",
                "Methodology",
                "Evaluation metric",
                "Data source"
            ]
        ],
        "impr+abs": [
            "Data source",
            "Entity type",
            "Database used",
            "Ontology",
            "Question answering system",
            "API used"
        ],
        "impr+120b": [
            "Research problem",
            "Task type",
            "Methodology",
            "Evaluation metric",
            "Dataset",
            "Answer format"
        ],
        "impr+120b+abs": [
            "Data source",
            "Entity extraction technique",
            "Ontology design",
            "Database system",
            "Knowledge graph construction",
            "Question answering system"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R218619",
        "research_problem": "Question answering dataset",
        "orkg_properties": "['method', 'data source', 'research problem', 'Domain', 'Type of knowledge source', 'Question Type', 'language']",
        "nechakhin_result": "['Text similarity', 'Natural language processing', 'Information retrieval', 'Machine learning', 'Semantic analysis', 'Document classification']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation",
        "abstract": "The proliferation of recipes and other food information on the Web presents an opportunity for discovering and organizing diet-related knowledge into a knowledge graph. Currently, there are several ontologies related to food, but they are specialized in specific domains, e.g., from an agricultural, production, or specific health condition point-of-view. There is a lack of a unified knowledge graph that is oriented towards consumers who want to eat healthily, and who need an integrated food suggestion service that encompasses food and recipes that they encounter on a day-to-day basis, along with the provenance of the information they receive. Our resource contribution is a software toolkit that can be used to create a unified food knowledge graph that links the various silos related to food while preserving the provenance information. We describe the construction process of our knowledge graph, the plan for its maintenance, and how this knowledge graph has been utilized in several applications. These applications include a SPARQL-based service that lets a user determine what recipe to make based on ingredients at hand while taking constraints such as allergies into account, as well as a cognitive agent that can perform natural language question answering on the knowledge graph.Resource Website:https://foodkg.github.io\nAmerican Diabetes Association: 4. lifestyle management: standards of medical care in diabetes\u20142018. Diab. Care40(Suppl. 1), S33\u2013S43 (2017)",
        "dimensions": [
            [
                "Natural Language Processing",
                "Machine Learning",
                "Information Retrieval",
                "Text Mining",
                "Semantic Search",
                "Question Answering Systems",
                "Deep Learning",
                "Knowledge Graphs",
                "Information Extraction",
                "Named Entity Recognition",
                "Information Retrieval",
                "Text Classification",
                "Information Extraction",
                "Information Retrieval",
                "Text Summarization"
            ],
            [
                "dataset type",
                "source",
                "language",
                "domain",
                "size",
                "annotation type",
                "creation date",
                "license"
            ],
            [
                "Topic",
                "Dataset Size",
                "Evaluation Metrics",
                "Model Architectures",
                "Benchmark Datasets",
                "Application Domains",
                "Data Sources"
            ],
            [
                "Dataset source",
                "Language",
                "Task type",
                "Domain",
                "Size",
                "Annotation type",
                "Evaluation metrics",
                "Publication date"
            ],
            [
                "Dataset name",
                "Data source",
                "Annotation type",
                "Task type",
                "Evaluation metric",
                "Language",
                "Domain",
                "License"
            ]
        ],
        "impr+abs": [
            "Knowledge graph type",
            "Resource contribution",
            "Application",
            "Provenance preservation"
        ],
        "impr+120b": [
            "Task type",
            "Data modality",
            "Domain",
            "Evaluation metric",
            "Dataset size",
            "Language"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Resource type",
            "Construction methodology",
            "Maintenance strategy",
            "Applications",
            "Data integration",
            "Provenance handling"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R220967",
        "research_problem": "Question Generation",
        "orkg_properties": "['method', 'data source', 'research problem', 'Domain', 'Type of knowledge source']",
        "nechakhin_result": "['problem statement', 'research question', 'objectives', 'data collection methods', 'data analysis techniques', 'findings', 'limitations', 'recommendations', 'future research directions']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Exploiting a Large-scale Knowledge Graph for Question Generation in Food Preference Interview Systems",
        "abstract": "This paper presents a dialogue system that acquires user's food preference through a conversation. First, we proposed a method for selecting relevant topics and generating questions based on Freebase, a large-scale knowledge graph. To select relevant topics, using the Wikipedia corpus, we created a topic-embedding model that represents the correlation among topics. For missing entities in Freebase, knowledge completion was applied using knowledge graph embedding. We incorporated these functions into a dialogue system and conducted a user study. The results reveal that the proposed dialogue system more efficiently elicited words related to food and common nouns, and these words were highly correlated in a word embedding space.",
        "dimensions": [
            [
                "Topic",
                "Keywords",
                "Publication date",
                "Authors",
                "Journals/conferences",
                "Citations",
                "Abstract",
                "Research methodology",
                "Research findings",
                "Research limitations",
                "Research implications",
                "Research contributions"
            ],
            [
                "topic",
                "target audience",
                "question type",
                "difficulty level",
                "cognitive domain",
                "learning objective"
            ],
            [
                "Research Problem",
                "Keywords and Phrases",
                "Publication Date",
                "Authors and Affiliations",
                "Methodology and Approach",
                "Publication Venue",
                "Citations and References",
                "Data and Results"
            ],
            [
                "Type of question",
                "Subject or topic of the question",
                "Difficulty level of the question",
                "Target audience for the question",
                "Purpose of the question"
            ],
            [
                "research problem",
                "Entity type",
                "provides API",
                "uses identifier system",
                "Metadata schema"
            ]
        ],
        "impr+abs": [
            "Knowledge graph",
            "Question generation method",
            "User study",
            "Entity completion",
            "Correlation model"
        ],
        "impr+120b": [
            "research problem",
            "task definition",
            "methodology",
            "dataset",
            "evaluation metric",
            "application domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Application domain",
            "Data source",
            "Methodology",
            "Knowledge graph",
            "Evaluation method"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R218619",
        "research_problem": "Question Answering using SPARQL",
        "orkg_properties": "['description', 'research problem', 'Domain', 'question answering task', 'question answering components', 'Question Answering category']",
        "nechakhin_result": "['Natural Language Processing', 'Information Retrieval', 'Question Answering Systems', 'Semantic Web', 'SPARQL', 'Linked Data', 'Semantic Queries']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation",
        "abstract": "The proliferation of recipes and other food information on the Web presents an opportunity for discovering and organizing diet-related knowledge into a knowledge graph. Currently, there are several ontologies related to food, but they are specialized in specific domains, e.g., from an agricultural, production, or specific health condition point-of-view. There is a lack of a unified knowledge graph that is oriented towards consumers who want to eat healthily, and who need an integrated food suggestion service that encompasses food and recipes that they encounter on a day-to-day basis, along with the provenance of the information they receive. Our resource contribution is a software toolkit that can be used to create a unified food knowledge graph that links the various silos related to food while preserving the provenance information. We describe the construction process of our knowledge graph, the plan for its maintenance, and how this knowledge graph has been utilized in several applications. These applications include a SPARQL-based service that lets a user determine what recipe to make based on ingredients at hand while taking constraints such as allergies into account, as well as a cognitive agent that can perform natural language question answering on the knowledge graph.Resource Website:https://foodkg.github.io\nAmerican Diabetes Association: 4. lifestyle management: standards of medical care in diabetes\u20142018. Diab. Care40(Suppl. 1), S33\u2013S43 (2017)",
        "dimensions": [
            [
                "Natural Language Processing",
                "Question Answering",
                "Semantic Web",
                "Knowledge Graph",
                "SPARQL",
                "Information Retrieval",
                "Machine Learning",
                "Artificial Intelligence"
            ],
            [
                "natural language processing",
                "question answering",
                "semantic web",
                "SPARQL query language",
                "knowledge graph",
                "entity linking",
                "query formulation",
                "answer extraction",
                "machine learning",
                "evaluation metrics"
            ],
            [
                "Natural Language Processing (NLP)",
                "Semantic Web",
                "Information Retrieval",
                "Machine Learning",
                "Knowledge Graphs",
                "Query Optimization"
            ],
            [
                "question type",
                "SPARQL query structure",
                "knowledge graph domain",
                "NLP techniques",
                "evaluation metrics",
                "dataset and benchmarks",
                "semantic web technologies",
                "query expansion and reformulation"
            ],
            [
                "Research problem",
                "SPARQL query",
                "Knowledge base",
                "Evaluation metric",
                "Data source",
                "Methodology"
            ]
        ],
        "impr+abs": [
            "Knowledge graph type",
            "Resource contribution",
            "Application",
            "Provenance preservation"
        ],
        "impr+120b": [
            "Research problem",
            "Methodology",
            "Technology used",
            "Query language",
            "Task type",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Resource type",
            "Construction methodology",
            "Maintenance strategy",
            "Applications",
            "Data integration",
            "Provenance handling"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R193153",
        "research_problem": "motion synthesis",
        "orkg_properties": "['dataset', 'research problem', 'Models']",
        "nechakhin_result": "['motion capture data', 'motion analysis', 'animation', 'motion planning', 'human movement', 'kinematics', 'dynamics', 'humanoid robots', 'machine learning', 'deep learning', 'computer vision', 'biomechanics', 'cognitive science']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Bayesian Adversarial Human Motion Synthesis",
        "abstract": ":We propose a generative probabilistic model for human motion synthesis. Our model has a hierarchy of three layers. At the bottom layer, we utilize Hidden semi-Markov Model (HSMM), which explicitly models the spatial pose, temporal transition and speed variations in motion sequences. At the middle layer, HSMM parameters are treated as random variables which are allowed to vary across data instances in order to capture large intra- and inter-class variations. At the top layer, hyperparameters define the prior distributions of parameters, preventing the model from overfitting. By explicitly capturing the distribution of the data and parameters, our model has a more compact parameterization compared to GAN-based generative models. We formulate the data synthesis as an adversarial Bayesian inference problem, in which the distributions of generator and discriminator parameters are obtained for data synthesis. We evaluate our method through a variety of metrics, where we show advantage than other competing methods with better fidelity and diversity. We further evaluate the synthesis quality as a data augmentation method for recognition task. Finally, we demonstrate the benefit of our fully probabilistic approach in data restoration task.\n",
        "dimensions": [
            [
                "Motion",
                "Synthesis",
                "Computer Graphics",
                "Animation",
                "Machine Learning",
                "Data-driven",
                "Human Motion",
                "Virtual Reality",
                "Simulation",
                "Character Animation"
            ],
            [
                "motion type",
                "data source",
                "motion capture technology",
                "application domain",
                "evaluation metrics"
            ],
            [
                "Application Domain",
                "Techniques and Algorithms",
                "Data Representation",
                "Evaluation Metrics",
                "Application Context",
                "Challenges and Limitations",
                "Publication Venue"
            ],
            [
                "Type of motion",
                "Data source",
                "Synthesis technique",
                "Application domain",
                "Evaluation metrics",
                "Publication date",
                "Computational resources",
                "Software tools"
            ],
            [
                "Motion type",
                "Data source",
                "Modeling approach",
                "Evaluation metric",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "Generative model type",
            "Model layers",
            "Parameter variation",
            "Evaluation metrics",
            "Data augmentation method",
            "Probabilistic approach",
            "Task type"
        ],
        "impr+120b": [
            "research problem",
            "methodology",
            "data used",
            "evaluation metric",
            "application domain",
            "output format"
        ],
        "impr+120b+abs": [
            "Problem domain",
            "Model type",
            "Model architecture",
            "Inference approach",
            "Evaluation metrics",
            "Application",
            "Baseline comparison"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189630",
        "research_problem": "3D Rotation Estimation",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "['Computer Vision', 'Pose Estimation', 'Geometry', 'Transformations', 'Feature Extraction']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "A Fast Inverse Kinematics Algorithm for Joint Animation",
        "abstract": "The cyclic coordinate descent(CCD) is a well-known algorithm used for inverse kinematics solutions in multi-joint chains. CCD algorithm can be easily implemented, but it can take a series of iterations before converging to a solution and also generate improper joint rotations. This paper presents a novel Target Triangle algorithm that can fast decides orientation and angle of joint rotation, and eliminates problems associated with improper and large angle rotations. Experimental results are presented to show the performance benefits of the proposed algorithm over CCD methods.",
        "dimensions": [
            [
                "Computer Vision",
                "3D Modeling",
                "Machine Learning",
                "Pose Estimation",
                "Feature Matching",
                "Geometric Transformations",
                "Deep Learning",
                "Sensor Fusion",
                "Robotics",
                "Simultaneous Localization and Mapping (SLAM)"
            ],
            [
                "rotation axis",
                "rotation angle",
                "rotation matrix",
                "quaternions",
                "3D point cloud",
                "camera calibration",
                "feature matching",
                "pose estimation",
                "sensor fusion"
            ],
            [
                "Problem Domain",
                "Techniques and Algorithms",
                "Applications",
                "Datasets",
                "Performance Metrics",
                "Publication Venue",
                "Time Period"
            ],
            [
                "Model",
                "Dataset",
                "Application",
                "Evaluation Metrics",
                "Algorithm",
                "Sensor Modality",
                "Publication Venue"
            ],
            [
                "Research problem",
                "Data input",
                "Algorithm",
                "Evaluation metric",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "Algorithm type",
            "Problem statement",
            "Solution method",
            "Experimental results"
        ],
        "impr+120b": [
            "research problem",
            "target variable",
            "methodology",
            "data used",
            "evaluation metric",
            "application domain"
        ],
        "impr+120b+abs": [
            "research problem",
            "baseline method",
            "proposed algorithm",
            "evaluation approach",
            "performance metric",
            "application domain"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189637",
        "research_problem": "Motion Capture",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "['Human movement',\n 'Pose estimation',\n 'Kinematics',\n '3D tracking',\n 'Skeleton tracking',\n 'Markerless motion capture',\n 'Optical motion capture',\n 'Inertial motion capture',\n 'Biomechanics',\n 'Animation',\n 'Computer vision',\n 'Pattern recognition',\n 'Machine learning',\n 'Data-driven modeling',\n 'Human-computer interaction']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Human upper-body inverse kinematics for increased embodiment in consumer-grade virtual reality",
        "abstract": "Having a virtual body can increase embodiment in virtual reality (VR) applications. However, comsumer-grade VR falls short of delivering sufficient sensory information for full-body motion capture. Consequently, most current VR applications do not even show arms, although they are often in the field of view. We address this shortcoming with a novel human upper-body inverse kinematics algorithm specifically targeted at tracking from head and hand sensors only. We present heuristics for elbow positioning depending on the shoulder-to-hand distance and for avoiding reaching unnatural joint limits. Our results show that our method increases the accuracy compared to general inverse kinematics applied to human arms with the same tracking input. In a user study, participants preferred our method over displaying disembodied hands without arms, but also over a more expensive motion capture system. In particular, our study shows that virtual arms animated with our inverse kinematics system can be used for applications involving heavy arm movement. We demonstrate that our method can not only be used to increase embodiment, but can also support interaction involving arms or shoulders, such as holding up a shield.",
        "dimensions": [
            [
                "Motion capture technology",
                "3D animation",
                "Computer vision",
                "Human movement analysis",
                "Biomechanics",
                "Machine learning",
                "Data processing",
                "Virtual reality",
                "Augmented reality",
                "Gesture recognition"
            ],
            [
                "technology used",
                "application area",
                "data capture frequency",
                "number of sensors",
                "body parts tracked",
                "accuracy level",
                "real-time capability",
                "data processing software"
            ],
            [
                "Technology",
                "Application",
                "Data Processing",
                "Interdisciplinary",
                "Accuracy and Precision",
                "Human vs. Non-Human Motion",
                "Validation and Calibration"
            ],
            [
                "Type of motion capture technology",
                "Application domain",
                "Motion capture setup",
                "Data processing and analysis techniques",
                "Performance metrics",
                "Motion capture dataset",
                "Motion capture applications"
            ],
            [
                "Motion capture technology",
                "Data processing",
                "Application field",
                "Accuracy measurement"
            ]
        ],
        "impr+abs": [
            "Embodiment in VR applications",
            "Inverse kinematics algorithm",
            "User study",
            "Motion capture system",
            "Shoulder-to-hand distance",
            "Joint limits",
            "Arm movement",
            "Interaction support"
        ],
        "impr+120b": [
            "Research problem",
            "Data acquisition method",
            "Technology used",
            "Application domain",
            "Evaluation metric",
            "Hardware"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Proposed method",
            "Input sensors",
            "Evaluation approach",
            "User study results",
            "Application scenario"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189640",
        "research_problem": "Orientation",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "[\"Research area\",\n \"Research question\",\n \"Methodology\",\n \"Data used\",\n \"Key findings\",\n \"Publication year\",\n \"Authors\",\n \"Citations\",\n \"Keywords\",\n \"Related work\",\n \"Funding sources\"]",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Real-Time Inverse Kinematics Techniques for Anthropomorphic Limbs",
        "abstract": "In this paper we develop a set of inverse kinematics algorithms suitable for an anthropomorphic arm or leg. We use a combination of analytical and numerical methods to solve generalized inverse kinematics problems including position, orientation, and aiming constraints. Our combination of analytical and numerical methods results in faster and more reliable algorithms than conventional inverse Jacobian and optimization-based techniques. Additionally, unlike conventional numerical algorithms, our methods allow the user to interactively explore all possible solutions using an intuitive set of parameters that define the redundancy of the system.",
        "dimensions": [
            [
                "Research problem",
                "Keywords",
                "Publication date",
                "Research methodology",
                "Authors",
                "Citations",
                "Journal/conference",
                "Research field",
                "Research findings"
            ],
            [
                "spatial orientation",
                "mental orientation",
                "physical orientation",
                "cultural orientation",
                "educational orientation",
                "career orientation"
            ],
            [
                "Topic",
                "Keywords",
                "Publication Date",
                "Authors",
                "Methodology",
                "Journals/Conferences",
                "Citations"
            ],
            [
                "research field",
                "research methodology",
                "geographic location",
                "time period",
                "key concepts or keywords"
            ],
            [
                "research problem",
                "Objective",
                "Methodology",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Kinematics type",
            "Resolution methods",
            "Algorithm performance"
        ],
        "impr+120b": [
            "research problem",
            "domain",
            "methodology",
            "data used",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Algorithm type",
            "Evaluation metric",
            "Application domain",
            "User interaction"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189646",
        "research_problem": "motion reconstruction",
        "orkg_properties": "['method', 'paper:published_in', 'research problem', 'deals with']",
        "nechakhin_result": "['motion capture data', 'kinematics', 'temporal dynamics', 'animation', 'trajectory', 'pose estimation', '3D reconstruction', 'motion tracking', 'human movement', 'camera calibration']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Learnt inverse kinematics for animation synthesis",
        "abstract": "Existing work on animation synthesis can be roughly split into two approaches, those that combine segments of motion-capture data, and those that perform inverse kinematics. In this paper, we present a method for performing animation synthesis of an articulated object (e.g. human body and a dog) from a minimal set of body joint positions, following the approach of inverse kinematics. We tackle this problem from a learning perspective. Firstly, we address the need for knowledge on the physical constraints of the articulated body, so as to avoid the generation of a physically impossible poses. A common solution is to heuristically specify the kinematic constraints for the skeleton model. In this paper however, the physical constraints of the articulated body are represented using a hierarchical cluster model learnt from a motion capture database. Additionally, we shall show that the learnt model automatically captures the correlation between different joints through simultaneous modelling of their angles. We then show how this model can be utilised to perform inverse kinematics in a simple and efficient manner. Crucially, we describe how IK is carried out from a minimal set of end-effector positions. Following this, we show how this \u201clearnt inverse kinematics\u201d framework can be used to perform animation syntheses on different types of articulated structures. To this end, the results presented include the retargeting of a flat surface walking animation to various uneven terrains to demonstrate the synthesis of a full human body motion from the positions of only the hands, feet and torso. Additionally, we show how the same method can be applied to the animation synthesis of a dog using only its feet and torso positions.",
        "dimensions": [
            [
                "Motion capture",
                "Human movement",
                "Kinematics",
                "3D reconstruction",
                "Computer vision",
                "Biomechanics",
                "Pose estimation",
                "Skeleton tracking",
                "Gesture recognition"
            ],
            [
                "motion capture technology",
                "sensors",
                "data processing",
                "algorithm",
                "application",
                "accuracy",
                "real-time processing",
                "noise reduction"
            ],
            [
                "Keywords",
                "Techniques and Algorithms",
                "Application Domains",
                "Data Types",
                "Evaluation Metrics",
                "Publication Date",
                "Authors and Institutions"
            ],
            [
                "Type of motion",
                "Sensor technology",
                "Reconstruction algorithm",
                "Application domain",
                "Accuracy assessment",
                "Data representation",
                "Real-time reconstruction",
                "Environmental factors"
            ],
            [
                "Motion capture technology",
                "Data processing",
                "Body markers",
                "Accuracy assessment",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "Animation synthesis approach",
            "Articulated object type",
            "Learning perspective",
            "Physical constraints",
            "Inverse kinematics method",
            "End-effector positions",
            "Animation synthesis application",
            "Motion capture database"
        ],
        "impr+120b": [
            "research problem",
            "methodology",
            "data used",
            "evaluation metric",
            "application domain",
            "algorithm type"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Data used",
            "Model type",
            "Learning approach",
            "Evaluation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189649",
        "research_problem": "Aiming",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "['Research problem',\n 'Hypotheses',\n 'Methodology',\n 'Data collection',\n 'Data analysis',\n 'Findings',\n 'Discussion',\n 'Conclusion',\n 'Limitations',\n 'Implications',\n 'Future research recommendations',\n 'References']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Analytical inverse kinematics with body posture control",
        "abstract": "This paper presents a novel whole-body analytical inverse kinematics (IK) method integrating collision avoidance and customizable body control for animating reaching tasks in real-time. Whole-body control is achieved with the interpolation of pre-designed key body postures, which are organized as a function of the direction to the goal to be reached. Arm postures are computed by the analytical IK solution for human-like arms and legs, extended with a new simple search method for achieving postures avoiding joint limits and collisions. In addition, a new IK resolution is presented that directly solves for joints parameterized in the swing-and-twist decomposition. The overall method is simple to implement, fast, and accurate, and therefore suitable for interactive applications controlling the hands of characters. The source code of the IK implementation is provided. Copyright \u00a9 2007 John Wiley & Sons, Ltd.",
        "dimensions": [
            [
                "Research problem",
                "Key concepts",
                "Methodology",
                "Results",
                "Publication year",
                "Authors",
                "Citations",
                "Journal/conference",
                "Keywords"
            ],
            [
                "goal",
                "target",
                "objective",
                "purpose",
                "intention"
            ],
            [
                "Research Problem",
                "Methodology",
                "Key Concepts",
                "Publication Date",
                "Authors",
                "Citation Network",
                "Journals/Conferences"
            ],
            [
                "social media platform",
                "mental health metrics",
                "user demographics",
                "types of social media usage",
                "research methodology"
            ],
            [
                "research problem",
                "Objective",
                "Methodology",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Inverse kinematics method",
            "Body posture control",
            "Collision avoidance",
            "Real-time animation",
            "IK resolution",
            "Interactive applications",
            "Source code availability"
        ],
        "impr+120b": [
            "Research objective",
            "Motivation",
            "Scope",
            "Expected outcome",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Algorithm type",
            "Control strategy",
            "Functional capability",
            "Performance characteristic",
            "Software availability",
            "Application domain"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189652",
        "research_problem": "2D Human Pose Estimation",
        "orkg_properties": "['method', 'paper:published_in', 'Algorithm', 'research problem', 'deals with']",
        "nechakhin_result": "['Computer Vision', 'Deep Learning', 'Image Processing', 'Pose Estimation', 'Human Body', 'Human Detection', 'Skeleton Tracking', 'Joint Localization', 'Keypoint Detection', 'Convolutional Neural Networks', 'Feature Extraction', 'Data Augmentation']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Style-based inverse kinematics",
        "abstract": "This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in real-time. Training the model on different input data leads to different styles of IK. The model is represented as a probability distribution over the space of all possible poses. This means that our IK system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles.Our style-based IK can replace conventional IK, wherever it is used in computer animation and computer vision. We demonstrate our system in the context of a number of applications: interactive character posing, trajectory keyframing, real-time motion capture with missing markers, and posing from a 2D image.",
        "dimensions": [
            [
                "Computer Vision",
                "Deep Learning",
                "Human Pose Estimation",
                "Image Processing",
                "Machine Learning",
                "Artificial Intelligence",
                "Skeleton Tracking",
                "Pose Recognition",
                "3D Pose Estimation"
            ],
            [
                "pose estimation method",
                "dataset used",
                "training algorithm",
                "evaluation metric",
                "pose representation",
                "image resolution",
                "pose estimation accuracy",
                "computational complexity"
            ],
            [
                "Methodology",
                "Datasets",
                "Evaluation Metrics",
                "Application Domain",
                "Year of Publication",
                "Citations and Impact"
            ],
            [
                "Model architecture",
                "Dataset",
                "Evaluation metric",
                "Preprocessing techniques",
                "Post-processing methods",
                "Application domain",
                "Publication date",
                "Keypoint representation"
            ],
            [
                "Research problem",
                "Data used",
                "Algorithm",
                "Evaluation metric",
                "Application",
                "Accuracy",
                "Limitation"
            ]
        ],
        "impr+abs": [
            "Inverse kinematics system",
            "Learning model",
            "Probability distribution",
            "Training data",
            "Model representation",
            "Automatic learning",
            "Interpolation procedure",
            "Application context"
        ],
        "impr+120b": [
            "research problem",
            "task type",
            "data modality",
            "methodology",
            "evaluation metric",
            "output format",
            "application area"
        ],
        "impr+120b+abs": [
            "Model type",
            "Training data",
            "Constraints handling",
            "Real-time capability",
            "Style interpolation method",
            "Application scenarios"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189876",
        "research_problem": " Xray spectroscopy of  highly ionized atoms",
        "orkg_properties": "['research problem', 'Paper type', 'has system qualities']",
        "nechakhin_result": "['X-ray spectroscopy',\n 'Highly ionized atoms']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 1,
        "nechakhin_deviation": 4,
        "title": "Precision Wavelength Determination of 2^1P_1 - 1^1S_0 and 2^3P_1 - 1^1S_0 Transitions in Helium-Like Sulfur Ions",
        "abstract": "Transitions from the 21P1- and 23P1-state to the ground state 11S0in helium-like sulphur ions have been measured with an accuracy of 4\u00d710-5. Energy calibration is described in detail and two reference wavelengths have been reevaluated. Substantial line-blending was observed, due to long-lived spectator electrons. The two transition energies were corrected for Doppler shift and compared with most refined theoretical calculations, including terms of order \u03b14Z6in the Breit operator and terms of order \u03b15Z6in the quantum-electrodynamical corrections. The experimental contributions to the ground-state QED shifts agree within its error (\u223c 15%) with the theoretical values.Export citation and abstractBibTeXRIS\nTransitions from the 21P1- and 23P1-state to the ground state 11S0in helium-like sulphur ions have been measured with an accuracy of 4\u00d710-5. Energy calibration is described in detail and two reference wavelengths have been reevaluated. Substantial line-blending was observed, due to long-lived spectator electrons. The two transition energies were corrected for Doppler shift and compared with most refined theoretical calculations, including terms of order \u03b14Z6in the Breit operator and terms of order \u03b15Z6in the quantum-electrodynamical corrections. The experimental contributions to the ground-state QED shifts agree within its error (\u223c 15%) with the theoretical values.",
        "dimensions": [
            [
                "X-ray spectroscopy",
                "Highly ionized atoms",
                "Atomic physics",
                "Spectroscopic analysis",
                "Ionization",
                "X-ray emission",
                "Plasma diagnostics"
            ],
            [
                "type of highly ionized atoms",
                "x-ray spectroscopy technique",
                "energy range",
                "spectral resolution",
                "experimental setup",
                "theoretical modeling"
            ],
            [
                "Research Problem",
                "Atomic Structure",
                "Spectroscopic Techniques",
                "Ionization States",
                "Experimental Methods",
                "Theoretical Models",
                "Applications",
                "Data Analysis"
            ],
            [
                "Atomic properties",
                "Spectroscopy technique",
                "Experimental setup",
                "Spectral analysis methods",
                "Ionization states",
                "Applications",
                "Theoretical models"
            ],
            [
                "Spectroscopy technique",
                "Atomic state",
                "Analysis method",
                "Energy level"
            ]
        ],
        "impr+abs": [
            "Measurement accuracy",
            "Energy calibration",
            "Reference wavelengths",
            "Line-blending",
            "Doppler shift correction",
            "Theoretical comparison",
            "QED shifts",
            "Experimental contributions"
        ],
        "impr+120b": [
            "research problem",
            "technique",
            "subject of study",
            "instrumentation",
            "measurement type",
            "data type"
        ],
        "impr+120b+abs": [
            "Physical system",
            "Measured transitions",
            "Measurement accuracy",
            "Calibration method",
            "Observed effects",
            "Theoretical comparison",
            "Error analysis"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R219153",
        "research_problem": "Spectroscopy of Highly Charged Ions (HCI) using a calorimeter",
        "orkg_properties": "['research problem', 'Paper type', 'has system qualities', 'has system measurements']",
        "nechakhin_result": "['Spectroscopy method used', 'Ions properties', 'Calorimeter properties', 'Energy levels', 'Ionization potential', 'Charge of ions', 'Calorimeter techniques', 'Calorimeter sensitivity', 'Calorimeter resolution', 'Calorimeter temperature range', 'Calorimeter materials']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "An Electron Beam Ion Trap (EBIT) Plus a Microcalorimeter: A Good Combination for Laboratory Astrophysics",
        "abstract": "An EBIT can selectively create, in principle, any charge state of every naturally occurring element, has good control on atomic collision processes, and can produce nearly ideal conditions for the analysis of highly ionized plasmas of astrophysical importance. A microcalorimeter enables the broadband detection of x-ray emission with high energy resolution and near-unity quantum efficiency in the energy range wherein many cosmic x-ray sources emit the bulk of their energy (0.2\u00a0keV\u201310\u00a0keV). The combination (EBIT+ microcalorimeter) provides a powerful tool for laboratory studies of the atomic/plasma processes underlying the energy release mechanisms in cosmic x-ray sources. We briefly describe some early experiments with a microcalorimeter built by the Smithsonian Astrophysical Observatory (SAO) and deployed on the NIST EBIT. We also present some very recent observations with a more advanced microcalorimeter built by SAO that can obtain an energy resolution of 4.5\u00a0eV. The higher spectral quality produced by the new system will be useful in laboratory measurements of interest in x-ray astronomy.Export citation and abstractBibTeXRIS\nAn EBIT can selectively create, in principle, any charge state of every naturally occurring element, has good control on atomic collision processes, and can produce nearly ideal conditions for the analysis of highly ionized plasmas of astrophysical importance. A microcalorimeter enables the broadband detection of x-ray emission with high energy resolution and near-unity quantum efficiency in the energy range wherein many cosmic x-ray sources emit the bulk of their energy (0.2\u00a0keV\u201310\u00a0keV). The combination (EBIT+ microcalorimeter) provides a powerful tool for laboratory studies of the atomic/plasma processes underlying the energy release mechanisms in cosmic x-ray sources. We briefly describe some early experiments with a microcalorimeter built by the Smithsonian Astrophysical Observatory (SAO) and deployed on the NIST EBIT. We also present some very recent observations with a more advanced microcalorimeter built by SAO that can obtain an energy resolution of 4.5\u00a0eV. The higher spectral quality produced by the new system will be useful in laboratory measurements of interest in x-ray astronomy.",
        "dimensions": [
            [
                "Spectroscopy",
                "Highly Charged Ions (HCI)",
                "Calorimeter",
                "Atomic Physics",
                "X-ray Spectroscopy",
                "Plasma Physics",
                "Experimental Physics"
            ],
            [
                "type of spectroscopy",
                "highly charged ions (HCI)",
                "calorimeter",
                "experimental setup",
                "energy resolution",
                "spectral range",
                "data analysis method"
            ],
            [
                "Spectroscopy Technique",
                "Highly Charged Ions (HCI)",
                "Calorimeter",
                "Energy Range",
                "Application Area"
            ],
            [
                "Spectroscopy technique",
                "Highly charged ions (HCI)",
                "Calorimeter type",
                "Energy range",
                "Temperature range",
                "Material composition",
                "Research environment",
                "Publication date"
            ],
            [
                "Spectroscopy technique",
                "Ions",
                "Calorimeter type",
                "Experimental method"
            ]
        ],
        "impr+abs": [
            "Laboratory equipment",
            "Research focus",
            "Energy range",
            "Observation method"
        ],
        "impr+120b": [
            "research problem",
            "target entity",
            "analysis technique",
            "instrumentation",
            "measurement type"
        ],
        "impr+120b+abs": [
            "Instrumentation",
            "Detector technology",
            "Energy range",
            "Energy resolution",
            "Experimental facility",
            "Application field",
            "Research focus"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R235828",
        "research_problem": "Spectroscopy in  highly charged  Iron ions",
        "orkg_properties": "['research problem', 'Paper type', 'has system qualities', 'has system measurements']",
        "nechakhin_result": "['spectroscopy', 'highly charged', 'iron ions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Laboratory Measurements and Modeling of the Fe XVII X\u2010Ray Spectrum",
        "abstract": "Detailed measurements, line identifications, and modeling calculations of the Fe XVII L-shell emission spectrum between 9.8 and 17.5 \u00c5 are presented. The measurements were carried out on an electron beam ion trap under precisely controlled conditions where electron-impact excitation followed by radiative cascades is the dominant line formation process. In addition to the strong transitions emanating from then= 3 shell, we identify and accurately determine wavelengths for transitions from higher shells up ton= 11, including two electric quadrupole transitions that have not been previously identified. Various theoretical values, including new distorted wave calculations, are compared to our measurements, which establish definitive values for testing spectral modeling predictions. We find a value of 3.04 \u00b1 0.12 for the ratio of the intensity of the 2p-3d1P1resonance and of the 2p-3d3D1intercombination line situated at 15.01 and 15.26 \u00c5, respectively. This value is higher than the values observed in solar spectra, which supports claims that the solar value is affected by resonant scattering. However, because our value is significantly lower than calculated values, the amount of scattering has probably been overestimated in past analyses. Comparisons of the measured intensity ratios of the transitions originating in levels of higher principal quantum numbernwith present distorted wave calculations show good agreement up ton= 6. The combined flux of all 2p-ndtransitions withn\u2265 5 and all 2s-nptransitions withn= 4 and 5 relative to the flux of the 15.01 \u00c5 resonance line has been measured to be 0.13+ 0.04\u22120.03.Export citation and abstractBibTeXRIS\nDetailed measurements, line identifications, and modeling calculations of the Fe XVII L-shell emission spectrum between 9.8 and 17.5 \u00c5 are presented. The measurements were carried out on an electron beam ion trap under precisely controlled conditions where electron-impact excitation followed by radiative cascades is the dominant line formation process. In addition to the strong transitions emanating from then= 3 shell, we identify and accurately determine wavelengths for transitions from higher shells up ton= 11, including two electric quadrupole transitions that have not been previously identified. Various theoretical values, including new distorted wave calculations, are compared to our measurements, which establish definitive values for testing spectral modeling predictions. We find a value of 3.04 \u00b1 0.12 for the ratio of the intensity of the 2p-3d1P1resonance and of the 2p-3d3D1intercombination line situated at 15.01 and 15.26 \u00c5, respectively. This value is higher than the values observed in solar spectra, which supports claims that the solar value is affected by resonant scattering. However, because our value is significantly lower than calculated values, the amount of scattering has probably been overestimated in past analyses. Comparisons of the measured intensity ratios of the transitions originating in levels of higher principal quantum numbernwith present distorted wave calculations show good agreement up ton= 6. The combined flux of all 2p-ndtransitions withn\u2265 5 and all 2s-nptransitions withn= 4 and 5 relative to the flux of the 15.01 \u00c5 resonance line has been measured to be 0.13+ 0.04\u22120.03.",
        "dimensions": [
            [
                "Spectroscopy",
                "Highly charged ions",
                "Iron ions",
                "Atomic physics",
                "X-ray spectroscopy",
                "Plasma spectroscopy",
                "Spectroscopic techniques",
                "Spectroscopic analysis"
            ],
            [
                "type of spectroscopy",
                "highly charged iron ions",
                "experimental setup",
                "spectral range",
                "ionization potential",
                "energy levels",
                "transition probabilities",
                "collisional effects"
            ],
            [
                "Spectroscopy Technique",
                "Ionization State",
                "Energy Range",
                "Environmental Conditions",
                "Application Area"
            ],
            [
                "Ionization state",
                "Spectral range",
                "Experimental technique",
                "Plasma conditions",
                "Theoretical modeling",
                "Plasma sources",
                "Spectral line identification",
                "Applications"
            ],
            [
                "Spectroscopy technique",
                "Ion type",
                "Experimental conditions",
                "Data analysis method"
            ]
        ],
        "impr+abs": [
            "Measurement technique",
            "Line identification",
            "Modeling calculations",
            "Comparison to theoretical values"
        ],
        "impr+120b": [
            "Research problem",
            "Target ion",
            "Charge state",
            "Spectroscopic technique",
            "Experimental method",
            "Data type"
        ],
        "impr+120b+abs": [
            "Experimental setup",
            "Measured quantities",
            "Theoretical method",
            "Spectral range",
            "Species studied",
            "Evaluation metric",
            "Reference data"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R203757",
        "research_problem": "COVID-19 and supply chain management",
        "orkg_properties": "['Methodology', 'research problem', 'Area of study', 'used method']",
        "nechakhin_result": "['COVID-19 impact', 'Supply chain disruptions', 'Supply chain resilience', 'Supply chain risk management', 'Pandemic response', 'Logistics', 'Inventory management', 'Demand forecasting', 'Supplier relationship management', 'Digital transformation', 'Information sharing', 'Transportation management']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "Corona virus, tariffs, trade wars and supply chain evolutionary design",
        "abstract": "PurposeUsing the constructal law of physics this study aims to provide guidance to future scholarship on global supply chain management. Further, through two case studies the authors are developing, the authors report interview findings with two senior VPs from two multi-national corporations being disrupted by COVID-19. This study suggests how this and recent events will impact on the design of future global supply chains.Design/methodology/approachThe authors apply the constructal law to explain the recent disruptions to the global supply chain orthodoxy. Two interviews are presented from case studies the authors are developing in the USA and UK \u2013 one a multi-national automobile parts supplier and the other is a earth-moving equipment manufacture. Specifically, this is an exploratory pathway work trying to make sense of the COVID-19 pandemic and its impact on supply chain scholarship.FindingsAdopting the approach of Bejan, the authors believe that what is happening today with COVID-19 and other trade disruptions such as Brexit and the USA imposing tariffs is creating new obstacles that will redirect the future flow of supply chains.Research limitations/implicationsIt is clear that the COVID-19 response introduced a bullwhip effect in the manufacturing sector on a scale never-before seen. For scholars, the authors would suggest there are four pathway topics going forward. These topics include: the future state of global sourcing, the unique nature of a combined \u201cdemand\u201d and \u201csupply shortage\u201d bullwhip effect, the resurrection of lean and local production systems and the development of risk-recovery contingency strategies to deal with pandemics.Practical implicationsSupply chain managers tend to be iterative and focused on making small and subtle changes to their current system and way of thinking, very often seeking to optimize cost or negotiate better contracts with suppliers. In the current environment, however, such activities have proved to be of little consequence compared to the massive forces of economic disruption of the past three years. Organizations that have more tightly compressed supply chains are enjoying a significant benefit during the COVID-19 crisis and are no longer being held hostage to governments of another country.Social implicationsAn implicit assumption in the press is that COVID-19 caught everyone by surprise, and that executives foolishly ignored the risks of outsourcing to China and are now paying the price. However, noted scholars and epidemiologists have been warning of the threats of pandemics since the severe acute respiratory syndrome (SARS) virus. The pundits would further posit that in their pursuit of low-cost production, global corporations made naive assumptions that nothing could disrupt them. Both the firms the authors have interviewed had to close plants to protect their workforce. It was indicated in the cases the authors are developing that it is going to take manufacturers on average one month to recover from 4\u20136 days of disruption. These companies employ many thousands of people, and direct and ancillary workers are now temporarily laid off and face an uncertain future as/when they will recover back to normal production.Originality/valueUsing the constructal law of physics, the authors seek to provide guidance to future scholarship on global supply chain management. Further, through two case studies, the authors provide the first insight from two senior VPs from two leading multi-national co",
        "dimensions": [
            [
                "COVID-19",
                "pandemic",
                "coronavirus",
                "supply chain",
                "logistics",
                "risk management",
                "resilience",
                "disruption",
                "global trade",
                "inventory management"
            ],
            [
                "pandemic impact",
                "supply chain disruptions",
                "inventory management",
                "logistics",
                "risk management",
                "demand forecasting",
                "resilience",
                "globalization",
                "technology adoption"
            ],
            [
                "Topic",
                "Timeframe",
                "Geographic Location",
                "Industry Sector",
                "Methodology",
                "Impact Assessment",
                "Stakeholders"
            ],
            [
                "Type of supply chain",
                "Geographic location",
                "Industry sector",
                "Risk management strategies",
                "Technology adoption",
                "Inventory management",
                "Resilience and flexibility",
                "Government policies and regulations",
                "Supplier relationships",
                "Environmental sustainability"
            ],
            [
                "research problem",
                "Supply chain disruption",
                "Risk management",
                "Resilience strategies",
                "Supply chain optimization",
                "Impact assessment",
                "Adaptation measures"
            ]
        ],
        "impr+abs": [
            "Purpose",
            "Design/methodology/approach",
            "Findings",
            "Research limitations/implications",
            "Practical implications",
            "Social implications",
            "Originality/value"
        ],
        "impr+120b": [
            "research problem",
            "domain",
            "context",
            "impact",
            "methodology",
            "data source"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Theoretical framework",
            "Methodology",
            "Case study context",
            "Data collection method",
            "Findings",
            "Research limitations",
            "Practical implications"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R203650",
        "research_problem": "Demand forecasting in supply chain ",
        "orkg_properties": "['method', 'metric', 'research problem', 'Data Dimension']",
        "nechakhin_result": "['Time series analysis', 'Statistical modeling', 'Inventory management', 'Forecasting methods', 'Machine learning', 'Data analytics', 'Demand patterns', 'Supply chain management', 'Forecast accuracy', 'Forecast error', 'Demand variability']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Evaluation of deep learning with long short-term memory networks for time series forecasting in supply chain management",
        "abstract": "Performance analysis and forecasting the evolution of complex systems are two challenging tasks in manufacturing. Time series data from complex systems capture the dynamic behaviors of the underlying processes. However, non-linear and non-stationary dynamics pose a major challenge for accurate forecasting. To overcome statistical complexities through analyzing time series, we approach the problem with deep learning methods. In this paper, we mainly focus on the long short-term memory (LSTM) networks for demand forecasts in supply chain management, where the future demand for a certain product is the basis for the respective replenishment systems. This study contributes to the literature by conducting experiments on real data to investigate the potential of using LSTM networks for final customer demand forecasting, and hence for increasing the overall value generated by a supply chain. Both forward LSTM and bidirectional LSTM (forward-backward) for short- and long-term demand prediction in supply chain management are considered in this study.\nThe proper selection of a demand forecasting method is directly linked to the success of supply chain management (SCM). However, today\u2019s manufacturing companies are confronted with uncertain and dynamic markets. Consequently, classical statistical methods are not always appropriate for accurate and reliable forecasting. Algorithms of Artificial intelligence (AI) are currently used to improve statistical methods. Existing literature only gives a very general overview of the AI methods used in combination with demand forecasting. This paper provides an analysis of the AI methods published in the last five years (2017-2021). Furthermore, a classification is presented by clustering the AI methods in order to define the trend of the methods applied. Finally, a classification of the different AI methods according to the dimensionality of data, volume of data, and time horizon of the forecast is presented. The goal is to support the selection of the appropriate AI method to optimize demand forecasting.\nNumerous recent studies have attempted to create efficient mechanical trading systems through the use of machine learning approaches for stock price estimation and portfolio management. Using the ability to foresee the future trends of the stock performance, the return of investment can be maximized for short-term trading. This paper will review various Artificial Intelligence (AI) and Machine Learning (ML) strategies for stock price forecasting. The aim of this review is to discuss various techniques for stock price prediction that incorporate ARIMA, LSTM, Hybrid LSTM, CNN, and Hybrid CNN. Additionally, it will also discuss the limitations and accuracy of the various models, including the ARIMA model, the LSTM model, the MI-LSTM model, the Bi-LSTM model, the LSTM-DRNN model, the CNN model, the GC-CNN model, the CNN-LSTM model, the CNN-TLSTM model, and the CNN-BiLSTM model, in terms of percentage of accuracy or error calculation in terms of standard accuracy measures like RMSE, MAPE, MAE. The models can be used to forecast either the accurate stock rate, induced by the low MSE, RMSE and MAE of LSTM models, or the general trend and deflection range of the stock the following day, induced by the ability to dynamically capture swift changes in the system of CNN models. These characteristics consequently illustrate the advantages of the hybrid model at efficiently and accurately forecasting stock attributes.",
        "dimensions": [
            [
                "Supply chain management",
                "Demand forecasting",
                "Inventory management",
                "Time series analysis",
                "Forecasting methods",
                "Predictive modeling",
                "Machine learning",
                "Data analytics",
                "Operations research",
                "Logistics",
                "Forecast accuracy",
                "Demand variability"
            ],
            [
                "forecasting method",
                "historical data",
                "product type",
                "market trends",
                "seasonality",
                "lead time",
                "inventory management",
                "supply chain network"
            ],
            [
                "Industry",
                "Time Horizon",
                "Data Sources",
                "Forecasting Methods",
                "Supply Chain Complexity",
                "Performance Metrics",
                "Demand Volatility",
                "Technology Adoption"
            ],
            [
                "Time period",
                "Forecasting methods",
                "Industry or sector",
                "Data sources",
                "Demand variability",
                "Forecast accuracy metrics",
                "Supply chain characteristics",
                "Technology and tools"
            ],
            [
                "Demand forecasting method",
                "Data sources",
                "Forecast accuracy evaluation",
                "Time horizon",
                "Forecasting model",
                "Demand variability",
                "Forecasting frequency"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Forecasting method",
            "Data used",
            "Evaluation metric",
            "AI method",
            "Supply chain management",
            "Stock price forecasting",
            "Model comparison"
        ],
        "impr+120b": [
            "research problem",
            "domain",
            "methodology",
            "data used",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Data type",
            "Evaluation metric",
            "Application domain",
            "AI methods classification",
            "Forecast horizon"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212254",
        "research_problem": "forward supply chain",
        "orkg_properties": "['research problem', 'Mathematical model', 'Solution approach', 'Disruption', 'Objectif function']",
        "nechakhin_result": "['logistics', 'inventory management', 'supplier relationship management', 'transportation management', 'distribution network design', 'demand forecasting', 'order fulfillment', 'collaboration and coordination', 'sustainability and environmental impact']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Strategies for protecting supply chain networks against facility and transportation disruptions: an improved Benders decomposition approach",
        "abstract": "Disruptions rarely occur in supply chains, but their negative financial and technical impacts make the recovery process very slow. In this paper, we propose a capacitated supply chain network design (SCND) model under random disruptions both in facility and transportation, which seeks to determine the optimal location and types of distribution centers (DC) and also the best plan to assign customers to each opened DC. Unlike other studies in the extent literature, we use new concepts of reliability to model the strategic behavior of DCs and customers at the network: (1) Failure of DCs might be partial, i.e. a disrupted DC might still be able to serve with a portion of its initial capacity (2) The lost capacity of a disrupted DC shall be provided from a non-disrupted one and (3) The lost capacity fraction of a disrupted DC depends on its initial investment amount in the design phase.In order to solve the proposed model optimally, a modified version of Benders\u2019 Decomposition (BD) is applied. This modification tackles the difficulties of the BD\u2019s master problem (MP), which ultimately improves the solution time of BD significantly. The classical BD approach results in low density cuts in some cases, Covering Cut Bundle (CCB) generation addresses this issue by generating a bundle of cuts instead of a single cut, which could cover more decision variables of the MP. Our inspiration to improve the CCB generation led to a new method, namely Maximum Density Cut (MDC) generation. MDC is based on the observation that in some cases CCB generation is cumbersome to solve in order to cover all decision variables of the MP rather than to cover part of them. Thus the MDC method generates a cut to cover the remaining decision variables which are not covered by CCB. Numerical experiments demonstrate the practicability of the proposed model to be promising in the SCND area, also the modified BD approach decreases the number of BD iterations and improves the CPU times, significantly.\n",
        "dimensions": [
            [
                "Supply chain management",
                "Logistics",
                "Inventory management",
                "Procurement",
                "Distribution",
                "Supplier relationship management",
                "Demand forecasting",
                "Transportation",
                "Warehousing",
                "Sustainability in supply chain",
                "Risk management in supply chain",
                "Global supply chain",
                "E-commerce in supply chain"
            ],
            [
                "logistics",
                "inventory management",
                "transportation",
                "supplier management",
                "demand forecasting",
                "procurement",
                "distribution channels",
                "sustainability practices"
            ],
            [
                "Industry",
                "Supply Chain Stage",
                "Technology",
                "Sustainability",
                "Risk Management",
                "Globalization",
                "Performance Metrics",
                "Collaboration and Partnerships"
            ],
            [
                "Type of industry",
                "Supply chain stages",
                "Technology and automation",
                "Sustainability and environmental impact",
                "Risk management",
                "Collaboration and partnerships",
                "Globalization and logistics",
                "Performance metrics"
            ],
            [
                "research problem",
                "Supply chain stage",
                "Risk assessment",
                "Sustainability factor",
                "Cost analysis"
            ]
        ],
        "impr+abs": [
            "Supply chain network design model",
            "Disruption type",
            "Solution methodology",
            "Reliability concept",
            "Optimization approach",
            "Numerical experiments",
            "Performance metrics"
        ],
        "impr+120b": [
            "research problem",
            "Supply chain orientation",
            "Process stage",
            "Stakeholders involved",
            "Optimization objective",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Modeling approach",
            "Disruption type",
            "Optimization method",
            "Algorithmic enhancement",
            "Evaluation metric",
            "Experimental setup"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212263",
        "research_problem": "Closed-loop supply chain",
        "orkg_properties": "['research problem', 'Uncertainty', 'Solution approach', 'Disruption', 'Objectif function']",
        "nechakhin_result": "['supply chain management', 'reverse logistics', 'circular economy', 'sustainability', 'environmental impact', 'remanufacturing', 'product recovery', 'waste management', 'inventory management', 'green logistics', 'product innovation', 'reducing carbon footprint', 'economic benefits']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Robust and reliable forward\u2013reverse logistics network design under demand uncertainty and facility disruptions",
        "abstract": "There are two broad categories of risk, which influence the supply chain design and management. The first category is concerned with uncertainty embedded in the model parameters, which affects the problem of balancing supply and demand. The second category of risks may arise from natural disasters, strikes and economic disruptions, terroristic acts, and etc. Most of the existing studies surveyed these types of risk, separately. This paper proposes a robust and reliable model for an integrated forward\u2013reverse logistics network design, which simultaneously takes uncertain parameters and facility disruptions into account. The proposed model is formulated based on a recent robust optimization approach to protect the network against uncertainty. Furthermore, amixed integer linear programingmodel with augmentedp-robust constraints is proposed to control the reliability of the network among disruption scenarios. The objective function of the proposed model is minimizing the nominal cost, while reducing disruption risk using thep-robustness criterion. To study the behavior of the robustness and reliability of the concerned network, several numerical examples are considered. Finally, a comparative analysis is carried out to study the performance of the augmentedp-robust criterion and other conventional robust criteria.\nThe forward/reverse logistics network design is an important and strategic issue due to its effects on efficiency and responsiveness of a supply chain. In practice, it is needed to formulate and solve real problems through efficient algorithms in a reasonable time. Hence, this paper tries to cover real case problem with a multi-objective model and an integrated forward/reverse logistics network design. Further, the model is customized and implemented for a case study in gold industry where the reverse logistics play crucial role. A new solution approach is applied for the proposed 7-layer network of the case study and the solutions are achieved in order solve the current difficulties of the investigated supply chain. This paper seeks to address how a multi objective logistics model in the gold industry can be created and solved through an efficient meta-heuristic algorithm. A green approach based on the CO2emission is considered in the network design approach. The developed model includes four echelons in the forward direction and three echelons in the reverse. First, an integer linear programming model is developed to minimize costs and emissions. Then, in order to solve the model, an algorithm based on ant colony optimization is developed. The performance of the proposed algorithm has been compared with the optimum solutions of the LINGO software through various numerical examples based on the random data and real-world instances. The evaluation studies demonstrate that the proposed model is practical and applicable and the developed algorithm is reliable and efficient. The results prove the managerial implications of the model and the solution approach in terms of presenting appropriate modifications to the mangers of the selected supply chain. Further, a Taguchi-based parameter setting is undertaken to ensure using the appropriate parameters for the algorithm.\nIn today\u2019s globalized and highly uncertain business environments, supply chains have become more vulnerable to disruptions. This paper presents a stochastic robust optimization model for the design of a closed-loop supply chain network that performs resiliently in the fa",
        "dimensions": [
            [
                "Supply chain management",
                "Reverse logistics",
                "Closed-loop system",
                "Sustainability",
                "Green supply chain",
                "Circular economy",
                "Remanufacturing",
                "Product recovery",
                "Waste management",
                "Environmental impact"
            ],
            [
                "product",
                "reverse logistics",
                "recycling",
                "remanufacturing",
                "inventory management",
                "sustainability",
                "supply chain network",
                "collaboration",
                "regulations",
                "costs"
            ],
            [
                "Industry",
                "Environmental Impact",
                "Technology",
                "Reverse Logistics",
                "Circular Economy",
                "Supply Chain Management",
                "Economic Considerations"
            ],
            [
                "Type of product",
                "Industry",
                "Environmental impact",
                "Reverse logistics",
                "Technology and innovation",
                "Regulatory framework",
                "Economic considerations",
                "Collaboration and partnerships"
            ],
            [
                "research problem",
                "Reverse logistics",
                "Sustainability factor",
                "Circular economy",
                "Supply chain management",
                "Recycling process",
                "Inventory management"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Uncertainty",
            "Facility disruptions",
            "Optimization approach",
            "Network design",
            "Performance evaluation",
            "Environmental impact",
            "Solution approach"
        ],
        "impr+120b": [
            "research problem",
            "Supply chain type",
            "Sustainability focus",
            "Optimization objective",
            "Methodology",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Risk categories",
            "Modeling approach",
            "Objective function",
            "Case study industry",
            "Solution algorithm",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212214",
        "research_problem": "knowledge graphs and Industrie 4.0",
        "orkg_properties": "['research problem', 'ontology component']",
        "nechakhin_result": "['knowledge graphs', 'Industrie 4.0']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "The industry 4.0 standards landscape from a semantic integration perspective",
        "abstract": ":Interoperability among actors, sensors, and heterogeneous systems is a crucial factor for realizing the Industry 4.0 vision, i.e., the creation of Smart Factories by enabling intelligent human-to-machine and machine-to-machine cooperation. In order to empower interoperability in Smart Factories, standards and reference architectures have been proposed. Standards allow for the description of components, systems, and processes, as well as interactions among them. Reference architectures classify, align, and integrate industrial standards according to their purposes and features. Industrial communities in Europe, the United States, and Asia have proposed various reference architectures. However, interoperability among analogous standards in these reference architectures is hampered due to different granularity representation of similar processes or production parts. In this paper, we survey the landscape of Industry 4.0 standards from a semantic perspective. To tackle the problem of interoperability between standards, we developed STO, an ontology for describing standards and their relations. Characteristics of I4.0 standards are described using STO, and these descriptions are exploited for classifying standards from different perspectives according to the reference architectures. Moreover, the semantics encoded in STO allows for the discovery of relations between I4.0 standards, and for mappings across reference architectures proposed by different industrial communities.",
        "dimensions": [
            [
                "knowledge graphs",
                "Industrie 4.0",
                "semantic web",
                "linked data",
                "industry automation",
                "data integration",
                "knowledge representation",
                "smart manufacturing"
            ],
            [
                "knowledge graphs",
                "Industrie 4.0",
                "data integration",
                "semantic web",
                "ontology",
                "linked data",
                "smart manufacturing",
                "cyber-physical systems"
            ],
            [
                "Topic",
                "Keywords",
                "Publication Date",
                "Authors",
                "Application Areas",
                "Methodology",
                "Industry Sector",
                "Impact Factor"
            ],
            [
                "Technology",
                "Applications",
                "Industry sectors",
                "Data integration",
                "Standards and protocols",
                "Case studies",
                "Challenges and opportunities"
            ],
            [
                "research problem",
                "Industry",
                "Technology",
                "Data sources",
                "Integration methods"
            ]
        ],
        "impr+abs": [
            "Interoperability",
            "Reference architectures",
            "Industrial standards",
            "Ontology",
            "Semantic integration",
            "Smart Factories",
            "Relations between standards",
            "Discovery of relations"
        ],
        "impr+120b": [
            "research problem",
            "domain",
            "application area",
            "technology",
            "data integration",
            "use case"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Artifact",
            "Domain",
            "Evaluation",
            "Contribution"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212229",
        "research_problem": "Exploring Role of Semantic IoT ",
        "orkg_properties": "['reasoning abilities', 'has number of modules', 'Ontology Evaluation', 'specific domain ', 'Formalization', 'research problem', 'Ontology name', 'Reused ontology', 'used methodology', 'copyright license']",
        "nechakhin_result": "['Internet of Things', 'Semantic web', 'Role-based access control', 'IoT applications', 'Semantic interoperability']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "ExtruOnt: An ontology for describing a\u00a0type\u00a0of manufacturing machine for\u00a0Industry\u00a04.0\u00a0systems",
        "abstract": "Semantically rich descriptions of manufacturing machines, offered in a machine-interpretable code, can provide interesting benefits in Industry 4.0 scenarios. However, the lack of that type of descriptions is evident. In this paper we present the development effort made to build an ontology, calledExtruOnt, for describing a type of manufacturing machine, more precisely, a type that performs an extrusion process (extruder). Although the scope of the ontology is restricted to a concrete domain, it could be used as a model for the development of other ontologies for describing manufacturing machines in Industry 4.0 scenarios.The terms of theExtruOntontology provide different types of information related with an extruder, which are reflected in distinct modules that constitute the ontology. Thus, it contains classes and properties for expressing descriptions about components of an extruder, spatial connections, features, and 3D representations of those components, and finally the sensors used to capture indicators about the performance of this type of machine. The ontology development process has been carried out in close collaboration with domain experts.",
        "dimensions": [
            [
                "Internet of Things (IoT)",
                "Semantic Web",
                "Semantic technologies",
                "Role-based access control",
                "Data integration",
                "Ontology",
                "Smart environments",
                "Semantic interoperability",
                "IoT applications"
            ],
            [
                "IoT devices",
                "Semantic web",
                "Data integration",
                "Interoperability",
                "Semantic modeling",
                "Knowledge representation",
                "Data security",
                "Data privacy"
            ],
            [
                "IoT Technology",
                "Semantic Web",
                "Applications and Use Cases",
                "Interoperability",
                "Security and Privacy",
                "Standardization and Frameworks",
                "Data Management and Analytics",
                "Machine Learning and AI"
            ],
            [
                "IoT Application Domain",
                "Semantic Technology",
                "IoT Data Integration",
                "Interoperability and Standardization",
                "Semantic Reasoning and Inference",
                "Security and Privacy",
                "Scalability and Performance",
                "Use Case Scenarios"
            ],
            [
                "IoT application",
                "Semantic technology",
                "Data integration",
                "Interoperability",
                "Security measures"
            ]
        ],
        "impr+abs": [
            "Ontology name",
            "Manufacturing process",
            "Ontology scope",
            "Information modules",
            "Collaboration type"
        ],
        "impr+120b": [
            "research problem",
            "Domain",
            "Technology",
            "Application area",
            "Methodology"
        ],
        "impr+120b+abs": [
            "research problem",
            "domain",
            "methodology",
            "collaboration",
            "information modules",
            "application scenario"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R192126",
        "research_problem": "Green supply chain",
        "orkg_properties": "['research problem', 'Resolution methods', 'Uncertainty', 'Objectif function']",
        "nechakhin_result": "['Sustainable practices', 'Environmental impact', 'Supply chain management', 'Carbon footprint', 'Renewable energy', 'Waste management', 'Logistics', 'Transparency', 'Corporate social responsibility', 'Circular economy', 'Green procurement']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Heuristic method for robust optimization model for green closed-loop supply chain network design of perishable goods",
        "abstract": "In the current study, a green closed-loop supply chain network design for perishable products is investigated under uncertain conditions. The demands, rate of return and the quality of returned products stand as an uncertain parameter. The considered chain, based on the study of a dairy company, is a multi-period and multi-product that comprises suppliers, manufacturers, warehouses, retailers and collection centers. A mixed-integer linear programming (MILP) model is projected to minimize the cost and environmental pollutant, simultaneously. Besides, an innovativeMILProbust model is developed for the problem under uncertainty. Due to the NP-hard nature of the problem, the research has developed an efficient heuristic, named YAG, to solve large-sized problems. Computational experiments conducted indicating that the YAG method has an average gap of less than 1.65 percent from the optimal solution within a reasonable time. Also, the YAG method finds the optimal solution in more than 34 percent of instances. The performance of the robust approach and theheuristic methodis examined in a realcase studyand a diverse range of problems. The results revealed that the robust model compared to thedeterministic modelhas better quality and seem quite more reliable. The effect of the product\u2019s lifetime, bi-objective modeling and environmental pollutant are considered throughout the study. The results indicate that the effects of products\u2019 lifetime and level of uncertainty vary for cost andenvironmental pollutionobjectives.\nMaximizing the value of resources and producing less waste are strategic decisions affecting sustainability and competitive advantage. Sustainable closed-loop supply chains (CLSCs) are designed to minimize waste by circling back (repairing, reselling, or dismantling for parts) previously discarded products into the value chain. This study presents a novel two-stage fuzzy supplier selection and order allocation model in a CLSC. In Stage 1, we use the fuzzy best-worst method (BWM) to select the most suitable suppliers according to economic, environmental, social, and circular criteria. In Stage 2, we use a multi-objective mixed-integer linear programming (MOMILP) model to design a multi-product, multi-period, CLSC network, and inventory-location-routing, vehicle scheduling, and quantity discounts considerations. In the proposed MOMILP, the total network costs, the undesired environmental effects, and the lost sales are minimized while job opportunities and sustainable supplier purchases are maximized. A fuzzy goal programming approach is proposed to transform the MOMILP into a single objective model. We present a case study to demonstrate the applicability of the proposed method in the garment manufacturing and distribution industry.\nThere is so much interest in online purchasing within supply chain networks nowadays. After expanding the internet access and services, customers\u2019 behavior has changed. Today, a customer\u2019s shopping manner usually begins with the internet search. With this approach, we face some new trends in this field, such as online-to-offline (O2O) commerce that aims to balance online and offline sales. Regarding the supply chain management, the O2O commerce can help the managers to conduct both online and offline businesses. The tire industry is one of the applications of the O2O approach, which also directly affects the supply chain network design (SCND). Therefore, this work for the first time proposes a dual-channel",
        "dimensions": [
            [
                "Sustainability",
                "Environmental impact",
                "Supply chain management",
                "Green logistics",
                "Carbon footprint",
                "Renewable energy",
                "Waste management",
                "Reverse logistics",
                "Corporate social responsibility",
                "Life cycle assessment"
            ],
            [
                "sustainability",
                "logistics",
                "environmental impact",
                "supplier management",
                "waste management",
                "renewable energy",
                "transportation",
                "packaging",
                "carbon footprint"
            ],
            [
                "Environmental Impact",
                "Sustainable Practices",
                "Corporate Social Responsibility (CSR)",
                "Green Procurement",
                "Regulatory Compliance",
                "Life Cycle Assessment",
                "Green Logistics",
                "Stakeholder Engagement"
            ],
            [
                "Environmental impact",
                "Supply chain management practices",
                "Corporate social responsibility (CSR)",
                "Green technology adoption",
                "Regulatory compliance",
                "Performance metrics",
                "Industry sector",
                "Collaboration and partnerships"
            ],
            [
                "research problem",
                "Sustainability factor",
                "Supply chain stage",
                "Environmental impact",
                "Regulatory compliance"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Uncertainty",
            "Optimization model",
            "Heuristic method",
            "Supply chain type",
            "Environmental impact",
            "Case study",
            "Trends in supply chain management"
        ],
        "impr+120b": [
            "research problem",
            "Sustainability factor",
            "Supply chain stage",
            "Methodology",
            "Evaluation metric",
            "Stakeholder group"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Optimization approach",
            "Uncertainty modeling",
            "Heuristic method",
            "Case study application",
            "Objectives",
            "Supply chain characteristics"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R206113",
        "research_problem": "Supply chain",
        "orkg_properties": "['research problem', 'Resolution methods', 'Uncertainty', 'Sustainability factor', 'Objectif function']",
        "nechakhin_result": "[\"Industry sector (e.g., manufacturing, retail, healthcare)\",\n\"Supply chain stages (e.g., procurement, production, distribution, logistics)\",\n\"Geographical location (e.g., global, regional, local)\",\n\"Supply chain network structure (e.g., centralized, decentralized, hybrid)\",\n\"Supply chain participants (e.g., suppliers, manufacturers, distributors)\",\n\"Supply chain processes (e.g., demand planning, inventory management, order fulfillment)\",\n\"Supply chain technologies (e.g., RFID, blockchain, AI)\",\n\"Supply chain performance metrics (e.g., cost, service level, sustainability)\",\n\"Supply chain risks and disruptions (e.g., natural disasters, global pandemics, cyber attacks)\"]",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Sustainable design of a closed-loop location-routing-inventory supply chain network under mixed uncertainty",
        "abstract": "Considering economic, environmental and social impacts, this paper presents a new sustainable closed-loop location-routing-inventory model under mixed uncertainty. The environmental impacts of CO2emissions, fuel consumption, wasted energy and the social impacts of created job opportunities and economic development are considered in this paper. The uncertain nature of the network is handled using a stochastic-possibilistic programming approach. Furthermore, for large-sized problems, a hybrid meta-heuristic algorithm and lower bounds are developed and discussed. Finally, a real case study is provided to demonstrate the applicability of the model in real-world applications, and several in-depth analyses are conducted to develop managerial implications.\nHighlights\u2022Designing a novel sustainable closed-loopsupply chainwith routing and inventory.\u2022Applying a stochastic-possibilistic programming method to cope with the uncertainty.\u2022Developing a new hybrid meta-heuristic algorithm to efficiently solve the problem.\u2022Applicability of the model is tested on a realcase study.",
        "dimensions": [
            [
                "Logistics",
                "Inventory management",
                "Supplier management",
                "Demand forecasting",
                "Supply chain optimization",
                "Transportation",
                "Warehousing",
                "Sustainability in supply chain",
                "Risk management in supply chain",
                "Information technology in supply chain"
            ],
            [
                "logistics",
                "inventory management",
                "transportation",
                "sourcing",
                "procurement",
                "supplier management",
                "demand forecasting",
                "warehouse management",
                "supply chain sustainability",
                "risk management"
            ],
            [
                "Industry",
                "Geographic Location",
                "Technology",
                "Sustainability",
                "Risk Management",
                "Collaboration and Partnerships",
                "Performance Metrics",
                "Outsourcing and Offshoring",
                "Regulatory Environment"
            ],
            [
                "Type of Supply Chain",
                "Supply Chain Management Strategies",
                "Technology and Innovation",
                "Risk Management in Supply Chain",
                "Sustainability and Ethical Practices",
                "Globalization and International Supply Chains",
                "Performance Metrics and KPIs",
                "Supply Chain Collaboration and Partnerships"
            ],
            [
                "research problem",
                "Resolution methods",
                "Uncertainty",
                "Sustainability factor",
                "Objectif function"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Sustainability factor",
            "Uncertainty",
            "Resolution methods",
            "Case study",
            "Analysis",
            "Applicability"
        ],
        "impr+120b": [
            "research problem",
            "Domain",
            "Methodology",
            "Data source",
            "Evaluation metric",
            "Sustainability aspect"
        ],
        "impr+120b+abs": [
            "research problem",
            "sustainability criteria",
            "uncertainty handling",
            "mathematical model",
            "solution methodology",
            "case study",
            "managerial implications"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R218454",
        "research_problem": "Green and Sustainable Supply chain",
        "orkg_properties": "['research problem', 'Resolution methods', 'Uncertainty', 'Sustainability factor', 'Objectif function', 'CO2 footprint process']",
        "nechakhin_result": "['Supply chain management', 'Sustainability', 'Environmental impact', 'Green practices', 'Carbon footprint', 'Renewable energy', 'Circular economy', 'Waste reduction', 'Social responsibility', 'Ethical sourcing']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "A robust-heuristic optimization approach to a green supply chain design with consideration of assorted vehicle types and carbon policies under uncertainty",
        "abstract": "Adoption of carbon regulation mechanisms facilitates an evolution toward green and sustainable supply chains followed by an increased complexity. Through the development and usage of a multi-choice goal programming model solved by an improved algorithm, this article investigates sustainability strategies for carbon regulations mechanisms. We first propose a sustainable logistics model that considers assorted vehicle types and gas emissions involved with product transportation. We then construct a bi-objective model that minimizes total cost as the first objective function and follows environmental considerations in the second one. With our novel robust-heuristic optimization approach, we seek to support the decision-makers in comparison and selection of carbon emission policies in supply chains in complex settings with assorted vehicle types, demand and economic uncertainty. We deploy our model in a case-study to evaluate and analyse two carbon reduction policies, i.e., carbon-tax and cap-and-trade policies. The results demonstrate that our robust-heuristic methodology can efficiently deal with demand and economic uncertainty, especially in large-scale problems. Our findings suggest that governmental incentives for a cap-and-trade policy would be more effective for supply chains in lowering pollution by investing in cleaner technologies and adopting greener practices.\nThe integration of sustainability issues into supply chain (SC) management has progressed remarkably, most of it focused on the areas of the green supply chain (GSC) and the sustainable supply chain (SSC) (Tang and Zhou2012; Golinska-Dawson et\u00a0al.2018; Heydari et\u00a0al.2020). Increasing concerns about the environmental impacts and international and government regulations have attracted research attention to the GSC problems beyond merely economic aspects (Ivanov et\u00a0al.2019). In an GSC, the environmental impacts from SCs need to be minimized complementing total cost minimization (Rezaee et\u00a0al.2017). Moreover, social aspects in SCs became a trend and lead to introducing the SSC network (Carter and Rogers2008; Pavlov et\u00a0al.2019). In general, when the financial, environmental and social impacts of the SC are considered simultaneously, the traditional SC shifts toward the SSC. The transition from the traditional goals of the SC to the new sustainable objectives is also identified as the company\u2019s competitive advantage (Dubey et\u00a0al.2015; Giannakis and Papadopoulos2016).Improvements in operating costs efficiency and service levels while paying special attention to the environmental, economic, and social considerations in the SC belong to major requirements to succeed in highly competitive markets (Golinska-Dawson et\u00a0al.2018; Brandenburg et\u00a0al.2019). Due to environmental pollution and increased global warming, government and international bodies have introduced laws obliging companies to address environmental issues. One of the most important parts of new regulations is reducing carbon emissions/footprint that improves the business\u2019s environmental performance (Golinska and Romano2012). In an SC, this will bring the integrity of all parts of the SC in social commitments. A carbon footprint reduction project is therefore of a global economic importance (Fahimnia and Jabbarzadeh2016).A recent European Commission report illustrates that the amount of transport gas emissions has been continually increasing, and if no action is taken, transport emissions could make up more than 30% of tota",
        "dimensions": [
            [
                "Environmental impact",
                "Logistics",
                "Sustainability",
                "Supply chain management",
                "Green practices",
                "Renewable energy",
                "Carbon footprint",
                "Waste management",
                "Reverse logistics",
                "Corporate social responsibility"
            ],
            [
                "environmental impact",
                "sourcing",
                "transportation",
                "packaging",
                "renewable energy",
                "waste management",
                "ethical labor practices",
                "sustainability certifications"
            ],
            [
                "Environmental Impact",
                "Social Responsibility",
                "Economic Viability",
                "Innovation and Technology",
                "Stakeholder Collaboration",
                "Regulatory Compliance",
                "Risk Management"
            ],
            [
                "Type of supply chain",
                "Environmental impact assessment",
                "Green procurement practices",
                "Carbon footprint reduction",
                "Renewable energy integration",
                "Sustainable logistics and transportation",
                "Circular economy principles",
                "Social responsibility in supply chain"
            ],
            [
                "research problem",
                "Sustainability factor",
                "Supply chain management",
                "Environmental impact",
                "Regulatory compliance"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Uncertainty",
            "Sustainability factor",
            "Optimization approach",
            "Policy analysis",
            "Environmental impact",
            "Supply chain type",
            "Regulation compliance"
        ],
        "impr+120b": [
            "research problem",
            "Sustainability dimension",
            "Supply chain scope",
            "Evaluation metric",
            "Implementation approach"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Mathematical model",
            "Objective functions",
            "Optimization technique",
            "Uncertainty handling",
            "Policy scenarios",
            "Case study"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212685",
        "research_problem": "effect of legume intercrops on soil nitrogen content",
        "orkg_properties": "['research problem', 'study location (country)', 'Control', 'Cropping System', 'Control Result Nutrients', 'Treatment Result Nutrients', 'Type of Soil Nutrient', 'Legume Treatment', 'Experimental Design', 'Experimental Setup', 'Planting design']",
        "nechakhin_result": "['legume species', 'intercrop management practices', 'soil type', 'soil nutrient composition', 'crop rotation', 'legume biomass production', 'legume nitrogen fixation', 'soil sampling frequency', 'nutrient management', 'soil nitrogen content measurement method']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Energizing marginal soils \u2013 The establishment of the energy crop Sida hermaphrodita as dependent on digestate fertilization, NPK, and legume intercropping",
        "abstract": "Growing energy crops in marginal, nutrient-deficient soils is a more sustainable alternative to conventional cultivation. The use of energy-intensive synthetic fertilizers needs to be reduced, preferably via closed nutrient loops in thebiomass productioncycle. In the present study based on the firstgrowing seasonof a mesocosm experiment using large bins outdoors, we evaluated the potential of the energy plantSida hermaphroditato grow in a marginalsandy soil. We applied different fertilization treatments using eitherdigestatefrombiogasproduction or a commercial mineral NPK-fertilizer. To further increase independence from synthetically produced N-fertilizers, the legume plantMedicago sativawas intercropped to introduce atmospherically fixed nitrogen and potentially facilitate the production of additionalS.hermaphroditabiomass. We found digestate to be the best performing fertilizer because it produced similar yields as the NPK fertilization but minimized nitrate leaching. Legume intercropping increased the total biomass yield by more than 100% compared toS.hermaphroditasingle cropping in the fertilized variants. However, it negatively influenced the performance ofS.hermaphroditain the following year. We conclude that a successful establishment ofS.hermaphroditafor biomass production in marginal soils is possible and digestate application formed the best fertilization method when considering a range of aspects including overall yield, nitrate leaching,nitrogen fixationofM.sativa, and sustainability over time.\nPerennial crops, as energy feedstocks, offer ecological advantages over fossil fuels by contributing to the reduction of greenhouse gases and fossil energy savings. Yet, the intensity of agricultural production may increase the pressure on soil, water resources and on biological and landscape diversity. Moreover, land use competition with food crops is demanding a spatial segregation of energy producing areas to land currently marginal for agricultural production. Therefore, the objective of this work was to determine the local and site-specific environmental impacts associated with the cultivation of perennial crops in marginal soils. The study, supported by the European Union (project OPTIMA - Optimization of Perennial Grasses for Biomass Production), was developed and applied to the cultivation phase of several perennial crops, in marginal soils of the Mediterranean region, using environmental impact assessment (EIA) protocols. Investigated crops includeMiscanthus(Miscanthus\u00d7giganteusGreef et Deu), giant reed (Arundo donaxL.), switchgrass (Panicum virgatumL.) and cardoon (Cynara cardunculusL.). Different categories were studied: fertilizers and pesticides related emissions, impact on soil and water resources and biological and landscape diversity. Results suggest that growing perennial crops in marginal Mediterranean soils do not inflict a higher impact to the environment than wheat farming (the current land use). At a scale from 0 (lower impact) to 10 (higher impact), against idle land (the reference system with a score of 5), wheat and giant reed showed the highest scores (6.7\u20137.3 and 6.7\u20137.1, respectively). Impact scores of the remaining perennials decreased in the order cardoon (5.7\u20136.0), Miscanthus (5.4\u20135.6), and switchgrass (5.2\u20135.5), the last one showing the lowest difference to the reference system. Overall results suggest that perennial crops provide benefits regarding soil properties and erodibility (with an average scor",
        "dimensions": [
            [
                "legume intercrops",
                "soil nitrogen content",
                "agricultural practices",
                "crop rotation",
                "soil fertility",
                "nitrogen fixation",
                "sustainable agriculture"
            ],
            [
                "legume species",
                "intercropping system",
                "soil nitrogen content measurement method",
                "soil type",
                "climate",
                "fertilization",
                "crop rotation",
                "planting density",
                "harvesting method"
            ],
            [
                "Crop Type",
                "Soil Type",
                "Nitrogen Content Measurement Method",
                "Intercropping Practices",
                "Geographic Location"
            ],
            [
                "Type of legume",
                "Intercropping system",
                "Soil type",
                "Climate and location",
                "Duration of intercropping",
                "Soil management practices",
                "Crop rotation history",
                "Nitrogen content measurement method"
            ],
            [
                "Intercropping method",
                "Soil nutrient",
                "Experimental design",
                "Data collection method"
            ]
        ],
        "impr+abs": [
            "Energy crop",
            "Fertilization method",
            "Intercropping",
            "Environmental impact assessment",
            "Biomass yield",
            "Nitrate leaching",
            "Nitrogen fixation",
            "Sustainability"
        ],
        "impr+120b": [
            "research problem",
            "intervention type",
            "target variable",
            "measurement method",
            "study ecosystem",
            "outcome metric"
        ],
        "impr+120b+abs": [
            "Study objective",
            "Experimental design",
            "Fertilization treatment",
            "Intercropping strategy",
            "Crop species",
            "Environmental impact categories",
            "Funding source"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R178407",
        "research_problem": "CAN BUS Intrusion Detection",
        "orkg_properties": "['Algorithm name', 'dataset', 'research problem', 'result']",
        "nechakhin_result": "['network security', 'intrusion detection system', 'CAN bus', 'automotive cybersecurity', 'anomaly detection', 'machine learning', 'data analysis']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "WINDS: A Wavelet-Based Intrusion Detection System for Controller Area Network (CAN)",
        "abstract": ":Vehicles are equipped with Electronic Control Units (ECUs) to increase their overall system functionality and connectivity. However, the rising connectivity exposes a defenseless internal Controller Area Network (CAN) to cyberattacks. An Intrusion Detection System (IDS) is a supervisory module, proposed for identifying CAN network malicious messages, without modifying legacy ECUs and causing high traffic overhead. The traditional IDS approaches rely on time and frequency thresholding, leading to high false alarm rates, whereas state-of-the-art solutions may suffer from vehicle dependency. This paper presents a wavelet-based approach to locating the behavior change in the CAN traffic by analyzing the CAN network's transmission pattern. The proposed Wavelet-based Intrusion Detection System (WINDS) is tested on various attack scenarios, using real vehicle traffic from two independent research centers, while being expanded toward more comprehensive attack scenarios using synthetic attacks. The technique is evaluated and compared against the state-of-the-art solutions and the baseline frequency method. Experimental results show that WINDS offers a vehicle-independent solution applicable for various vehicles through a unique approach while generating low false alarms.The flowchart of wavelet-based intrusion detection system for in-vehicle communication.",
        "dimensions": [
            [
                "Intrusion detection",
                "Controller Area Network (CAN) bus",
                "Automotive security",
                "Cyber-physical systems",
                "Anomaly detection",
                "Machine learning",
                "Network security"
            ],
            [
                "CAN bus protocol version",
                "Intrusion detection method",
                "Intrusion detection system architecture",
                "Type of attacks detected",
                "Detection performance metrics",
                "Hardware platform",
                "Software platform",
                "Dataset used for evaluation"
            ],
            [
                "Technology",
                "Intrusion Detection Techniques",
                "Security Threats",
                "Data Sources",
                "Evaluation Metrics",
                "Application Context",
                "Research Methodology"
            ],
            [
                "Type of Intrusion Detection System",
                "CAN Bus Protocol Version",
                "Machine Learning Techniques",
                "Attack Scenarios",
                "Hardware Implementation",
                "Dataset Used for Evaluation",
                "Performance Metrics",
                "Industry Application"
            ],
            [
                "Intrusion detection method",
                "Communication protocol",
                "Security measures",
                "Data analysis technique"
            ]
        ],
        "impr+abs": [
            "Intrusion Detection System",
            "Controller Area Network (CAN)",
            "Attack scenarios",
            "Evaluation method",
            "Vehicle dependency"
        ],
        "impr+120b": [
            "research problem",
            "target system",
            "Detection technique",
            "Data source",
            "Evaluation metric",
            "Threat model"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Data used",
            "Evaluation metric",
            "Comparison baseline",
            "Application domain",
            "System type"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R191545",
        "research_problem": "Fake Account Detection",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['Social media platform',\n 'User behavior',\n 'User profile',\n 'Account activity',\n 'Content analysis',\n 'Network analysis',\n 'Machine learning algorithms',\n 'Data mining techniques',\n 'Text classification',\n 'Image analysis',\n 'User engagement',\n 'Temporal patterns']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Towards fast and lightweight spam account detection in mobile social networks through fog computing",
        "abstract": "Now, mobile devices play an increasingly important role in social networks by sharing information quickly, such as mobile phones and wearable health surveillance devices. Mobile social networks are vulnerable to spammers because of the fragile security policies of mobile operating systems. Especially, social networks on mobile devices face many difficulties in defending against spammers due to their low computing power, poor network quality and long response time. Since graph-based algorithms require huge computing power, machine-learning classifiers require very short response time, and existing PC-based research is not suitable for mobile devices, we need a lightweight and fast response method for mobile devices to detect spammers in mobile social networks. Regarded as the extension of cloud computing, fog computing puts the data, data processing and applications in the devices that are at the edge of the Internet (without storing all of them in the cloud), which leads to a better real-time performance, adapts to the wide geographical distribution and the high mobility of mobile devices. In this paper, we propose COLOR + , a method based on fog computing that performs most computations at terminal (mobile devices). It only uses the interaction between the account and its neighbors, which makes it easy to store and calculate a local graph on a mobile device. Each interaction value can be applied to any request. COLOR + detects spammers based on a threshold of the suspicion degree. We collect 50 million normal accounts and about 40,000 spammers from Twitter. Experiments show that the accuracy of COLOR + is about 85.95%, whose average time to detect an account is 0.01s. Therefore, COLOR + is an effective detection method that can be quickly applied.\nBecause of the limitation of actual conditions, only a few part of data can be crawled from the whole Twitter dataset by us. At the same time, the Twitter dataset is downloaded from existing researches. Since spam accounts keep changing, the timeliness of data should be taken into consideration. To achieve a detection algorithm with better performance, we need to improve the algorithm on the latest dataset. However, it is still a big challenge for researchers to collect numerous data from MSNs because of the privacy protection strategy, especially real-time MSN datasets without any mistake or loss.In addition, the spam accounts analyzed and evaluated by us are still difficult to cover the typical types of MSNs, while there are still some accounts that are labeled normal by our algorithm sending spam information. As we cannot obtain the detailed information of each account accurately, there may be some mistakes in the recognition work. The wrongly labeled sample may lead to some influence on our algorithm. From another point of view, important spam accounts in the Twitter dataset come from the existing researches, and there may be different opinions about spammers. Many fans of public accounts may exceed the limit of our algorithms\u2019 calculation and judgment.As we can see from the evaluation, we have not applied this detection system to the existing MSNs. Although the algorithms\u2019 performance on the dataset is favorable, we\u2019ve no idea about its real performance on the MSN server. Furthermore, the algorithm has not been compared with other existing work yet. The characteristic-based machine learning algorithm is mostly adopted in existing work. Since characteristics and program details cannot be ",
        "dimensions": [
            [
                "Social media platform",
                "Machine learning",
                "Natural language processing",
                "Data mining",
                "User behavior",
                "Fraud detection",
                "Cybersecurity",
                "Pattern recognition",
                "Anomaly detection"
            ],
            [
                "social media platform",
                "user activity",
                "machine learning algorithm",
                "data sources",
                "feature selection",
                "labeling technique",
                "evaluation metrics"
            ],
            [
                "Text analysis",
                "Social network analysis",
                "Machine learning algorithms",
                "Data sources",
                "Feature engineering",
                "Evaluation metrics",
                "Ethical considerations"
            ],
            [
                "Type of data",
                "Machine learning algorithms",
                "Feature selection",
                "Evaluation metrics",
                "Dataset source",
                "Social media platform",
                "Publication date"
            ],
            [
                "User behavior",
                "Data sources",
                "Machine learning model",
                "Evaluation metrics",
                "Feature extraction",
                "Training data",
                "Validation method"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Computing method",
            "Evaluation metric",
            "Data source"
        ],
        "impr+120b": [
            "research problem",
            "Detection technique",
            "Data source",
            "Feature set",
            "Evaluation metric",
            "Application domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Data source",
            "Evaluation metric",
            "Deployment environment",
            "Limitations"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R193190",
        "research_problem": "Social Netwrok Security and Privacy",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['Security measures', 'Privacy protection', 'User authentication', 'Data encryption', 'Anonymity', 'Policy enforcement', 'Data access control', 'Privacy policies', 'Data sharing', 'User consent', 'Data breaches', 'Trust and reputation systems']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Twitter fake account detection",
        "abstract": ":Social networking sites such as Twitter and Facebook attracts millions of users across the world and their interaction with social networking has affected their life. Thi...Show MoreMetadata",
        "dimensions": [
            [
                "Cybersecurity",
                "Privacy protection",
                "Social network analysis",
                "Data protection",
                "Information security",
                "Online social networks",
                "User privacy",
                "Network security",
                "Data privacy",
                "Social media security"
            ],
            [
                "type of social network",
                "security measures",
                "privacy settings",
                "data encryption",
                "user authentication",
                "data collection",
                "data sharing policies",
                "user consent",
                "third-party access"
            ],
            [
                "Topic",
                "Publication Date",
                "Keywords",
                "Authors",
                "Type of Publication",
                "Social Network Platforms",
                "Regulatory Framework",
                "Methodology"
            ],
            [
                "Type of Social Network Platform",
                "Security Measures",
                "Privacy Policies",
                "User Behavior",
                "Data Protection Regulations",
                "Privacy-Preserving Technologies",
                "Threats and Attacks",
                "Ethical Considerations"
            ],
            [
                "research problem",
                "Security measures",
                "Privacy concerns",
                "Data protection",
                "User authentication"
            ]
        ],
        "impr+abs": [
            "Social media platform",
            "User interaction",
            "Fake account detection",
            "Impact assessment"
        ],
        "impr+120b": [
            "research problem",
            "domain",
            "security aspect",
            "privacy aspect",
            "threat model",
            "mitigation techniques"
        ],
        "impr+120b+abs": [
            "research problem",
            "Data source",
            "Methodology",
            "Features used",
            "Model type",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R191183",
        "research_problem": "Misinformation & Fake News",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['Topic',\n 'Author',\n 'Publication Date',\n 'Keywords',\n 'Language',\n 'Source reliability',\n 'Credibility',\n 'Fact-checking',\n 'Source type (e.g., news website, social media)',\n 'Geographical location',\n 'Demographic targeting',\n 'Level of sensationalism',\n 'Level of bias',\n 'Level of trustworthiness']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "SAFE: Similarity-Aware Multi-modal Fake News Detection",
        "abstract": "Effective detection of fake news has recently attracted significant attention. Current studies have made significant contributions to predicting fake news with less focus on exploiting the relationship (similarity) between the textual and visual information in news articles. Attaching importance to such similarity helps identify fake news stories that, for example, attempt to use irrelevant images to attract readers\u2019 attention. In this work, we propose a\\(\\mathsf {S}\\)imilarity-\\(\\mathsf {A}\\)ware\\(\\mathsf {F}\\)ak\\(\\mathsf {E}\\)news detection method (\\(\\mathsf {SAFE}\\)) which investigates multi-modal (textual and visual) information of news articles. First, neural networks are adopted to separately extract textual and visual features for news representation. We further investigate the relationship between the extracted features across modalities. Such representations of news textual and visual information along with their relationship are jointly learned and used to predict fake news. The proposed method facilitates recognizing the falsity of news articles based on their text, images, or their \u201cmismatches.\u201d We conduct extensive experiments on large-scale real-world data, which demonstrate the effectiveness of the proposed method.\nCastillo, C., Mendoza, M., Poblete, B.: Information credibility on Twitter. In: The World Wide Web Conference, pp. 675\u2013684. ACM (2011)",
        "dimensions": [
            [
                "Social media platforms",
                "News sources",
                "Information credibility",
                "Fact-checking",
                "Dissemination of information",
                "Media literacy",
                "Regulation and policy",
                "Psychological impact",
                "Technology and algorithms",
                "Public opinion"
            ],
            [
                "type of misinformation",
                "platform",
                "spread mechanism",
                "impact",
                "detection method",
                "prevention method",
                "audience",
                "regulation"
            ],
            [
                "Topic",
                "Publication Date",
                "Methodology",
                "Geographic Focus",
                "Interdisciplinary Approach",
                "Impact Assessment",
                "Source Credibility",
                "Stakeholder Analysis"
            ],
            [
                "Type of misinformation",
                "Platform or medium",
                "Target audience",
                "Fact-checking methods",
                "Impact and consequences",
                "Geographic scope",
                "Technology and tools",
                "Regulatory and policy responses"
            ],
            [
                "Misinformation type",
                "Spread mechanism",
                "Detection method",
                "Impact assessment"
            ]
        ],
        "impr+abs": [
            "Fake news detection",
            "Multi-modal information",
            "Neural network",
            "Relationship between modalities",
            "Experimental validation"
        ],
        "impr+120b": [
            "research problem",
            "data source",
            "detection method",
            "evaluation metric",
            "mitigation approach",
            "application domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Data modality",
            "Feature extraction technique",
            "Similarity measure",
            "Dataset",
            "Evaluation"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R212763",
        "research_problem": "COVID-19 Fake News Detection",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['Topic',\n 'Keywords',\n 'Author',\n 'Publication Date',\n 'Journal',\n 'Abstract',\n 'Introduction',\n 'Methodology',\n 'Dataset',\n 'Results',\n 'Discussion',\n 'Conclusion',\n 'References',\n 'Citation',\n 'Funding']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "COVID-19 Fake News Detection by Using BERT and RoBERTa models",
        "abstract": ":We live in a world where COVID-19 news is an everyday occurrence with which we interact. We are receiving that information, either consciously or unconsciously, without fact-checking it. In this regard, it has become an enormous challenge to keep only true COVID-19 news relevant. People are exposed to these stories on a daily basis, and not all of them are true and fact-checked reports on the COVID-19 pandemic, which was the primary reason for our research. We accepted the challenge that fake news is extremely common and that some people take these news as they are. Knowing the true power of the most recent NLP achievements, in this research we focus on detecting fake news regarding COVID-19. Our approach includes using pre-trained BERT and RoBERTa models, which we then fine-tune on real and fake news about the COVID-19 pandemic. By using pre-trained BERT and RoBERTa models on tweet data, we explore their capabilities and compare them to previous research in regard to fine-tuned BERT models for this task in which we achieve better accuracy, recall and f1 score.",
        "dimensions": [
            [
                "COVID-19",
                "Fake news",
                "Detection",
                "Misinformation",
                "Social media",
                "Information credibility",
                "Machine learning",
                "Natural language processing",
                "Data mining",
                "Text analysis"
            ],
            [
                "type of fake news",
                "dataset used",
                "feature extraction method",
                "classification algorithm",
                "evaluation metrics",
                "publication date"
            ],
            [
                "Topic",
                "Keywords",
                "Methodology",
                "Publication Date",
                "Source Credibility",
                "Geographic Focus",
                "Data Sources"
            ],
            [
                "Type of fake news",
                "Data sources",
                "Detection techniques",
                "Features used for detection",
                "Evaluation metrics",
                "Dataset used",
                "Ethical considerations",
                "Application domains"
            ],
            [
                "research problem",
                "Data sources",
                "Feature extraction",
                "Model selection",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "NLP model used",
            "Evaluation metric",
            "Data source"
        ],
        "impr+120b": [
            "research problem",
            "target domain",
            "data source",
            "methodology",
            "evaluation metric",
            "language"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Domain",
            "Data used",
            "Methodology",
            "Models",
            "Evaluation metric",
            "Comparison"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R221022",
        "research_problem": "Fake News Detection",
        "orkg_properties": "['method', 'research problem', 'Has Datasets', 'result']",
        "nechakhin_result": "['News source reliability', 'Content analysis', 'Social media analysis', 'Fact-checking', 'Text analysis', 'NLP techniques', 'Machine learning algorithms', 'Data sources', 'Language', 'Geographic location', 'Publication date']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Fake news detection using Deep Learning",
        "abstract": ":The evolution of the information and communication technologies has dramatically increased the number of people with access to the Internet, which has changed the way the information is consumed. As a consequence of the above, fake news have become one of the major concerns because its potential to destabilize governments, which makes them a potential danger to modern society. An example of this can be found in the US. electoral campaign, where the term \"fake news\" gained great notoriety due to the influence of the hoaxes in the final result of these. In this work the feasibility of applying deep learning techniques to discriminate fake news on the Internet using only their text is studied. In order to accomplish that, three different neural network architectures are proposed, one of them based on BERT, a modern language model created by Google which achieves state-of-the-art results.",
        "dimensions": [
            [
                "Source of information",
                "Credibility of sources",
                "Language and writing style",
                "Use of images and videos",
                "Social media presence",
                "Fact-checking methods",
                "Machine learning algorithms",
                "Natural language processing techniques",
                "Ethical considerations"
            ],
            [
                "news source",
                "content",
                "language",
                "date published",
                "fact-checking method",
                "classification algorithm",
                "training dataset",
                "evaluation metrics"
            ],
            [
                "Textual Similarity",
                "Publication Date",
                "Methodology",
                "Dataset",
                "Author Affiliation",
                "Citation Network",
                "Evaluation Metrics",
                "Application Domain"
            ],
            [
                "Type of fake news",
                "Data sources",
                "Detection techniques",
                "Evaluation metrics",
                "Application domains",
                "Dataset size",
                "Geographical focus"
            ],
            [
                "Text analysis",
                "Data sources",
                "Feature extraction",
                "Evaluation metric",
                "Machine learning model",
                "Training dataset",
                "Validation method"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Deep learning technique",
            "Neural network architecture",
            "Language model"
        ],
        "impr+120b": [
            "research problem",
            "Data source",
            "Methodology",
            "Evaluation metric",
            "Features used",
            "Domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Model architecture",
            "Data type",
            "Evaluation approach",
            "Technology used"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R178376",
        "research_problem": "What is the correlation between SARS-CoV-2 viral load and disease severity",
        "orkg_properties": "['material', 'method', 'Study type', 'research problem', 'Statistical tests', 'Location', 'Time period for data collection', 'patient age', 'result']",
        "nechakhin_result": "['Viral load',\n 'Disease severity',\n 'Correlation',\n 'SARS-CoV-2']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "SARS-CoV-2 viral load as a predictor for disease severity in outpatients and hospitalised patients with COVID-19: A prospective cohort study",
        "abstract": "IntroductionWe aimed to examine if severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) polymerase chain reaction (PCR) cycle quantification (Cq) value, as a surrogate for SARS-CoV-2 viral load, could predict hospitalisation and disease severity in adult patients with coronavirus disease 2019 (COVID-19).MethodsWe performed a prospective cohort study of adult patients with PCR positive SARS-CoV-2 airway samples including all out-patients registered at the Department of Infectious Diseases, Odense University Hospital (OUH) March 9-March 17 2020, and all hospitalised patients at OUH March 10-April 21 2020. To identify associations between Cq-values and a) hospital admission and b) a severe outcome, logistic regression analyses were used to compute odds ratios (OR) and 95% Confidence Intervals (CI), adjusting for confounding factors (aOR).ResultsWe included 87 non-hospitalised and 82 hospitalised patients. The median baseline Cq-value was 25.5 (interquartile range 22.3\u201329.0). We found a significant association between increasing Cq-value and hospital-admission in univariate analysis (OR 1.11, 95% CI 1.04\u20131.19). However, this was due to an association between time from symptom onset to testing and Cq-values, and no association was found in the adjusted analysis (aOR 1.08, 95% CI 0.94\u20131.23). In hospitalised patients, a significant association between lower Cq-values and higher risk of severe disease was found (aOR 0.89, 95% CI 0.81\u20130.98), independent of timing of testing.ConclusionsSARS-CoV-2 PCR Cq-values in outpatients correlated with time after symptom onset, but was not a predictor of hospitalisation. However, in hospitalised patients lower Cq-values were associated with higher risk of severe disease.",
        "dimensions": [
            [
                "SARS-CoV-2",
                "viral load",
                "disease severity",
                "correlation",
                "COVID-19"
            ],
            [
                "viral load",
                "disease severity",
                "correlation",
                "patient population",
                "sampling method",
                "measurement technique",
                "time of measurement",
                "clinical outcomes"
            ],
            [
                "Viral Load",
                "Disease Severity",
                "Patient Demographics",
                "Time Course of Infection",
                "Viral Variants",
                "Treatment Interventions",
                "Laboratory Methods"
            ],
            [
                "Virus",
                "Patient characteristics",
                "Viral load measurement method",
                "Disease severity criteria",
                "Time course",
                "Treatment status",
                "Viral shedding",
                "Host immune response"
            ],
            [
                "Correlation",
                "Viral load measurement",
                "Disease severity assessment",
                "Statistical analysis",
                "Clinical data collection"
            ]
        ],
        "impr+abs": [
            "Study location",
            "Methodology",
            "Data source",
            "Analysis",
            "Outcome measure"
        ],
        "impr+120b": [
            "Research problem",
            "Independent variable",
            "Dependent variable",
            "Measurement method",
            "Study population",
            "Data source",
            "Statistical analysis"
        ],
        "impr+120b+abs": [
            "Study design",
            "Study population",
            "Predictor variable",
            "Outcome variable",
            "Statistical analysis",
            "Setting",
            "Study period"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R199183",
        "research_problem": "Can animal noroviruses infect humans?",
        "orkg_properties": "['material', 'method', 'research problem', 'Location', 'type study', 'Norovirus', 'antigen used', 'immunoglobulin class']",
        "nechakhin_result": "['Animal species', 'Viral strains', 'Host range', 'Transmission methods', 'Pathogenicity', 'Clinical symptoms', 'Human susceptibility', 'Animal model studies', 'Genetic similarity', 'Host cell receptor recognition', 'Immune response']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Exposure to Human and Bovine Noroviruses in a Birth Cohort in Southern India from 2002 to 2006",
        "abstract": "ABSTRACTHuman and bovine norovirus virus-like particles were used to evaluate antibodies in Indian children at ages 6 and 36 months and their mothers. Antibodies to genogroup II viruses were acquired early and were more prevalent than antibodies to genogroup I. Low levels of IgG antibodies against bovine noroviruses indicate possible zoonotic transmission.",
        "dimensions": [
            [
                "Virus type",
                "Host species",
                "Transmission",
                "Genetic similarity",
                "Clinical symptoms",
                "Epidemiology",
                "Zoonotic potential"
            ],
            [
                "virus type",
                "host range",
                "transmission route",
                "symptoms",
                "prevention measures",
                "vaccine development"
            ],
            [
                "Virus type",
                "Transmission route",
                "Host range",
                "Zoonotic potential",
                "Epidemiological evidence",
                "Immune response",
                "Public health implications"
            ],
            [
                "Virus type",
                "Transmission route",
                "Genetic similarity",
                "Host susceptibility",
                "Epidemiological evidence"
            ],
            [
                "Research problem",
                "Virus type",
                "Host specificity",
                "Transmission route",
                "Infection symptoms"
            ]
        ],
        "impr+abs": [
            "Virus type",
            "Antibody acquisition",
            "Age group",
            "Transmission type"
        ],
        "impr+120b": [
            "research problem",
            "Pathogen type",
            "Animal host",
            "Human susceptibility",
            "Public health relevance"
        ],
        "impr+120b+abs": [
            "Study location",
            "Study period",
            "Cohort age",
            "Pathogen",
            "Serological assay",
            "Zoonotic transmission assessment",
            "Antibody type"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R200000",
        "research_problem": "Do human noroviruses infect animals",
        "orkg_properties": "['material', 'method', 'Symptoms and signs', 'research problem', 'Species ', 'Location', 'antigen used', 'immunoglobulin class', 'primers', 'noroviruses found', 'norovirus genotype']",
        "nechakhin_result": "['viral infectivity', 'host range', 'cross-species transmission', 'zoonotic potential']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Seroprevalence for norovirus genogroup II, IV and VI in dogs",
        "abstract": "Molecular and serological data suggest that noroviruses (NoVs) might be transmitted between humans and domestic carnivores. In this study we screened an age-stratified collection of canine sera (n=516) by using an ELISA assay based on virus-like particles (VLPs) of human NoVs GII.4 and GIV.1 and carnivore NoVs GIV.2 and GVI.2. Antibodies against GII.4 and GIV.1 human NoVs and GIV.2 and GVI.2 NoVs from carnivores were identified in dog sera (13.0%, 67/516) suggesting their exposure to homologous and heterologous NoVs. Analysis of the trends of age-class prevalence showed a gradual increase in the positive rate from 9.0% and 7.0%, in young dogs <1year of age to 15.0% in dogs older than 12 years, for GII.4 and GVI.2 NoVs, respectively. A significant difference in the IgG distribution by age classes was observed for GIV.1 NoVs, with the highest rate of antibodies (7.0%) in the age group <1year and the lowest (1.0%) in the age-classes 7\u20139 (P=0.049). High correlation between the reactivity to GII.4 and GVI.2 NoVs was observed, likely due to conserved epitopes in the capsid structure.\nViral metagenomics is slowly taking over the traditional and widely used molecular techniques for the investigation of pathogenic viruses responsible for illness and inflicting great economic burden on the farm animal industry. Owing to the continued improvements in sequencing technologies and the dramatic reduction of per base costs of sequencing the use of next generation sequencing have been key factors in this progress. Discoveries linked to viral metagenomics are expected to be beneficial to the field of veterinary medicine starting from the development of better diagnostic assays to the design of new subunit vaccines with minimal investments. With these achievements the research has taken a giant leap even toward the better healthcare of animals and, as a result, the animal sector could be growing at an unprecedented pace.\n",
        "dimensions": [
            [
                "Virus type",
                "Host specificity",
                "Transmission",
                "Genetic similarity",
                "Epidemiology",
                "Pathogenesis",
                "Immune response",
                "Animal models"
            ],
            [
                "virus type",
                "host range",
                "infection",
                "transmission",
                "zoonotic potential"
            ],
            [
                "Virus specificity",
                "Animal models",
                "Zoonotic potential",
                "Epidemiological studies",
                "Virology and pathogenesis"
            ],
            [
                "Virus type",
                "Host range",
                "Transmission pathways",
                "Disease outcomes",
                "Epidemiology",
                "Genetic analysis",
                "Intervention strategies"
            ],
            [
                "Research problem",
                "Infectious agent",
                "Host specificity",
                "Transmission route"
            ]
        ],
        "impr+abs": [
            "Virus type",
            "Seroprevalence",
            "Age-related trends",
            "Viral metagenomics",
            "Sequencing technology",
            "Veterinary medicine impact",
            "Diagnostic assays",
            "Vaccine development"
        ],
        "impr+120b": [
            "Research problem",
            "Virus type",
            "Target host",
            "Infection assessment method",
            "Study scope"
        ],
        "impr+120b+abs": [
            "Study species",
            "Virus genogroups investigated",
            "Sample size",
            "Serological assay type",
            "Age stratification",
            "Key findings",
            "Implications for veterinary diagnostics"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R196928",
        "research_problem": "What is the tropism of human norovirus",
        "orkg_properties": "['material', 'method', 'Symptoms and signs', 'research problem', 'Species ', 'Source', 'patient age', 'Result: Cell tropism', 'Norovirus', 'route of infection']",
        "nechakhin_result": "['Tropism of human norovirus']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 2,
        "title": "Detection of human norovirus in intestinal biopsies from immunocompromised transplant patients",
        "abstract": "Human noroviruses (HuNoVs) can often cause chronic infections in solid organ and haematopoietic stem cell transplant (HSCT) patients. Based on histopathological changes observed during HuNoV infections, the intestine is the presumed site of virus replication in patients; however, the cell types infected by HuNoVs remain unknown. The objective of this study was to characterize histopathological changes during HuNoV infection and to determine the cell types that may be permissive for HuNoV replication in transplant patients. We analysed biopsies from HuNoV-infected and non-infected (control) transplant patients to assess histopathological changes in conjunction with detection of HuNoV antigens to identify the infected cell types. HuNoV infection in immunocompromised patients was associated with histopathological changes such as disorganization and flattening of the intestinal epithelium. The HuNoV major capsid protein, VP1, was detected in all segments of the small intestine, in areas of biopsies that showed histopathological changes. Specifically, VP1 was detected in enterocytes, macrophages, T cells and dendritic cells. HuNoV replication was investigated by detecting the non-structural proteins, RdRp and VPg. We detected RdRp and VPg along with VP1 in duodenal and jejunal enterocytes. These results provide critical insights into histological changes due to HuNoV infection in immunocompromised patients and propose human enterocytes as a physiologically relevant cell type for HuNoV cultivation.Received:22/06/2016Accepted:08/07/2016Published Online:01/09/2016Keyword(s):enterocytes,immunocompromised patients,intestinal biopsy,norovirus replicationandtransplant patients\u00a9 Microbiology Society\n",
        "dimensions": [
            [
                "Human norovirus",
                "Tropism",
                "Viral tropism",
                "Host range",
                "Cellular receptors",
                "Infection mechanism",
                "Viral replication",
                "Pathogenesis",
                "Clinical symptoms",
                "Epidemiology",
                "Transmission",
                "Prevention and control"
            ],
            [
                "virus type",
                "host range",
                "tissue specificity",
                "infection route",
                "viral receptor",
                "cell culture models"
            ],
            [
                "Virus Type",
                "Host Tissues",
                "Cellular Receptors",
                "Pathogenesis",
                "Epidemiology",
                "Animal Models",
                "Viral Evolution",
                "Immune Response"
            ],
            [
                "Host specificity",
                "Viral receptors",
                "Transmission routes",
                "Tissue tropism",
                "Genetic variability"
            ],
            [
                "Virus tropism",
                "Host specificity",
                "Infection site",
                "Cell receptor interaction"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Affected cell types",
            "Histopathological changes",
            "Virus detection method"
        ],
        "impr+120b": [
            "research problem",
            "virus type",
            "host tissue tropism",
            "methodology",
            "experimental system",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Study population",
            "Sample type",
            "Detection method",
            "Target proteins",
            "Key findings"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R206347",
        "research_problem": "What is the tropism of SARS-CoV-2",
        "orkg_properties": "['material', 'method', 'Date', 'Symptoms and signs', 'research problem', 'Species ', 'Result: Organ tropism', 'Result: Cell tropism']",
        "nechakhin_result": "['viral tropism', 'SARS-CoV-2', 'host cell tropism', 'cellular receptors', 'tissue tropism']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "SARS-CoV-2 cell tropism and multiorgan infection",
        "abstract": "We acknowledge Mingyue Xu, Hengrui Hu, Xijia Liu, Zhengyuan Su, Min Zhou for their critical support. We thank Jia Wu, Hao Tang and Jun Liu from National Biosafety Laboratory (Wuhan), Chinese Academy of Sciences for their support during the study. The study was supported in part by grants from Ministry of Science and Technology of China (2020YFC0844700 and 2020FYC0841700), Hubei Science and Technology funding (2020FCA003 and 2020FCA045), Open Research Fund Program of the State Key Laboratory of Virology of China (2021IOV004), and the National Natural Science Foundation of China (31621061).\nOpen AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visithttp://creativecommons.org/licenses/by/4.0/.Reprints and permissions\nTo date, the number of confirmed coronavirus disease 2019 (COVID-19) cases has surpassed 100 million, with deaths exceeding 2 million, yet the mechanism by which severe acute respiratory syndrome coronavirus (SARS-CoV)-2 attacks the body remains unclear. Although SARS-CoV-2 is known to primarily target the lung, it is also believed to cause multi-organ dysfunction and comprehensive studies on SARS-CoV-2 cell tropism in humans are lacking. SARS-CoV-2 exploits the host angiotensin-converting enzyme 2 (ACE2) as its receptor for cell entry1, but the correlation between SARS-CoV-2 organ/cell tropism and ACE2 distribution is unclear. Here, we studied these issues via a systemic analysis of postmortem specimens from a 66-year-old female COVID-19 patient who had rapidly developed multiorgan failure. The patient died in the hospital on Day 13 of admission (Day 16 of illness) and her autopsy was performed at 8\u2009h after death.\nIn conclusion, our results identified SARS-CoV-2 cell tropism in multiple organs (Fig.1e), indicating that SARS-CoV-2 infects not only the respiratory system (e.g., lungs and trachea) but also the kidneys, small intestines, pancreas, blood vessels, and other tissues. Recently, we disclosed that SARS-CoV-2 also targeted sweat glands and vascular endothelial cells in the skin15. These findings suggest that direct viral infection could, at least partially, contribute to multiorgan injury. Our results also proved a possible correlation between SARS-CoV-2 organotropism and ACE2 distribution, providing supporting evidence for the multiorgan infection of the virus. While our study was limited by its size, its sheds new light into the mechanism of viral infection/transmission and provides valuable information for COVID-19 control.\nIn the trachea, epithelial marker KRT7 clearly revealed viral infection in epithelial cells of the mucosa (Supplementary Fig.S3a-i), conduits, and glands (Supplementary Fig.S3a-ii). Among mucosal epithelial cells, viral antigens were found in trefoil factor 1-positiv",
        "dimensions": [
            [
                "Virus",
                "SARS-CoV-2",
                "Tropism",
                "Host",
                "Cell",
                "Infection",
                "Pathogenesis",
                "Receptor",
                "Entry",
                "Replication",
                "Immune response"
            ],
            [
                "virus",
                "host",
                "cell type",
                "receptor",
                "infection mechanism",
                "tissue distribution"
            ],
            [
                "Biological Mechanism",
                "Host Factors",
                "Tissue Specificity",
                "Clinical Manifestations",
                "Comparative Virology",
                "Therapeutic Implications"
            ],
            [
                "Virus type",
                "Host species",
                "Tissue specificity",
                "Cell type",
                "Transmission route",
                "Geographic location",
                "Time period",
                "Disease severity",
                "Mutation variants",
                "Research methodology"
            ],
            [
                "Virus tropism",
                "Host cells",
                "Infection mechanism",
                "Pathogenesis"
            ]
        ],
        "impr+abs": [
            "Acknowledgments",
            "Funding source",
            "Study limitations",
            "Viral cell tropism",
            "Organ infection",
            "ACE2 distribution",
            "Multiorgan injury",
            "Viral antigens"
        ],
        "impr+120b": [
            "Research problem",
            "Target virus",
            "Biological focus",
            "Methodology",
            "Data source",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Study organism",
            "Sample type",
            "Research focus",
            "Methodology",
            "Organs examined",
            "Funding source",
            "Ethical considerations"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R191261",
        "research_problem": "Representation Learning",
        "orkg_properties": "['metric', 'method', 'On evaluation dataset', 'research problem', 'Has evaluation task']",
        "nechakhin_result": "['neural networks', 'unsupervised learning', 'feature extraction', 'dimensionality reduction', 'clustering', 'deep learning', 'autoencoders', 'generative models']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "LinkBERT: Pretraining Language Models with Document Links",
        "abstract": "Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data.",
        "dimensions": [
            [
                "Machine learning",
                "Deep learning",
                "Unsupervised learning",
                "Feature learning",
                "Neural networks",
                "Dimensionality reduction",
                "Embedding",
                "Clustering",
                "Transfer learning",
                "Natural language processing",
                "Computer vision"
            ],
            [
                "algorithm",
                "input data type",
                "training objective",
                "evaluation metric",
                "application domain",
                "computational resources"
            ],
            [
                "Application Domain",
                "Techniques and Algorithms",
                "Evaluation Metrics",
                "Data Types",
                "Problem Complexity",
                "Publication Venue",
                "Time Period"
            ],
            [
                "Model architecture",
                "Training dataset",
                "Evaluation metric",
                "Pretraining task",
                "Dimensionality of representations",
                "Transfer learning applications",
                "Publication date",
                "Open-source implementation"
            ],
            [
                "Data representation",
                "Learning method",
                "Evaluation metric",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "LM pretraining method",
            "Document links",
            "LM inputs",
            "Pretrained models",
            "Downstream tasks",
            "Joint self-supervised objectives",
            "Text corpus as a graph",
            "New proposal"
        ],
        "impr+120b": [
            "research problem",
            "learning paradigm",
            "model architecture",
            "data modality",
            "evaluation metric",
            "application area"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Data used",
            "Domain",
            "Evaluation tasks",
            "Evaluation metric",
            "Release artifacts"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R191297",
        "research_problem": "Representation Learning on Biomedical Data",
        "orkg_properties": "['metric', 'method', 'On evaluation dataset', 'research problem', 'Has evaluation task']",
        "nechakhin_result": "['biomedical domain', 'representation learning', 'data', 'algorithm', 'techniques', 'deep learning', 'machine learning', 'feature extraction', 'dimensionality reduction', 'classification', 'clustering', 'similarity measures', 'evaluation metrics']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Publicly Available Clinical BERT Embeddings",
        "abstract": "Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.",
        "dimensions": [
            [
                "Biomedical data",
                "Representation learning",
                "Machine learning",
                "Deep learning",
                "Feature extraction",
                "Data mining",
                "Bioinformatics",
                "Health informatics"
            ],
            [
                "biomedical data type",
                "representation learning method",
                "data size",
                "evaluation metrics",
                "application domain"
            ],
            [
                "Biomedical Domain",
                "Type of Biomedical Data",
                "Representation Learning Techniques",
                "Evaluation Metrics",
                "Biomedical Applications",
                "Datasets",
                "Publication Venue"
            ],
            [
                "Data type",
                "Representation learning algorithm",
                "Application domain",
                "Evaluation metrics",
                "Benchmark datasets",
                "Publication date",
                "Biomedical data preprocessing",
                "Computational resources"
            ],
            [
                "Data type",
                "Learning method",
                "Evaluation metric",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "Word embedding model",
            "Specialty corpora",
            "Pre-trained model",
            "Performance improvement",
            "NLP tasks",
            "Domain-specific model",
            "State-of-the-art",
            "De-identification tasks"
        ],
        "impr+120b": [
            "Research problem",
            "Data domain",
            "Learning paradigm",
            "Methodology",
            "Evaluation metric",
            "Application area"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data source",
            "Model architecture",
            "Domain",
            "Evaluation tasks",
            "Performance metric",
            "Resource availability"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189554",
        "research_problem": "Mechanical properties of nacre-inspired materials",
        "orkg_properties": "['method', 'result', 'research problem', 'has material']",
        "nechakhin_result": "['Material composition', 'Nacre structure', 'Mechanical testing methods', 'Stress-strain behavior', 'Strength and toughness', 'Material processing techniques', 'Microstructure', 'Biocompatibility', 'Environmental conditions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "A Constrained Assembly Strategy for High-Strength Natural Nanoclay Film",
        "abstract": "Click to copy section linkSection link copied!High Resolution ImageDownload MS PowerPoint SlideDeveloping high-performance materials from existing natural materials is highly desired because of their environmental friendliness and low cost; two-dimensional nanoclay exfoliated from layered silicate minerals is a good building block to construct multilayered macroscopic assemblies for achieving high mechanical and functional properties. Nevertheless, the efforts have been frustrated by insufficient inter-nanosheet stress transfer and nanosheet misalignment caused by capillary force during solution-based spontaneous assembly, degrading the mechanical strength of clay-based materials. Herein, a constrained assembly strategy that is implemented by in-plane stretching a robust water-containing nanoclay network with hydrogen and ionic bonding is developed to adjust the 2D topography of nanosheets within multilayered nanoclay film. In-plane stretching overcomes capillary force during water removal and thus restrains nanosheet conformation transition from nearly flat to wrinkled, leading to a highly aligned multilayered nanostructure with synergistic hydrogen and ionic bonding. It is proved that inter-nanosheet hydrogen and ionic bonding and nanosheet conformation extension generate profound mechanical reinforcement. The tensile strength and modulus of natural nanoclay film reach up to 429.0 MPa and 43.8 GPa and surpass the counterparts fabricated by normal spontaneous assembly. Additionally, improved heat insulation function and good nonflammability are shown for the natural nanoclay film and extend its potential for realistic uses.This publication is licensed under the terms of your\n                            institutional subscription.Request reuse permissions.",
        "dimensions": [
            [
                "Material composition",
                "Mechanical properties",
                "Nacre structure",
                "Bioinspired materials",
                "Composite materials",
                "Nanocomposites",
                "Toughness",
                "Strength",
                "Stiffness",
                "Fracture resistance"
            ],
            [
                "material composition",
                "structure",
                "mechanical testing method",
                "toughness",
                "elastic modulus",
                "strength",
                "fracture behavior",
                "biomimicry"
            ],
            [
                "Material composition",
                "Structural hierarchy",
                "Mechanical testing methods",
                "Biomimetic design strategies",
                "Applications"
            ],
            [
                "Material composition",
                "Microstructure",
                "Mechanical testing methods",
                "Biomimicry approach",
                "Mechanical properties",
                "Environmental conditions",
                "Processing techniques",
                "Applications"
            ],
            [
                "Material composition",
                "Mechanical properties",
                "Structure",
                "Biological inspiration"
            ]
        ],
        "impr+abs": [
            "Material type",
            "Assembly strategy",
            "Mechanical properties",
            "Functional properties",
            "Bonding type",
            "Tensile strength",
            "Modulus",
            "Heat insulation function"
        ],
        "impr+120b": [
            "research problem",
            "material system",
            "target property",
            "inspiration source",
            "characterization technique",
            "application domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Material system",
            "Fabrication method",
            "Assembly strategy",
            "Mechanical performance metric",
            "Functional performance metric",
            "Potential applications"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R178353",
        "research_problem": "Barrier properties of nanocomposite coating",
        "orkg_properties": "['method', 'result', 'research problem', 'has material']",
        "nechakhin_result": "['Nanocomposite materials',\n 'Coating technology',\n 'Barrier properties',\n 'Film thickness',\n 'Material composition',\n 'Nanoparticle dispersion',\n 'Surface roughness',\n 'Coating morphology',\n 'Adhesion strength',\n 'Permeability',\n 'Chemical resistance',\n 'Temperature stability',\n 'Humidity resistance',\n 'Mechanical properties',\n 'Curing conditions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Can high oxygen and water vapor barrier nanocomposite coatings be obtained with a waterborne formulation?",
        "abstract": "Modification of synthetic,high aspect ratioclay with 6-aminocaprohydroxamic acid hydrochloride pushes the interaction between thepolyvinyl alcohol(PVA) matrix and the filler to the level where the waterbornenanocompositebecomes rather insensitive to swelling, even at an elevatedrelative humidity(RH). The modifier can form stronghydrogen bondswith the hydroxyl groups of PVA via the hydroxamic acid functional group. This prevents the swelling of crystalline PVA domains. Perfectly texturednanocomposite filmsare obtained by spraying polymer-filler suspensions. The combination of the various effects shifts the onset of significant swelling of the nanocomposites to high RH regions. Even at 90% RH, surprisingly low oxygen andwater vaportransmission rates (0.11cm3m\u22122day\u22121bar\u22121and 0.18gm\u22122day\u22121, respectively, for a coating of 0.42\u00b5m) are observed that may render PVA-based, waterborne coatings interesting for food packaging applications.\nHigh performance poly(vinyl alcohol) (PVA)/lignin nanomicelle (LNM) nanocomposite films with good vapor barrier and advanced UV-shielding properties were fabricated in this study. LNM was homogeneously distributed in the PVA matrix and strong robust hydrogen bonds were successfully constructed between LNM and PVA matrix. With only 5\u202fwt% loading of LNM into the PVA/LNM nanocomposite, the water vapor transmission rate (WVTR) was declined by about 189% compared with pure PVA. Moreover, after introducing lignin, the PVA nanocomposite films showed improved tensile strength and toughness, excellent UV-blocking and good thermal stability. As both lignin and PVA are biodegradable, this study shows a meaningful design approach for biodegradable functional nanocomposite films using cheap and easily available biomass and biodegradable raw materials.\n",
        "dimensions": [
            [
                "Nanocomposite materials",
                "Coating technology",
                "Barrier properties",
                "Nanoparticles",
                "Material science",
                "Surface engineering",
                "Polymer composites",
                "Thin films",
                "Nanotechnology",
                "Corrosion resistance"
            ],
            [
                "type of nanocomposite",
                "coating method",
                "nanoparticle size",
                "nanoparticle concentration",
                "matrix material",
                "testing method",
                "environmental conditions"
            ],
            [
                "Material composition",
                "Coating thickness",
                "Nanofiller type and concentration",
                "Testing conditions",
                "Application method",
                "Substrate material",
                "Barrier performance evaluation techniques"
            ],
            [
                "Type of nanocomposite",
                "Nanoparticle size and distribution",
                "Coating thickness",
                "Nanoparticle surface functionalization",
                "Matrix material",
                "Testing conditions",
                "Barrier performance measurement techniques",
                "Application areas"
            ],
            [
                "Material composition",
                "Coating thickness",
                "Barrier performance",
                "Nanoparticle type",
                "Adhesion strength"
            ]
        ],
        "impr+abs": [
            "Material modification",
            "Film fabrication method",
            "Barrier properties",
            "Application potential"
        ],
        "impr+120b": [
            "research problem",
            "material studied",
            "property investigated",
            "coating type",
            "characterization technique",
            "performance metric"
        ],
        "impr+120b+abs": [
            "Material system",
            "Nanofiller modification",
            "Fabrication technique",
            "Barrier performance metrics",
            "Target application",
            "Mechanical property",
            "UV shielding property"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R197288",
        "research_problem": "transformer model",
        "orkg_properties": "['has model', 'Pretraining Architecture', 'Pretraining Task', 'Optimizer', 'organization', 'research problem', 'Model Family', 'license']",
        "nechakhin_result": "['NLP', 'deep learning', 'language modeling', 'attention mechanism', 'neural network', 'natural language processing', 'sequence to sequence', 'BERT', 'GPT', 'transformer architecture', 'self-attention', 'position encoding', 'pre-training', 'fine-tuning']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 3,
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "abstract": ":Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
        "dimensions": [
            [
                "Natural Language Processing",
                "Deep Learning",
                "Attention Mechanism",
                "Neural Networks",
                "Language Modeling",
                "Transformer Architecture"
            ],
            [
                "model",
                "date created",
                "pretraining architecture",
                "pretraining task",
                "training corpus",
                "optimizer",
                "tokenization",
                "number of parameters",
                "license"
            ],
            [
                "Architecture",
                "Applications",
                "Training Data",
                "Performance Metrics",
                "Pre-training vs. Fine-tuning"
            ],
            [
                "model",
                "date created",
                "pretraining architecture",
                "pretraining task",
                "training corpus",
                "optimizer",
                "tokenization",
                "number of parameters",
                "license"
            ],
            [
                "Model type",
                "Application domain",
                "Training data",
                "Evaluation metric",
                "Architecture",
                "Hyperparameters"
            ]
        ],
        "impr+abs": [
            "Language model pretraining",
            "Training data size",
            "Hyperparameter choices",
            "Performance evaluation",
            "Model release"
        ],
        "impr+120b": [
            "Research problem",
            "Model architecture",
            "Training data",
            "Evaluation metric",
            "Application domain",
            "Implementation framework"
        ],
        "impr+120b+abs": [
            "Model architecture",
            "Pretraining methodology",
            "Hyperparameter analysis",
            "Training data size",
            "Evaluation benchmarks",
            "Performance results",
            "Code and model release"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R575541",
        "research_problem": "Large Language Models (LLMs)",
        "orkg_properties": "['has model', 'Pretraining Architecture', 'Pretraining Task', 'Optimizer', 'organization', 'research problem', 'Model Family', 'license']",
        "nechakhin_result": "['Natural Language Processing',\n 'Machine Learning',\n 'Deep Learning',\n 'Text Mining',\n 'Information Retrieval',\n 'Text Generation',\n 'Language Understanding',\n 'Language Modeling',\n 'Semantic Analysis',\n 'Sequence Modeling',\n 'Pre-training and Fine-tuning']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "ERNIE: Enhanced Language Representation with Informative Entities",
        "abstract": "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",
        "dimensions": [
            [
                "Natural Language Processing",
                "Machine Learning",
                "Deep Learning",
                "Text Generation",
                "Language Understanding",
                "Transformer Models",
                "Ethical Implications",
                "Data Privacy",
                "Model Bias",
                "Model Interpretability"
            ],
            [
                "model size",
                "training data",
                "pretraining task",
                "fine-tuning task",
                "architecture",
                "performance metrics",
                "applications"
            ],
            [
                "Model Architecture",
                "Training Data",
                "Applications",
                "Evaluation Metrics",
                "Fine-tuning Techniques",
                "Ethical Considerations"
            ],
            [
                "model architecture",
                "training data",
                "fine-tuning task",
                "pretraining task",
                "number of parameters",
                "publication date",
                "evaluation metrics",
                "application domain"
            ],
            [
                "Research problem",
                "Model type",
                "Training data",
                "Evaluation metric",
                "Applications"
            ]
        ],
        "impr+abs": [
            "Language representation model",
            "Knowledge graph integration",
            "Training data sources",
            "Model performance",
            "Availability of code and datasets"
        ],
        "impr+120b": [
            "research problem",
            "model type",
            "application domain",
            "evaluation metric",
            "training data",
            "methodology"
        ],
        "impr+120b+abs": [
            "Model architecture",
            "Pretraining corpus",
            "Knowledge graph integration",
            "Evaluation tasks",
            "Performance benchmark",
            "Code availability"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R213454",
        "research_problem": "Named Entity Recognition",
        "orkg_properties": "['data source', 'research problem', 'Concept types', 'Data Domain', 'inter-annotator agreement', 'genre', 'inLanguage']",
        "nechakhin_result": "['NLP technique', 'Text analysis', 'Machine learning', 'Feature extraction', 'Semantic understanding', 'Named entity types', 'Entity detection', 'Language processing', 'Information extraction', 'Text classification', 'Training data', 'Supervised learning', 'Unsupervised learning', 'Deep learning', 'Semi-supervised learning', 'Rule-based approaches', 'Evaluation metrics', 'Data preprocessing', 'Entity linking', 'Word embeddings', 'Contextual word representations']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 3,
        "title": "Named Entity Recognition for Hindi-English Code-Mixed Social Media Text",
        "abstract": "Named Entity Recognition (NER) is a major task in the field of Natural Language Processing (NLP), and also is a sub-task of Information Extraction. The challenge of NER for tweets lie in the insufficient information available in a tweet. There has been a significant amount of work done related to entity extraction, but only for resource rich languages and domains such as newswire. Entity extraction is, in general, a challenging task for such an informal text, and code-mixed text further complicates the process with it\u2019s unstructured and incomplete information. We propose experiments with different machine learning classification algorithms with word, character and lexical features. The algorithms we experimented with are Decision tree, Long Short-Term Memory (LSTM), and Conditional Random Field (CRF). In this paper, we present a corpus for NER in Hindi-English Code-Mixed along with extensive experiments on our machine learning models which achieved the best f1-score of 0.95 with both CRF and LSTM.",
        "dimensions": [
            [
                "Natural Language Processing",
                "Machine Learning",
                "Text Mining",
                "Information Extraction",
                "Named Entity Recognition",
                "Entity Extraction",
                "Information Retrieval",
                "Text Analysis",
                "Language Understanding",
                "Named Entity Classification"
            ],
            [
                "task type",
                "language",
                "training data",
                "evaluation metric",
                "model architecture",
                "pretraining",
                "fine-tuning",
                "entity types",
                "domain"
            ],
            [
                "Research Area",
                "Methodology",
                "Datasets",
                "Performance Metrics",
                "Application Domains",
                "Publication Venue",
                "Temporal Relevance"
            ],
            [
                "Application domain",
                "NLP techniques",
                "Datasets",
                "Evaluation metrics",
                "Named entity types",
                "Language",
                "Publication venue"
            ],
            [
                "Entity type",
                "Named entity",
                "Text analysis",
                "Machine learning model",
                "Training data",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Research task",
            "Challenges",
            "Machine learning algorithms",
            "Corpus",
            "Evaluation metric"
        ],
        "impr+120b": [
            "research problem",
            "task type",
            "data source",
            "evaluation metric",
            "model architecture",
            "language",
            "application domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data used",
            "Algorithms",
            "Features",
            "Evaluation metric",
            "Language",
            "Domain"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162364",
        "research_problem": "Protein interaction theme article retrieval",
        "orkg_properties": "['data source', 'research problem', 'Data coverage']",
        "nechakhin_result": "['Protein interaction databases',\n 'Biological networks',\n 'Protein-protein interaction prediction',\n 'Machine learning',\n 'Data mining',\n 'Text mining',\n 'Protein interaction networks',\n 'Functional proteomics',\n 'Protein interaction analysis',\n 'Gene ontology',\n 'Pathway analysis',\n 'Protein interaction visualization',\n 'Protein structure analysis']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Overview of the protein-protein interaction annotation extraction task of BioCreative II",
        "abstract": "Background:The biomedical literature is the primary information source for manual protein-protein interaction annotations. Text-mining systems have been implemented to extract binary protein interactions from articles, but a comprehensive comparison between the different techniques as well as with manual curation was missing.Results:We designed a community challenge, the BioCreative II protein-protein interaction (PPI) task, based on the main steps of a manual protein interaction annotation workflow. It was structured into four distinct subtasks related to: (a) detection of protein interaction-relevant articles; (b) extraction and normalization of protein interaction pairs; (c) retrieval of the interaction detection methods used; and (d) retrieval of actual text passages that provide evidence for protein interactions. A total of 26 teams submitted runs for at least one of the proposed subtasks. In the interaction article detection subtask, the top scoring team reached an F-score of 0.78. In the interaction pair extraction and mapping to SwissProt, a precision of 0.37 (with recall of 0.33) was obtained. For associating articles with an experimental interaction detection method, an F-score of 0.65 was achieved. As for the retrieval of the PPI passages best summarizing a given protein interaction in full-text articles, 19% of the submissions returned by one of the runs corresponded to curator-selected sentences. Curators extracted only the passages that best summarized a given interaction, implying that many of the automatically extracted ones could contain interaction information but did not correspond to the most informative sentences.Conclusion:The BioCreative II PPI task is the first attempt to compare the performance of text-mining tools specific for each of the basic steps of the PPI extraction pipeline. The challenges identified range from problems in full-text format conversion of articles to difficulties in detecting interactor protein pairs and then linking them to their database records. Some limitations were also encountered when using a single (and possibly incomplete) reference database for protein normalization or when limiting search for interactor proteins to co-occurrence within a single sentence, when a mention might span neighboring sentences. Finally, distinguishing between novel, experimentally verified interactions (annotation relevant) and previously known interactions adds additional complexity to these tasks.\nPhysical protein-protein interactions have been studied extensively because of their crucial role in controlling central biological processes such as cell division and their implications in a range of human diseases including cancer. A collection of experimental techniques is available to characterize protein-protein interactions; some of them are more suitable to determine stable complexes whereas others are generally considered better for detecting transient interactions. The use of large-scale proteomics approaches for experimentally obtaining protein interaction information has resulted in an additional source of interaction data. Also, bioinformatics techniques based on sequence, structural, or evolutionary information have been devised to predict binary protein interactions.To capture and provide efficient access to the underlying information, structured interaction annotations have been stored in public databases. These databases vary in annotation depth and type of interactions, but a common character",
        "dimensions": [
            [
                "Protein",
                "Interaction",
                "Article",
                "Retrieval",
                "Bioinformatics",
                "Molecular Biology",
                "Genomics",
                "Proteomics",
                "Biological Networks",
                "Data Mining",
                "Machine Learning",
                "Computational Biology"
            ],
            [
                "Protein type",
                "Interaction type",
                "Experimental method",
                "Biological system",
                "Publication date",
                "Journal",
                "Authors",
                "Keywords"
            ],
            [
                "Protein Names/Types",
                "Interaction Types",
                "Research Methods",
                "Biological Context",
                "Publication Date"
            ],
            [
                "Protein Interaction Type",
                "Experimental Techniques",
                "Biological System",
                "Protein Interaction Network Analysis",
                "Disease Relevance",
                "Computational Methods",
                "Protein Interaction Database",
                "Biological Significance"
            ],
            [
                "Research problem",
                "Protein interaction type",
                "Data source",
                "Analysis method",
                "Key findings"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Task structure",
            "Performance metrics",
            "Data source",
            "Annotation method",
            "Challenges identified",
            "Biological significance",
            "Experimental techniques"
        ],
        "impr+120b": [
            "research problem",
            "domain",
            "data source",
            "retrieval method",
            "evaluation metric",
            "output type"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Task description",
            "Evaluation metric",
            "Data used",
            "Normalization approach",
            "Participants",
            "Identified challenges"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162369",
        "research_problem": "Protein Interaction Article Classification Task",
        "orkg_properties": "['research problem', 'Data coverage']",
        "nechakhin_result": "['Protein', 'Interaction', 'Article', 'Classification', 'Task']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "The Protein-Protein Interaction tasks of BioCreative III: classification/ranking of articles and linking bio-ontology concepts to full text",
        "abstract": "BackgroundDetermining usefulness of biomedical text mining systems requires realistic task definition and data selection criteria without artificial constraints, measuring performance aspects that go beyond traditional metrics. The BioCreative III Protein-Protein Interaction (PPI) tasks were motivated by such considerations, trying to address aspects including how the end user would oversee the generated output, for instance by providing ranked results, textual evidence for human interpretation or measuring time savings by using automated systems. Detecting articles describing complex biological events like PPIs was addressed in the Article Classification Task (ACT), where participants were asked to implement tools for detecting PPI-describing abstracts. Therefore the BCIII-ACT corpus was provided, which includes a training, development and test set of over 12,000 PPI relevant and non-relevant PubMed abstracts labeled manually by domain experts and recording also the human classification times. The Interaction Method Task (IMT) went beyond abstracts and required mining for associations between more than 3,500 full text articles and interaction detection method ontology concepts that had been applied to detect the PPIs reported in them.ResultsA total of 11 teams participated in at least one of the two PPI tasks (10 in ACT and 8 in the IMT) and a total of 62 persons were involved either as participants or in preparing data sets/evaluating these tasks. Per task, each team was allowed to submit five runs offline and another five online via the BioCreative Meta-Server. From the 52 runs submitted for the ACT, the highest Matthew's Correlation Coefficient (MCC) score measured was 0.55 at an accuracy of 89% and the best AUC iP/R was 68%. Most ACT teams explored machine learning methods, some of them also used lexical resources like MeSH terms, PSI-MI concepts or particular lists of verbs and nouns, some integrated NER approaches. For the IMT, a total of 42 runs were evaluated by comparing systems against manually generated annotations done by curators from the BioGRID and MINT databases. The highest AUC iP/R achieved by any run was 53%, the best MCC score 0.55. In case of competitive systems with an acceptable recall (above 35%) the macro-averaged precision ranged between 50% and 80%, with a maximum F-Score of 55%.ConclusionsThe results of the ACT task of BioCreative III indicate that classification of large unbalanced article collections reflecting the real class imbalance is still challenging. Nevertheless, text-mining tools that report ranked lists of relevant articles for manual selection can potentially reduce the time needed to identify half of the relevant articles to less than 1/4 of the time when compared to unranked results. Detecting associations between full text articles and interaction detection method PSI-MI terms (IMT) is more difficult than might be anticipated. This is due to the variability of method term mentions, errors resulting from pre-processing of articles provided as PDF files, and the heterogeneity and different granularity of method term concepts encountered in the ontology. However, combining the sophisticated techniques developed by the participants with supporting evidence strings derived from the articles for human interpretation could result in practical modules for biological annotation workflows.\nCompeting interestsThe authors declare that they have no competing interests.Authors' contributionsMK, MV and FL o",
        "dimensions": [
            [
                "Protein interaction",
                "Article classification",
                "Bioinformatics",
                "Machine learning",
                "Natural language processing",
                "Biological networks",
                "Graph theory",
                "Data mining",
                "Text mining",
                "Feature extraction",
                "Classification algorithms",
                "Biological databases",
                "Protein-protein interaction prediction"
            ],
            [
                "protein type",
                "interaction type",
                "experimental method",
                "dataset used",
                "classification algorithm",
                "evaluation metric",
                "publication date"
            ],
            [
                "Protein Names and Types",
                "Interaction Type",
                "Experimental Methods",
                "Biological Context",
                "Data Analysis Techniques",
                "Research Outcome"
            ],
            [
                "Protein types",
                "Interaction detection method",
                "Experimental conditions",
                "Biological context",
                "Data analysis techniques",
                "Validation methods",
                "Biological implications"
            ],
            [
                "Research problem",
                "Classification method",
                "Data source",
                "Evaluation metric",
                "Feature extraction",
                "Model performance",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "Task type",
            "Performance metrics",
            "Data set",
            "Participant count",
            "Methodology",
            "Challenges",
            "Results interpretation",
            "Competing interests"
        ],
        "impr+120b": [
            "research problem",
            "task type",
            "domain",
            "methodology",
            "data used",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data used",
            "Task type",
            "Methodology",
            "Evaluation metric",
            "Participants",
            "Output format"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162526",
        "research_problem": "Human kinome full-text triage",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains']",
        "nechakhin_result": "['Kinome', 'Human', 'Triage', 'Full-text', 'Research problem']",
        "nechakhin_mappings": 2,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Overview of the BioCreative VI text-mining services for Kinome Curation Track",
        "abstract": "The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants\u2019 systems had to identify and rank relevant articles in a collection of 5.2\u00a0M MEDLINE citations (task 1) or 530\u00a0000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search.",
        "dimensions": [
            [
                "Kinome",
                "Human",
                "Full-text",
                "Triage",
                "Biomedical",
                "Research",
                "Text mining",
                "Information retrieval",
                "Machine learning",
                "Bioinformatics"
            ],
            [
                "kinome",
                "human",
                "full-text",
                "triage",
                "biomedical literature",
                "text mining",
                "machine learning",
                "natural language processing",
                "biological pathways",
                "protein kinases"
            ],
            [
                "Keywords",
                "Publication Date",
                "Author(s)",
                "Journal/Conference",
                "Methodology/Techniques",
                "Biological Systems",
                "Research Findings"
            ],
            [
                "Keywords",
                "Publication date",
                "Kinome coverage",
                "Triage method",
                "Biological assays",
                "Disease relevance"
            ],
            [
                "Research problem",
                "Data source",
                "Text mining method",
                "Classification criteria",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Competition type",
            "Data set",
            "Task type",
            "Text mining strategies",
            "Performance evaluation",
            "Supervised approach",
            "Baseline comparison"
        ],
        "impr+120b": [
            "Study subject",
            "Data source",
            "Task type",
            "Methodology",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data source",
            "Task description",
            "Methodology",
            "Evaluation metric",
            "Dataset size",
            "Curation axis"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162526",
        "research_problem": "Human kinome abstracts triage",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains']",
        "nechakhin_result": "['Kinome',\n 'Human',\n 'Abstracts',\n 'Triage']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Overview of the BioCreative VI text-mining services for Kinome Curation Track",
        "abstract": "The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants\u2019 systems had to identify and rank relevant articles in a collection of 5.2\u00a0M MEDLINE citations (task 1) or 530\u00a0000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search.",
        "dimensions": [
            [
                "Kinome",
                "Human",
                "Abstracts",
                "Triage",
                "Biomedical",
                "Text mining",
                "Machine learning",
                "Bioinformatics",
                "Information retrieval",
                "Natural language processing"
            ],
            [
                "kinome",
                "abstracts",
                "triage",
                "human",
                "biomedical",
                "text mining",
                "machine learning",
                "natural language processing",
                "biological pathways",
                "disease associations"
            ],
            [
                "Keywords",
                "Protein Names",
                "Biological Processes",
                "Diseases and Disorders",
                "Techniques and Methods",
                "Publication Date"
            ],
            [
                "Keywords",
                "Publication date",
                "Research methodology",
                "Biological context",
                "Kinase targets",
                "Disease relevance",
                "Cellular pathways",
                "Experimental models",
                "Data analysis techniques"
            ],
            [
                "Research problem",
                "Data source",
                "Classification method",
                "Evaluation metric",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "Competition type",
            "Data set",
            "Task type",
            "Text mining strategies",
            "Performance evaluation",
            "Supervised approach",
            "Baseline comparison"
        ],
        "impr+120b": [
            "Research problem",
            "Domain",
            "Data source",
            "Task type",
            "Methodology",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data source",
            "Dataset size",
            "Task definition",
            "Methodology",
            "Evaluation approach",
            "Baseline comparison"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162546",
        "research_problem": "Identification and ranking of relevant PubMed citations describing protein\u2013protein interactions (PPIs) affected by mutations",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains']",
        "nechakhin_result": "['PubMed citations', 'Protein\u2013protein interactions (PPIs)', 'Mutations']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Overview of the BioCreative VI Precision Medicine Track: mining protein interactions and mutations for precision medicine",
        "abstract": "The Precision Medicine Initiative is a multicenter effort aiming at formulating personalized treatments leveraging on individual patient data (clinical, genome sequence and functional genomic data) together with the information in large knowledge bases (KBs) that integrate genome annotation, disease association studies, electronic health records and other data types. The biomedical literature provides a rich foundation for populating these\nKBs, reporting genetic and molecular interactions that provide the scaffold for the cellular regulatory systems and detailing the influence of genetic variants in these interactions. The goal of BioCreative VI Precision Medicine Track was to extract this particular type of information and was organized in two tasks: (i) document triage task, focused on identifying scientific literature containing experimentally verified protein\u2013protein interactions (PPIs) affected by genetic mutations and (ii) relation extraction task, focused on extracting the affected interactions (protein pairs). To assist system developers and task participants, a large-scale corpus of PubMed documents was manually annotated for this task. Ten teams worldwide contributed 22 distinct text-mining models for the document triage task, and six teams worldwide contributed 14 different text-mining systems for the relation extraction task. When comparing the text-mining system predictions with human annotations, for the triage task, the best F-score was 69.06%, the best precision was 62.89%, the best recall was 98.0% and the best average precision was 72.5%. For the relation extraction task, when taking homologous genes into account, the best F-score was 37.73%, the best precision was 46.5% and the best recall was 54.1%. Submitted systems explored a wide range of methods, from traditional rule-based, statistical and machine learning systems to state-of-the-art deep learning methods. Given the level of participation and the individual team results we find the precision medicine track to be successful in engaging the text-mining research community. In the meantime, the track produced a manually annotated corpus of 5509 PubMed documents developed by BioGRID curators and relevant for precision medicine. The data set is freely available to the community, and the specific interactions have been integrated into the BioGRID data set. In addition, this challenge provided the first results of automatically identifying PubMed articles that describe PPI affected by mutations, as well as extracting the affected relations from those articles. Still, much progress is needed for computer-assisted precision medicine text mining to become mainstream. Future work should focus on addressing the remaining technical challenges and incorporating the practical benefits of text-mining tools into real-world precision medicine information-related curation.",
        "dimensions": [
            [
                "PubMed citations",
                "Protein-protein interactions (PPIs)",
                "Mutations",
                "Relevance",
                "Ranking"
            ],
            [
                "PubMed ID",
                "Protein names",
                "Mutation type",
                "Interaction type",
                "Biological context",
                "Experimental method",
                "Publication date",
                "Journal",
                "Keywords"
            ],
            [
                "Keywords",
                "Protein Interaction Databases",
                "Mutation Types",
                "Experimental Techniques",
                "Biological Context",
                "Publication Date"
            ],
            [
                "Protein-protein interactions (PPIs)",
                "Mutations",
                "PubMed citations",
                "Ranking"
            ],
            [
                "Research problem",
                "PubMed search criteria",
                "Mutation type",
                "Protein interaction type",
                "Citation ranking method",
                "Data source",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Research initiative",
            "Task type",
            "Text-mining methods",
            "Performance metrics",
            "Data set",
            "Community engagement",
            "Future work"
        ],
        "impr+120b": [
            "research problem",
            "data source",
            "entity type",
            "task type",
            "domain",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "research domain",
            "task type",
            "data used",
            "evaluation metric",
            "methodology",
            "participant count",
            "data availability"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162352",
        "research_problem": "extract text fragments to support the annotation of a given protein- GO term association",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains', 'Coarse-grained Entity types', 'Number of training data mentions', 'Number of test data mentions', 'Annotation type']",
        "nechakhin_result": "['Protein sequence',\n 'GO term',\n 'Text fragments',\n 'Annotation',\n 'Association']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Evaluation of BioCreAtIvE assessment of task 2",
        "abstract": "BackgroundMolecular Biology accumulated substantial amounts of data concerning functions of genes and proteins. Information relating to functional descriptions is generally extracted manually from textual data and stored in biological databases to build up annotations for large collections of gene products. Those annotation databases are crucial for the interpretation of large scale analysis approaches using bioinformatics or experimental techniques. Due to the growing accumulation of functional descriptions in biomedical literature the need for text mining tools to facilitate the extraction of such annotations is urgent. In order to make text mining tools useable in real world scenarios, for instance to assist database curators during annotation of protein function, comparisons and evaluations of different approaches on full text articles are needed.ResultsThe Critical Assessment for Information Extraction in Biology (BioCreAtIvE) contest consists of a community wide competition aiming to evaluate different strategies for text mining tools, as applied to biomedical literature. We report on task two which addressed the automatic extraction and assignment of Gene Ontology (GO) annotations of human proteins, using full text articles. The predictions of task 2 are based on triplets ofprotein \u2013 GO term \u2013 article passage. The annotation-relevant text passages were returned by the participants and evaluated by expert curators of the GO annotation (GOA) team at the European Institute of Bioinformatics (EBI). Each participant could submit up to three results for each sub-task comprising task 2. In total more than 15,000 individual results were provided by the participants. The curators evaluated in addition to the annotation itself, whether the protein and the GO term were correctly predicted and traceable through the submitted text fragment.ConclusionConcepts provided by GO are currently the most extended set of terms used for annotating gene products, thus they were explored to assess how effectively text mining tools are able to extract those annotations automatically. Although the obtained results are promising, they are still far from reaching the required performance demanded by real world applications. Among the principal difficulties encountered to address the proposed task, were the complex nature of the GO terms and protein names (the large range of variants which are used to express proteins and especially GO terms in free text), and the lack of a standard training set. A range of very different strategies were used to tackle this task. The dataset generated in line with the BioCreative challenge is publicly available and will allow new possibilities for training information extraction methods in the domain of molecular biology.\nThe dataset produced at the BioCreative contest task two is freely available from:http://www.pdg.cnb.uam.es/BioLINK/BioCreative.eval.html[18] and is given in as an XML-like format. From the nine registered users who participated in task 2.1, a total of 15,992 evidence passages were provided to the curators. Out of those, 12,014 corresponded to the requested queries (the rest corresponded to new predictions which were not contained in the test set). On average 11.34 (standard deviation of 2.30) submissions of annotation predictions were sent for each single query triplet across all the user submissions (21 runs). Users submitted between a single run up to the maximum of three runs allowed; there were 21 runs s",
        "dimensions": [
            [
                "Text fragments",
                "Protein",
                "GO term",
                "Association",
                "Annotation"
            ],
            [
                "text fragment",
                "protein identifier",
                "GO term identifier",
                "annotation method",
                "evidence code",
                "publication identifier"
            ],
            [
                "Protein Name/ID",
                "Gene Ontology (GO) Term",
                "Text Fragments",
                "Experimental Methods",
                "Biological Context",
                "Publication Date",
                "Keywords"
            ],
            [
                "Protein name",
                "Gene Ontology (GO) term",
                "Association evidence",
                "Species",
                "Publication date",
                "Experimental methods",
                "Confidence score",
                "Keywords"
            ],
            [
                "Protein name",
                "GO term",
                "Experimental evidence",
                "Annotation method",
                "Reference"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Evaluation metric",
            "Data source",
            "Annotation type",
            "Task type"
        ],
        "impr+120b": [
            "Research problem",
            "Data source",
            "Methodology",
            "Annotation target",
            "Evaluation metric",
            "Tool",
            "Output format"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Task description",
            "Dataset",
            "Evaluation method",
            "Participants",
            "Domain",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162364",
        "research_problem": "Protein interaction description sentences retrieval",
        "orkg_properties": "['data source', 'research problem', 'Data coverage']",
        "nechakhin_result": "['Protein domain',\n 'Protein structure',\n 'Protein function',\n 'Protein interaction type',\n 'Protein interaction strength',\n 'Protein interaction network',\n 'Protein interaction database',\n 'Text mining',\n 'Natural language processing',\n 'Semantic similarity',\n 'Machine learning',\n 'Information retrieval']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Overview of the protein-protein interaction annotation extraction task of BioCreative II",
        "abstract": "Background:The biomedical literature is the primary information source for manual protein-protein interaction annotations. Text-mining systems have been implemented to extract binary protein interactions from articles, but a comprehensive comparison between the different techniques as well as with manual curation was missing.Results:We designed a community challenge, the BioCreative II protein-protein interaction (PPI) task, based on the main steps of a manual protein interaction annotation workflow. It was structured into four distinct subtasks related to: (a) detection of protein interaction-relevant articles; (b) extraction and normalization of protein interaction pairs; (c) retrieval of the interaction detection methods used; and (d) retrieval of actual text passages that provide evidence for protein interactions. A total of 26 teams submitted runs for at least one of the proposed subtasks. In the interaction article detection subtask, the top scoring team reached an F-score of 0.78. In the interaction pair extraction and mapping to SwissProt, a precision of 0.37 (with recall of 0.33) was obtained. For associating articles with an experimental interaction detection method, an F-score of 0.65 was achieved. As for the retrieval of the PPI passages best summarizing a given protein interaction in full-text articles, 19% of the submissions returned by one of the runs corresponded to curator-selected sentences. Curators extracted only the passages that best summarized a given interaction, implying that many of the automatically extracted ones could contain interaction information but did not correspond to the most informative sentences.Conclusion:The BioCreative II PPI task is the first attempt to compare the performance of text-mining tools specific for each of the basic steps of the PPI extraction pipeline. The challenges identified range from problems in full-text format conversion of articles to difficulties in detecting interactor protein pairs and then linking them to their database records. Some limitations were also encountered when using a single (and possibly incomplete) reference database for protein normalization or when limiting search for interactor proteins to co-occurrence within a single sentence, when a mention might span neighboring sentences. Finally, distinguishing between novel, experimentally verified interactions (annotation relevant) and previously known interactions adds additional complexity to these tasks.\nPhysical protein-protein interactions have been studied extensively because of their crucial role in controlling central biological processes such as cell division and their implications in a range of human diseases including cancer. A collection of experimental techniques is available to characterize protein-protein interactions; some of them are more suitable to determine stable complexes whereas others are generally considered better for detecting transient interactions. The use of large-scale proteomics approaches for experimentally obtaining protein interaction information has resulted in an additional source of interaction data. Also, bioinformatics techniques based on sequence, structural, or evolutionary information have been devised to predict binary protein interactions.To capture and provide efficient access to the underlying information, structured interaction annotations have been stored in public databases. These databases vary in annotation depth and type of interactions, but a common character",
        "dimensions": [
            [
                "Protein",
                "Interaction",
                "Description",
                "Sentences",
                "Retrieval"
            ],
            [
                "Protein",
                "interaction",
                "description",
                "sentences",
                "retrieval"
            ],
            [
                "Keywords",
                "Protein Types",
                "Interaction Techniques",
                "Biological Context",
                "Publication Date",
                "Journal or Conference",
                "Author(s)"
            ],
            [
                "Keywords",
                "Protein Interaction Types",
                "Experimental Techniques",
                "Biological Context",
                "Protein Interaction Networks",
                "Data Sources",
                "Biological Significance",
                "Computational Methods"
            ],
            [
                "Protein interaction",
                "Experimental method",
                "Data source",
                "Biological context",
                "Interaction type",
                "Validation method"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Task structure",
            "Performance metrics",
            "Experimental techniques",
            "Database characteristics"
        ],
        "impr+120b": [
            "research problem",
            "task type",
            "data source",
            "retrieval method",
            "evaluation metric",
            "domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Task description",
            "Evaluation metric",
            "Data used",
            "Normalization approach",
            "Participants",
            "Identified challenges"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162482",
        "research_problem": "Retrieve evidence sentences for a BEL statement",
        "orkg_properties": "['research problem', 'Data coverage']",
        "nechakhin_result": "[\"biological evidence\", \"BEL statement\", \"text mining\", \"information retrieval\", \"natural language processing\"]",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "BioCreative V track 4: a shared task for the extraction of causal network information using the Biological Expression Language",
        "abstract": "Automatic extraction of biological network information is one of the most desired and most complex tasks in biological and medical text mining. Track 4 at BioCreative V attempts to approach this complexity using fragments of large-scale manually curated biological networks, represented in Biological Expression Language (BEL), as training and test data. BEL is an advanced knowledge representation format which has been designed to be both human readable and machine processable. The specific goal of track 4 was to evaluate text mining systems capable of automatically constructing BEL statements from given evidence text, and of retrieving evidence text for given BEL statements. Given the complexity of the task, we designed an evaluation methodology which gives credit to partially correct statements. We identified various levels of information expressed by BEL statements, such as entities, functions, relations, and introduced an evaluation framework which rewards systems capable of delivering useful BEL fragments at each of these levels. The aim of this evaluation method is to help identify the characteristics of the systems which, if combined, would be most useful for achieving the overall goal of automatically constructing causal biological networks from text.",
        "dimensions": [
            [
                "Evidence source",
                "Context",
                "Supporting entity",
                "Relation",
                "Confidence score"
            ],
            [
                "evidence type",
                "evidence source",
                "evidence date",
                "evidence reliability score",
                "evidence context",
                "evidence relevance score"
            ],
            [
                "Biological Entity",
                "Biological Process",
                "Experimental Techniques",
                "Publication Date",
                "Authors and Affiliations",
                "Keywords and MeSH Terms",
                "Citation Network",
                "Journal Impact Factor"
            ],
            [
                "type of mindfulness meditation",
                "anxiety measurement scale",
                "study design",
                "duration of intervention",
                "population demographics",
                "comparator group",
                "outcome measures",
                "follow-up period"
            ],
            [
                "research problem",
                "Entity type",
                "provides API",
                "uses identifier system",
                "Metadata schema"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Data used",
            "Evaluation methodology",
            "Knowledge representation format"
        ],
        "impr+120b": [
            "Research problem",
            "Input type",
            "Output type",
            "Methodology",
            "Data source",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Task definition",
            "Data used",
            "Knowledge representation format",
            "Evaluation methodology",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162526",
        "research_problem": "Human kinase snippet extraction",
        "orkg_properties": "['research problem', 'Data coverage', 'Data domains', 'Coarse-grained Entity types']",
        "nechakhin_result": "['Kinase activity', 'Human subjects', 'Biological pathways', 'Protein phosphorylation', 'Enzyme inhibitors', 'Protein kinases', 'Cancer research', 'Signal transduction', 'Protein-protein interactions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Overview of the BioCreative VI text-mining services for Kinome Curation Track",
        "abstract": "The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants\u2019 systems had to identify and rank relevant articles in a collection of 5.2\u00a0M MEDLINE citations (task 1) or 530\u00a0000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search.",
        "dimensions": [
            [
                "Kinase",
                "Human",
                "Protein",
                "Enzyme",
                "Phosphorylation",
                "Cell Signaling",
                "Drug Target",
                "Biological Function",
                "Cellular Regulation",
                "Pathway",
                "Disease",
                "Therapeutic Intervention"
            ],
            [
                "kinase type",
                "substrate specificity",
                "cellular localization",
                "regulatory mechanisms",
                "disease relevance",
                "phosphorylation sites",
                "interaction partners"
            ],
            [
                "Biological Context",
                "Molecular Structure",
                "Experimental Techniques",
                "Disease Associations",
                "Cellular Localization",
                "Regulatory Mechanisms",
                "Bioinformatics and Data Analysis"
            ],
            [
                "Type of kinase",
                "Substrate specificity",
                "Activation mechanism",
                "Disease relevance",
                "Cellular localization",
                "Structural features",
                "Kinase inhibitors",
                "Signaling pathways"
            ],
            [
                "Protein kinase",
                "Substrate specificity",
                "Cell signaling pathway",
                "Regulation mechanism",
                "Drug target",
                "Catalytic activity",
                "Phosphorylation site",
                "Disease association"
            ]
        ],
        "impr+abs": [
            "Competition type",
            "Data set",
            "Task type",
            "Text mining strategies",
            "Performance evaluation",
            "Supervised approach",
            "Baseline comparison"
        ],
        "impr+120b": [
            "research problem",
            "target entity",
            "data source",
            "extraction method",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data source",
            "Task description",
            "Methodology",
            "Evaluation metric",
            "Dataset size",
            "Curation axis"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R171842",
        "research_problem": "Retrieving GO evidence sentences for relevant genes",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Data domains', 'Annotation type']",
        "nechakhin_result": "['Gene ID', 'GO ID', 'Evidence Code', 'Evidence Sentence']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "BC4GO: a full-text corpus for the BioCreative IV GO task",
        "abstract": "Gene function curation via Gene Ontology (GO) annotation is a common task among Model Organism Database groups. Owing to its manual nature, this task is considered one of the bottlenecks in literature curation. There have been many previous attempts at automatic identification of GO terms and supporting information from full text. However, few systems have delivered an accuracy that is comparable with humans. One recognized challenge in developing such systems is the lack of marked sentence-level evidence text that provides the basis for making GO annotations. We aim to create a corpus that includes the GO evidence text along with the three core elements of GO annotations: (i) a gene or gene product, (ii) a GO term and (iii) a GO evidence code. To ensure our results are consistent with real-life GO data, we recruited eight professional GO curators and asked them to follow their routine GO annotation protocols. Our annotators marked up more than 5000 text passages in 200 articles for 1356 distinct GO terms. For evidence sentence selection, the inter-annotator agreement (IAA) results are 9.3% (strict) and 42.7% (relaxed) in F1-measures. For GO term selection, the IAAs are 47% (strict) and 62.9% (hierarchical). Our corpus analysis further shows that abstracts contain \u223c10% of relevant evidence sentences and 30% distinct GO terms, while the Results/Experiment section has nearly 60% relevant sentences and >70% GO terms. Further, of those evidence sentences found in abstracts, less than one-third contain enough experimental detail to fulfill the three core criteria of a GO annotation. This result demonstrates the need of using full-text articles for text mining GO annotations. Through its use at the BioCreative IV GO (BC4GO) task, we expect our corpus to become a valuable resource for the BioNLP research community.Database URL:http://www.biocreative.org/resources/corpora/bc-iv-go-task-corpus/.",
        "dimensions": [
            [
                "Gene name",
                "GO term",
                "Evidence type",
                "Sentence context",
                "Biological process",
                "Molecular function",
                "Cellular component",
                "Experimental evidence",
                "Inferred from",
                "Phenotype",
                "Regulation",
                "Interaction",
                "Species",
                "Publication",
                "Date"
            ],
            [
                "gene name",
                "GO term",
                "evidence type",
                "evidence sentence",
                "publication date",
                "publication title",
                "author",
                "journal",
                "organism"
            ],
            [
                "Gene Name/ID",
                "Gene Function",
                "Gene Ontology (GO) Terms",
                "Evidence Type",
                "Publication Date",
                "Species",
                "Journal/Conference"
            ],
            [
                "Gene symbol",
                "Gene function",
                "Biological process",
                "Molecular function",
                "Cellular component",
                "Species",
                "Publication date"
            ],
            [
                "Data source",
                "Gene name",
                "Evidence type",
                "Annotation",
                "Reference",
                "Confidence score"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Annotation elements",
            "Annotation accuracy",
            "Data collection method",
            "Annotation protocol",
            "Inter-annotator agreement",
            "Corpus analysis",
            "Resource availability"
        ],
        "impr+120b": [
            "research problem",
            "target entity",
            "data source",
            "retrieval method",
            "evaluation metric",
            "output format"
        ],
        "impr+120b+abs": [
            "Corpus description",
            "Annotation elements",
            "Annotator details",
            "Inter-annotator agreement",
            "Evaluation metric",
            "Task context",
            "Resource availability"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162352",
        "research_problem": "automatically assigning GO terms to protein and article pairs",
        "orkg_properties": "['data source', 'research problem', 'Data Domain', 'Data coverage', 'Ontology used', 'Number of training data mentions', 'Number of test data mentions', 'Annotation type']",
        "nechakhin_result": "['protein similarity', 'article content', 'gene ontology (GO) terms', 'semantic similarity', 'classification algorithm', 'text mining', 'machine learning', 'knowledge base']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Evaluation of BioCreAtIvE assessment of task 2",
        "abstract": "BackgroundMolecular Biology accumulated substantial amounts of data concerning functions of genes and proteins. Information relating to functional descriptions is generally extracted manually from textual data and stored in biological databases to build up annotations for large collections of gene products. Those annotation databases are crucial for the interpretation of large scale analysis approaches using bioinformatics or experimental techniques. Due to the growing accumulation of functional descriptions in biomedical literature the need for text mining tools to facilitate the extraction of such annotations is urgent. In order to make text mining tools useable in real world scenarios, for instance to assist database curators during annotation of protein function, comparisons and evaluations of different approaches on full text articles are needed.ResultsThe Critical Assessment for Information Extraction in Biology (BioCreAtIvE) contest consists of a community wide competition aiming to evaluate different strategies for text mining tools, as applied to biomedical literature. We report on task two which addressed the automatic extraction and assignment of Gene Ontology (GO) annotations of human proteins, using full text articles. The predictions of task 2 are based on triplets ofprotein \u2013 GO term \u2013 article passage. The annotation-relevant text passages were returned by the participants and evaluated by expert curators of the GO annotation (GOA) team at the European Institute of Bioinformatics (EBI). Each participant could submit up to three results for each sub-task comprising task 2. In total more than 15,000 individual results were provided by the participants. The curators evaluated in addition to the annotation itself, whether the protein and the GO term were correctly predicted and traceable through the submitted text fragment.ConclusionConcepts provided by GO are currently the most extended set of terms used for annotating gene products, thus they were explored to assess how effectively text mining tools are able to extract those annotations automatically. Although the obtained results are promising, they are still far from reaching the required performance demanded by real world applications. Among the principal difficulties encountered to address the proposed task, were the complex nature of the GO terms and protein names (the large range of variants which are used to express proteins and especially GO terms in free text), and the lack of a standard training set. A range of very different strategies were used to tackle this task. The dataset generated in line with the BioCreative challenge is publicly available and will allow new possibilities for training information extraction methods in the domain of molecular biology.\nThe dataset produced at the BioCreative contest task two is freely available from:http://www.pdg.cnb.uam.es/BioLINK/BioCreative.eval.html[18] and is given in as an XML-like format. From the nine registered users who participated in task 2.1, a total of 15,992 evidence passages were provided to the curators. Out of those, 12,014 corresponded to the requested queries (the rest corresponded to new predictions which were not contained in the test set). On average 11.34 (standard deviation of 2.30) submissions of annotation predictions were sent for each single query triplet across all the user submissions (21 runs). Users submitted between a single run up to the maximum of three runs allowed; there were 21 runs s",
        "dimensions": [
            [
                "Protein features",
                "Gene Ontology (GO) terms",
                "Natural Language Processing (NLP)",
                "Machine learning algorithms",
                "Text mining",
                "Biological databases",
                "Semantic similarity",
                "Protein function prediction",
                "Article content",
                "Ontology mapping"
            ],
            [
                "protein",
                "article",
                "GO terms",
                "text mining",
                "machine learning",
                "biological process",
                "cellular component",
                "molecular function",
                "evaluation metrics"
            ],
            [
                "Protein Information",
                "Gene Ontology (GO) Terms",
                "Textual Content",
                "Publication Information",
                "Methodology and Techniques"
            ],
            [
                "Protein features",
                "Article content",
                "Machine learning algorithms",
                "Evaluation metrics",
                "Datasets and benchmarks",
                "Ontology structure",
                "Application domains"
            ],
            [
                "Automation method",
                "Data source",
                "Evaluation metric",
                "Algorithm",
                "Output format"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Evaluation metric",
            "Data source",
            "Annotation type",
            "Task participants",
            "Submission count",
            "Training data availability"
        ],
        "impr+120b": [
            "Research problem",
            "Target entities",
            "Assignment objective",
            "Data source",
            "Methodology",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Task description",
            "Dataset",
            "Evaluation method",
            "Participants",
            "Domain",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162360",
        "research_problem": "gene name normalization",
        "orkg_properties": "['data source', 'research problem', 'Dataset name', 'Data coverage', 'Coarse-grained Entity types', 'Ontology used', 'Annotation type']",
        "nechakhin_result": "['gene ontology', 'gene expression', 'gene sequences', 'gene function', 'gene mutations', 'gene interactions', 'gene regulation', 'gene variants']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Overview of BioCreative II gene normalization",
        "abstract": "Background:The goal of the gene normalization task is to link genes or gene products mentioned in the literature to biological databases. This is a key step in an accurate search of the biological literature. It is a challenging task, even for the human expert; genes are often described rather than referred to by gene symbol and, confusingly, one gene name may refer to different genes (often from different organisms). For BioCreative II, the task was to list the Entrez Gene identifiers for human genes or gene products mentioned in PubMed/MEDLINE abstracts. We selected abstracts associated with articles previously curated for human genes. We provided 281 expert-annotated abstracts containing 684 gene identifiers for training, and a blind test set of 262 documents containing 785 identifiers, with a gold standard created by expert annotators. Inter-annotator agreement was measured at over 90%.Results:Twenty groups submitted one to three runs each, for a total of 54 runs. Three systems achieved F-measures (balanced precision and recall) between 0.80 and 0.81. Combining the system outputs using simple voting schemes and classifiers obtained improved results; the best composite system achieved an F-measure of 0.92 with 10-fold cross-validation. A 'maximum recall' system based on the pooled responses of all participants gave a recall of 0.97 (with precision 0.23), identifying 763 out of 785 identifiers.Conclusion:Major advances for the BioCreative II gene normalization task include broader participation (20 versus 8 teams) and a pooled system performance comparable to human experts, at over 90% agreement. These results show promise as tools to link the literature with biological databases.\nPerformance on the BioCreative II GN task demonstrates progress since the first BioCreative workshop in 2004. The results obtained for human gene/protein identification are comparable to results obtained earlier for mouse and fly; three teams achieved an F-measure of 0.80 or above for one of their runs. However, there is significant progress along several new dimensions. First, the assessment involved 20 groups, as compared with eight groups for BioCreative I. The results achieved by combining input from all of the participating systems outperformed any single system, achieving F-measures from 0.85 to 0.92, depending on the method of combination.The participating teams explored the 'solution space' for this challenge evaluation well. Four teams incorporated explicit handling of conjunction and enumeration; this no longer seems to be a significant cause of loss in recall. The 'maximum recall' system achieved a recall of 96.2% (precision 23.1%). A number of groups did contrastive studies on the utility of adding lexical resources and contextual resources, and on the benefits of lexicon curation. The participants also explored novel approaches to the matching of mentions and lexical resources, and there was significant exploration of contextual models for disambiguation and removal of false positives. An interesting finding was that many groups did not feel the need for large training corpora, especially those using lexicon-based approaches.What does this mean in terms of practical performance? Performance depends on a number of factors: the quality and completeness of the lexical resources; the selection criteria of the articles, including date, journal, domain, and whether they are likely to contain curatable information; the amount of both intra-species and ",
        "dimensions": [
            [
                "Natural Language Processing",
                "Biomedical Text Mining",
                "Named Entity Recognition",
                "Information Retrieval",
                "Machine Learning",
                "Biomedical Ontologies",
                "Text Preprocessing",
                "Entity Linking",
                "Semantic Similarity",
                "Biomedical Literature",
                "Gene Nomenclature",
                "Text Classification"
            ],
            [
                "gene name",
                "normalization method",
                "database used",
                "text corpus",
                "evaluation metric"
            ],
            [
                "Gene Name",
                "Normalization Techniques",
                "Biological Context",
                "Publication Date",
                "Experimental Methods",
                "Data Types",
                "Citation Network"
            ],
            [
                "text data",
                "Natural Language Processing (NLP) techniques",
                "Biomedical domain",
                "Evaluation metrics",
                "Datasets",
                "Machine learning algorithms",
                "Named Entity Recognition (NER)",
                "Biomedical ontologies"
            ],
            [
                "Gene name",
                "Normalization method",
                "Data source",
                "Reference database"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Task description",
            "Performance evaluation",
            "Data used",
            "Comparison with previous work",
            "Evaluation metric",
            "Participation",
            "System performance"
        ],
        "impr+120b": [
            "research problem",
            "Domain",
            "Data source",
            "Normalization technique",
            "Evaluation metric",
            "Tool",
            "Reference database"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Task definition",
            "Dataset",
            "Evaluation metric",
            "Number of participating groups",
            "Methodology",
            "Annotation process",
            "Resources"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162364",
        "research_problem": "Protein-protein interaction confirmation experiment name recognition and normalization",
        "orkg_properties": "['data source', 'research problem', 'Data coverage', 'Ontology used', 'Annotation type']",
        "nechakhin_result": "['Protein-protein interaction experiment type', 'Name recognition method', 'Normalization method']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Overview of the protein-protein interaction annotation extraction task of BioCreative II",
        "abstract": "Background:The biomedical literature is the primary information source for manual protein-protein interaction annotations. Text-mining systems have been implemented to extract binary protein interactions from articles, but a comprehensive comparison between the different techniques as well as with manual curation was missing.Results:We designed a community challenge, the BioCreative II protein-protein interaction (PPI) task, based on the main steps of a manual protein interaction annotation workflow. It was structured into four distinct subtasks related to: (a) detection of protein interaction-relevant articles; (b) extraction and normalization of protein interaction pairs; (c) retrieval of the interaction detection methods used; and (d) retrieval of actual text passages that provide evidence for protein interactions. A total of 26 teams submitted runs for at least one of the proposed subtasks. In the interaction article detection subtask, the top scoring team reached an F-score of 0.78. In the interaction pair extraction and mapping to SwissProt, a precision of 0.37 (with recall of 0.33) was obtained. For associating articles with an experimental interaction detection method, an F-score of 0.65 was achieved. As for the retrieval of the PPI passages best summarizing a given protein interaction in full-text articles, 19% of the submissions returned by one of the runs corresponded to curator-selected sentences. Curators extracted only the passages that best summarized a given interaction, implying that many of the automatically extracted ones could contain interaction information but did not correspond to the most informative sentences.Conclusion:The BioCreative II PPI task is the first attempt to compare the performance of text-mining tools specific for each of the basic steps of the PPI extraction pipeline. The challenges identified range from problems in full-text format conversion of articles to difficulties in detecting interactor protein pairs and then linking them to their database records. Some limitations were also encountered when using a single (and possibly incomplete) reference database for protein normalization or when limiting search for interactor proteins to co-occurrence within a single sentence, when a mention might span neighboring sentences. Finally, distinguishing between novel, experimentally verified interactions (annotation relevant) and previously known interactions adds additional complexity to these tasks.\nPhysical protein-protein interactions have been studied extensively because of their crucial role in controlling central biological processes such as cell division and their implications in a range of human diseases including cancer. A collection of experimental techniques is available to characterize protein-protein interactions; some of them are more suitable to determine stable complexes whereas others are generally considered better for detecting transient interactions. The use of large-scale proteomics approaches for experimentally obtaining protein interaction information has resulted in an additional source of interaction data. Also, bioinformatics techniques based on sequence, structural, or evolutionary information have been devised to predict binary protein interactions.To capture and provide efficient access to the underlying information, structured interaction annotations have been stored in public databases. These databases vary in annotation depth and type of interactions, but a common character",
        "dimensions": [
            [
                "Protein-protein interaction",
                "Confirmation experiment",
                "Name recognition",
                "Normalization"
            ],
            [
                "experiment type",
                "protein names",
                "interaction confirmation method",
                "database source",
                "normalization method",
                "publication date"
            ],
            [
                "Experiment Type",
                "Protein Names",
                "Interaction Confirmation Techniques",
                "Species",
                "Publication Date",
                "Experimental Conditions",
                "Data Source"
            ],
            [
                "Experiment type",
                "Protein databases",
                "Normalization techniques",
                "Text mining algorithms",
                "Validation criteria",
                "Publication date",
                "Application domain"
            ],
            [
                "Experiment type",
                "Protein interaction",
                "Normalization method",
                "Data source",
                "Validation method"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Task subcategories",
            "Performance metrics",
            "Experimental techniques",
            "Database characteristics"
        ],
        "impr+120b": [
            "research problem",
            "entity type",
            "task type",
            "methodology",
            "data source",
            "evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Task description",
            "Evaluation metric",
            "Data used",
            "Normalization approach",
            "Participants",
            "Identified challenges"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R163875",
        "research_problem": "Software named entity recognition",
        "orkg_properties": "['data source', 'research problem', 'Relation types', 'Data coverage', 'Data domains', 'Entity types', 'Annotation type', 'Software entity types']",
        "nechakhin_result": "['Natural language processing techniques', 'Machine learning algorithms', 'Text mining', 'Information retrieval', 'Text classification', 'Named entity recognition', 'Software engineering']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "The role of software in science: a knowledge graph-based analysis of software mentions in PubMed Central",
        "abstract": "View X ",
        "dimensions": [
            [
                "Natural Language Processing",
                "Named Entity Recognition",
                "Machine Learning",
                "Deep Learning",
                "Information Extraction",
                "Text Mining",
                "Information Retrieval",
                "Artificial Intelligence",
                "Data Mining",
                "Text Processing"
            ],
            [
                "software type",
                "entity recognition method",
                "training data",
                "evaluation metrics",
                "language",
                "domain",
                "open source",
                "commercial",
                "accuracy",
                "speed"
            ],
            [
                "Technique/Methodology",
                "Datasets",
                "Evaluation Metrics",
                "Application Domains",
                "Language/Platform",
                "Publication Venue"
            ],
            [
                "Application domain",
                "NER techniques",
                "Evaluation metrics",
                "Dataset",
                "Language",
                "NER applications",
                "NER tools and frameworks",
                "Publication date"
            ],
            [
                "Software type",
                "Named entity",
                "Recognition method",
                "Training data",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Software type",
            "Analysis method",
            "Data source",
            "Entity type"
        ],
        "impr+120b": [
            "research problem",
            "application domain",
            "methodology",
            "data source",
            "evaluation metric",
            "software implementation"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Data source",
            "Methodology",
            "Task",
            "Entity type",
            "Domain"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R166497",
        "research_problem": "Named entity recognition",
        "orkg_properties": "['model', 'data source', 'research problem', 'Concept types', 'Results', 'Data coverage', 'Data domains', 'inter-annotator agreement', 'Annotation level', 'Generation method', 'Has-statistics', 'inLanguage']",
        "nechakhin_result": "['Natural Language Processing',\n 'Machine Learning',\n 'Information Extraction',\n 'Text Mining',\n 'Language Modeling',\n 'Entity Extraction',\n 'Named Entity Classification',\n 'Deep Learning']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "Softcite dataset: A dataset of software mentions in biomedical and economic research publications",
        "abstract": "Software contributions to academic research are relatively invisible, especially to the formalized scholarly reputation system based on bibliometrics. In this article, we introduce a gold-standard dataset of software mentions from the manual annotation of 4,971 academic PDFs in biomedicine and economics. The dataset is intended to be used for automatic extraction of software mentions from PDF format research publications by supervised learning at scale. We provide a description of the dataset and an extended discussion of its creation process, including improved text conversion of academic PDFs. Finally, we reflect on our challenges and lessons learned during the dataset creation, in hope of encouraging more discussion about creating datasets for machine learning use.",
        "dimensions": [
            [
                "Natural Language Processing",
                "Machine Learning",
                "Information Retrieval",
                "Text Mining",
                "Named Entity Recognition",
                "Entity Extraction",
                "Information Extraction",
                "Text Analysis",
                "Named Entity Recognition Systems",
                "Named Entity Recognition Algorithms",
                "Named Entity Recognition Models",
                "Named Entity Recognition Applications"
            ],
            [
                "language",
                "corpus",
                "annotation scheme",
                "training data",
                "model architecture",
                "evaluation metric"
            ],
            [
                "Research Area",
                "Techniques and Algorithms",
                "Datasets",
                "Evaluation Metrics",
                "Applications",
                "Language",
                "Publication Venue"
            ],
            [
                "task",
                "model architecture",
                "training data",
                "evaluation metric",
                "domain",
                "publication date",
                "language",
                "dataset source"
            ],
            [
                "research problem",
                "Entity type",
                "Data source",
                "Evaluation metric",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "Dataset name",
            "Research field",
            "Data source",
            "Supervised learning",
            "Creation process",
            "Challenges",
            "Lessons learned"
        ],
        "impr+120b": [
            "research problem",
            "task type",
            "methodology",
            "dataset",
            "evaluation metric",
            "application domain"
        ],
        "impr+120b+abs": [
            "Dataset description",
            "Domain(s)",
            "Data source",
            "Annotation methodology",
            "Number of documents",
            "Intended use",
            "Creation challenges"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R166504",
        "research_problem": "Bioinformatics databases and software named entity recognition",
        "orkg_properties": "['data source', 'research problem', 'Dataset name', 'Data coverage', 'Data domains', 'Knowledge Base', 'Entity types']",
        "nechakhin_result": "['Bioinformatics databases', 'Software', 'Named entity recognition']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "bioNerDS: exploring bioinformatics\u2019 database and software use through literature mining",
        "abstract": "BackgroundBiology-focused databases and software define bioinformatics and their use is central to computational biology. In such a complex and dynamic field, it is of interest to understand what resources are available, which are used, how much they are used, and for what they are used. While scholarly literature surveys can provide some insights, large-scale computer-based approaches to identify mentions of bioinformatics databases and software from primary literature would automate systematic cataloguing, facilitate the monitoring of usage, and provide the foundations for the recovery of computational methods for analysing biological data, with the long-term aim of identifying best/common practice in different areas of biology.ResultsWe have developed bioNerDS, a named entity recogniser for the recovery of bioinformatics databases and software from primary literature. We identify such entities with an F-measure ranging from 63% to 91% at the mention level and 63-78% at the document level, depending on corpus. Not attaining a higher F-measure is mostly due to high ambiguity in resource naming, which is compounded by the on-going introduction of new resources. To demonstrate the software, we applied bioNerDS to full-text articles from BMC Bioinformatics and Genome Biology. General mention patterns reflect the remit of these journals, highlighting BMC Bioinformatics\u2019s emphasis on new tools and Genome Biology\u2019s greater emphasis on data analysis. The data also illustrates some shifts in resource usage: for example, the past decade has seen R and the Gene Ontology join BLAST and GenBank as the main components in bioinformatics processing.ConclusionsWe demonstrate the feasibility of automatically identifying resource names on a large-scale from the scientific literature and show that the generated data can be used for exploration of bioinformatics database and software usage. For example, our results help to investigate the rate of change in resource usage and corroborate the suspicion that a vast majority of resources are created, but rarely (if ever) used thereafter. bioNerDS is available athttp://bionerds.sourceforge.net/.\nCompeting interestsThe authors declare that they have no competing interests.Authors\u2019 contributionsGD programmed and ran bioNerDS on the Genome Biology and BMC Bioinformatics journal corpora, and drafted the manuscript. AB helped with the data analysis. GN, DLR and RS initially conceptualised the project and provided continual guidance and discussion. All authors read and approved the final manuscript.\nBelow are the links to the authors\u2019 original submitted files for images.Authors\u2019 original file for figure 1Authors\u2019 original file for figure 2Authors\u2019 original file for figure 3Authors\u2019 original file for figure 4Authors\u2019 original file for figure 5Authors\u2019 original file for figure 6Authors\u2019 original file for figure 7\n",
        "dimensions": [
            [
                "Bioinformatics",
                "Databases",
                "Software",
                "Named Entity Recognition",
                "Natural Language Processing",
                "Machine Learning",
                "Text Mining",
                "Information Retrieval"
            ],
            [
                "Bioinformatics",
                "databases",
                "software",
                "named entity recognition",
                "text mining",
                "machine learning",
                "natural language processing",
                "algorithm",
                "accuracy",
                "performance evaluation"
            ],
            [
                "Field of Study: Bioinformatics",
                "Named Entity Recognition (NER) Techniques",
                "Bioinformatics Databases",
                "Software Tools for NER",
                "Applications of NER in Bioinformatics"
            ],
            [
                "Database Name",
                "Software Name",
                "Entity Type",
                "Evaluation Metrics",
                "Data Source",
                "Performance Comparison",
                "Application Domain",
                "Publication Year"
            ],
            [
                "Bioinformatics databases",
                "Software",
                "Named entity recognition",
                "Data sources",
                "Application domain"
            ]
        ],
        "impr+abs": [
            "Database and software usage",
            "Named entity recognition",
            "Resource naming ambiguity",
            "Resource usage patterns",
            "Availability of resources",
            "Competing interests",
            "Authors' contributions",
            "Original submitted files"
        ],
        "impr+120b": [
            "research problem",
            "Domain",
            "Data source",
            "Task",
            "Methodology",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Evaluation metric",
            "Data source",
            "Software availability",
            "Domain"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R186167",
        "research_problem": "SPARQL query optimization",
        "orkg_properties": "['Algorithm', 'research problem', 'Hardware', 'uses benchmark', 'has performance']",
        "nechakhin_result": "['Query language', 'Semantic web', 'Optimization techniques', 'RDF graph', 'Triplestore', 'Query execution', 'Query rewriting', 'Query planning', 'Query evaluation', 'Indexing', 'Join algorithms', 'Parallelization', 'Cost estimation', 'Query performance']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Scalable SPARQL querying of large RDF graphs",
        "abstract": "The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster.",
        "dimensions": [
            [
                "Query optimization",
                "SPARQL",
                "Semantic web",
                "Database management",
                "Performance tuning",
                "Linked data",
                "Graph databases",
                "Data integration"
            ],
            [
                "query",
                "optimization",
                "SPARQL",
                "performance",
                "execution plan",
                "indexing",
                "query rewriting",
                "cost estimation",
                "query caching"
            ],
            [
                "Query Optimization Techniques",
                "Semantic Web Technologies",
                "Performance Metrics",
                "Database Systems",
                "Machine Learning and AI",
                "Benchmarking and Comparative Studies"
            ],
            [
                "Query structure",
                "Query execution engine",
                "Indexing and caching",
                "Query rewriting and optimization",
                "Federated SPARQL queries",
                "Cost-based optimization",
                "Parallel and distributed query processing",
                "Query performance evaluation"
            ],
            [
                "Query type",
                "Optimization technique",
                "Data source",
                "Performance metric",
                "Query execution time"
            ]
        ],
        "impr+abs": [
            "Data used",
            "Research problem",
            "Resolution methods",
            "Performance metric"
        ],
        "impr+120b": [
            "research problem",
            "optimization technique",
            "evaluation metric",
            "dataset",
            "implementation language",
            "performance metric"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Proposed solution",
            "Data partitioning strategy",
            "Query decomposition method",
            "Performance improvement claim",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R76786",
        "research_problem": "SPARQL query ",
        "orkg_properties": "['Algorithm', 'research problem', 'uses benchmark', 'has performance']",
        "nechakhin_result": "['Research problem', 'Keywords', 'Title', 'Abstract', 'Authors', 'Publication year', 'Citation count', 'Journal/conference name', 'Methodology', 'Results', 'Conclusion', 'Institution/organization', 'Funding', 'Data source', 'Field/domain', 'Topic', 'References']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 5,
        "title": "SPARQL basic graph pattern optimization using selectivity estimation",
        "abstract": "In this paper, we formalize the problem of Basic Graph Pattern (BGP) optimization for SPARQL queries and main memory graph implementations of RDF data. We define and analyze the characteristics of heuristics for selectivity-based static BGP optimization. The heuristics range from simple triple pattern variable counting to more sophisticated selectivity estimation techniques. Customized summary statistics for RDF data enable the selectivity estimation of joined triple patterns and the development of efficient heuristics. Using the Lehigh University Benchmark (LUBM), we evaluate the performance of the heuristics for the queries provided by the LUBM and discuss some of them in more details.",
        "dimensions": [
            [
                "Research problem",
                "Keywords",
                "Publication date",
                "Authors",
                "Journal/conference",
                "Citations",
                "Abstract",
                "Research methodology",
                "Related concepts",
                "Data sources"
            ],
            [
                "Research problem",
                "Data source",
                "Query type",
                "Query language",
                "Result format",
                "Query execution time",
                "Number of results",
                "Query endpoint"
            ],
            [
                "Keywords",
                "Dataset",
                "SPARQL Endpoint",
                "Query Structure",
                "Publication Date",
                "Authors"
            ],
            [
                "Query type",
                "Query complexity",
                "Dataset used",
                "Semantic web technologies"
            ],
            [
                "Query type",
                "Data source",
                "Output format",
                "Filter conditions",
                "Result variables"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Query optimization",
            "Data characteristics",
            "Evaluation method"
        ],
        "impr+120b": [
            "Research problem",
            "Query language",
            "Data source",
            "Evaluation metric",
            "Implementation platform"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Heuristics",
            "Evaluation dataset",
            "Evaluation metric",
            "Data type",
            "Implementation platform"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R215710",
        "research_problem": "empirical research in requirements engineering",
        "orkg_properties": "['method', 'Database', 'research problem', 'number of papers', 'research_field_investigated', 'time interval', 'venue investigated', 'topic investigated', 'result']",
        "nechakhin_result": "['Research question', 'Methodology', 'Data collection', 'Data analysis', 'Sample size', 'Research context', 'Publication year']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Empirical research methodologies and studies in Requirements Engineering: How far did we come?",
        "abstract": "Since the inception of the RE conference series (1992), both researchers and practitioners in the RE community have acknowledged the significance of empirical evaluation as an instrument to gain knowledge about various aspects of RE phenomena and the validity of our research results. A significant number of empirical studies have been conducted in the search for knowledge about RE problems as well as evidence of successful and less successful application of proposed solutions. This editorial presents the progress empirical RE research has made since 1992. Based on a search in the Scopus digital library, we report from an analysis of peer-reviewed systematic literature reviews and mapping studies to showcase major areas of RE research that use methods from the Empirical Software Engineering paradigm. We summarize prior empirical research in RE and introduce the contributors to this special issue on empirical research methodologies and studies in RE.\nOver the past 20 years, software startups have created many products that have changed human life. Since these companies are creating brand-new products or services, requirements are difficult to gather and highly volatile. Although scientific interest in software development in this context has increased, the studies on requirements engineering in software startups are still scarce and mostly focused on elicitation activities.This study overcomes this gap by answering how requirements engineering practices are performed in this context.We conducted a grounded theory study based on 17 interviews with software startups practitioners.We constructed a model to show that software startups do not follow a single set of practices but, instead, build a custom process, changed throughout the development of the company, combining different practices according to a set of influences (Founders, Software Development Manager, Developers, Market, Business Model and Startup Ecosystem).Our findings show that requirements engineering activities in software startups are similar to those in agile teams, but some steps vary as a consequence of the lack of an accessible customer.\nHighlights\u2022This editorial provides a reflection on past empirical research inRequirements Engineering(RE) and challenges lying ahead.\u2022We describe the growth of empirical RE publications, based on a search in the Scopus digital library.\u2022We compare observations from published mapping studies and systematic literature reviews in RE, with RE topicsin two RE roadmaps.\u2022We propose RE sub-areas where more evaluation of evidence is needed and also call for more careful use of theories in empirical RE research.\n",
        "dimensions": [
            [
                "Empirical research",
                "Requirements engineering",
                "Software engineering",
                "Empirical studies",
                "Requirement analysis",
                "Software development",
                "Research methodology",
                "Data collection",
                "Data analysis",
                "Case studies"
            ],
            [
                "research type",
                "research methodology",
                "requirements engineering area",
                "empirical study design",
                "data collection method",
                "sample size",
                "research location",
                "research duration",
                "research findings"
            ],
            [
                "Research Methodology",
                "Research Focus",
                "Empirical Data Collection Techniques",
                "Empirical Data Analysis Methods",
                "Application Domains",
                "Publication Venue",
                "Time Period"
            ],
            [
                "Research methodology",
                "Research focus",
                "Industry domain",
                "Research metrics",
                "Publication venue",
                "Collaboration network",
                "Research impact"
            ],
            [
                "Empirical approach",
                "Research methodology",
                "Data collection method",
                "Analysis technique",
                "Research findings"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Empirical methods",
            "Study location",
            "Evaluation metric",
            "Research area",
            "Publication source",
            "Comparison method",
            "Recommendation"
        ],
        "impr+120b": [
            "research problem",
            "research method",
            "data source",
            "evaluation metric",
            "study context",
            "domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Data source",
            "Data collection method",
            "Study population",
            "Findings",
            "Research type"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R216298",
        "research_problem": "empirical research in software engineering",
        "orkg_properties": "['method', 'research problem', 'number of papers', 'research_field_investigated', 'time interval', 'venue investigated', 'topic investigated', 'result']",
        "nechakhin_result": "['research topic', 'research methodology', 'software engineering domain', 'research objectives', 'research design', 'data collection methods', 'data analysis techniques', 'research outcomes', 'research participants', 'research context']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "Empirical Research in Software Engineering \u2014 A Literature Survey",
        "abstract": "Empirical research is playing a significant role in software engineering (SE), and it has been applied to evaluate software artifacts and technologies. There have been a great number of empirical research articles published recently. There is also a large research community in empirical software engineering (ESE). In this paper, we identify both the overall landscape and detailed implementations of ESE, and investigate frequently applied empirical methods, targeted research purposes, used data sources, and applied data processing approaches and tools in ESE. The aim is to identify new trends and obtain interesting observations of empirical software engineering across different sub-fields of software engineering. We conduct a mapping study on 538 selected articles from January 2013 to November 2017, with four research questions. We observe that the trend of applying empirical methods in software engineering is continuously increasing and the most commonly applied methods are experiment, case study and survey. Moreover, open source projects are the most frequently used data sources. We also observe that most of researchers have paid attention to the validity and the possibility to replicate their studies. These observations are carefully analyzed and presented as carefully designed diagrams. We also reveal shortcomings and demanded knowledge/strategies in ESE and propose recommendations for researchers.\n",
        "dimensions": [
            [
                "software engineering",
                "empirical research",
                "software development",
                "software testing",
                "software maintenance",
                "software quality",
                "software metrics",
                "software process improvement",
                "empirical studies",
                "experimental research",
                "quantitative analysis",
                "qualitative analysis"
            ],
            [
                "research type",
                "software engineering subfield",
                "research methodology",
                "data collection method",
                "sample size",
                "research findings",
                "publication date",
                "research location"
            ],
            [
                "Research Methodology",
                "Software Engineering Subfield",
                "Empirical Study Type",
                "Research Metrics",
                "Software Development Lifecycle Phase",
                "Empirical Data Collection Techniques",
                "Software Engineering Tools and Technologies"
            ],
            [
                "Research methodology",
                "Software engineering domain",
                "Research metrics",
                "Software development lifecycle phase",
                "Empirical data sources",
                "Empirical research tools",
                "Software engineering practices",
                "Empirical research outcomes"
            ],
            [
                "Research problem",
                "Methodology",
                "Data collection",
                "Analysis",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Research method",
            "Data sources",
            "Data processing approach",
            "Trends",
            "Observations",
            "Validity",
            "Recommendations"
        ],
        "impr+120b": [
            "research problem",
            "Methodology",
            "Data source",
            "Study context",
            "Evaluation metric",
            "Findings"
        ],
        "impr+120b+abs": [
            "Research domain",
            "Study type",
            "Data sources",
            "Methodology",
            "Time span",
            "Number of studies",
            "Research questions"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R162731",
        "research_problem": "Application and forming of hard material coatings ",
        "orkg_properties": "['result', 'research problem', 'has material', 'realizes']",
        "nechakhin_result": "['Material type', 'Coating methods', 'Coating properties', 'Application techniques', 'Surface preparation', 'Coating thickness', 'Coating adhesion', 'Coating durability', 'Coating performance', 'Coating structure', 'Coating composition', 'Coating deposition', 'Coating substrate', 'Coating temperature', 'Coating environment']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Cross-wedge rolling of PTA-welded hybrid steel billets with rolling bearing steel and hard material coatings",
        "abstract": "Within the Collaborative Research Centre 1153 \u201cTailored Forming\u201c a process chain for the manufacturing of hybrid high performance components is developed. Exemplary process steps consist of deposit welding of high performance steel on low-cost steel, pre-shaping by cross-wedge rolling and finishing by milling.Hard material coatings such as Stellite 6 or Delcrome 253 are used as wear or corrosion protection coatings in industrial applications. Scientists of the Institute of Material Science welded these hard material alloys onto a base material, in this case C22.8, to create a hybrid workpiece. Scientists of the Institut f\u00fcr Integrierte Produktion Hannover have shown that these hybrid workpieces can be formed without defects (e.g. detachment of the coating) by cross-wedge rolling. After forming, the properties of the coatings are retained or in some cases even improved (e.g. the transition zone between base material and coating). By adjustments in the welding process, it was possible to apply the 100Cr6 rolling bearing steel, as of now declared as non-weldable, on the low-cost steel C22.8. 100Cr6 was formed afterwards in its hybrid bonding state with C22.8 by cross-wedge rolling, thus a component-integrated bearing seat was produced. Even after welding and forming, the rolling bearing steel coating could still be quench-hardened to a hardness of over 60 HRC. This paper shows the potential of forming hybrid billets to tailored parts. Since industrially available standard materials can be used for hard material coatings by this approach, even though they are not weldable by conventional methods, it is not necessary to use expensive, for welding designed materials to implement a hybrid component concept.TopicsCorrosion,Alloys,Machining,Welding,Scientific society and organization",
        "dimensions": [
            [
                "Material composition",
                "Coating techniques",
                "Hardness testing methods",
                "Surface roughness",
                "Adhesion strength",
                "Wear resistance",
                "Corrosion resistance",
                "Coating thickness",
                "Microstructure analysis"
            ],
            [
                "coating type",
                "application method",
                "substrate material",
                "coating composition",
                "coating thickness",
                "hardness",
                "adhesion strength",
                "wear resistance"
            ],
            [
                "Material Type",
                "Coating Techniques",
                "Industrial Applications",
                "Coating Properties",
                "Surface Engineering"
            ],
            [
                "Coating type",
                "Deposition method",
                "Substrate material",
                "Coating properties",
                "Industrial applications",
                "Surface characterization techniques",
                "Environmental conditions",
                "Coating thickness"
            ],
            [
                "Application method",
                "Coating material",
                "Hardness measurement",
                "Surface preparation",
                "Adhesion strength",
                "Coating thickness"
            ]
        ],
        "impr+abs": [
            "Research project",
            "Material used",
            "Process steps",
            "Application area",
            "Scientific institution",
            "Properties after forming",
            "Welding process adjustment",
            "Industrial application"
        ],
        "impr+120b": [
            "Research problem",
            "Material type",
            "Coating technique",
            "Application area",
            "Performance evaluation",
            "Substrate material"
        ],
        "impr+120b+abs": [
            "Research objective",
            "Materials used",
            "Coating type",
            "Forming method",
            "Welding technique",
            "Evaluation metric",
            "Application area"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R171846",
        "research_problem": "Lifespan of bearings with various material combinations",
        "orkg_properties": "['result', 'research problem', 'has material', 'realizes']",
        "nechakhin_result": "['Materials of bearings', 'Lifespan of bearings', 'Material combinations', 'Bearing performance', 'Wear and tear of bearings', 'Friction properties of bearing materials', 'Load capacity of bearings', 'Operating conditions of bearings']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "Investigation of the material combination 20MnCr5 and X45CrSi9-3 in the Tailored Forming of shafts with bearing seats",
        "abstract": "The Tailored Forming process chain is used to manufacture hybrid components and consists of a joining process or Additive Manufacturing for various materials (e.g. deposition welding), subsequent hot forming, machining and heat treatment. In this way, components can be produced with materials adapted to the load case. For this paper, hybrid shafts are produced by deposition welding of a cladding made of X45CrSi9-3 onto a workpiece made from 20MnCr5. The hybrid shafts are then formed by means of cross-wedge rolling. It is investigated, how the thickness of the cladding and the type of cooling after hot forming (in air or in water) affect the properties of the cladding. The hybrid shafts are formed without layer separation. However, slight core loosening occurres in the area of the bearing seat due to the Mannesmann effect. The microhardness of the cladding is only slightly effected by the cooling strategy, while the microhardness of the base material is significantly higher in water cooled shafts. The microstructure of the cladding after both cooling strategies consists mainly of martensite. In the base material, air cooling results in a mainly ferritic microstructure with grains of ferrite-pearlite. Quenching in water results in a microstructure containing mainly martensite.\nComponents, e.g. shafts, must withstand various chemical, tribological and physical stresses during usage. During operation, some component areas are exposed to high mechanical loads. If the high stresses only occur in the area close to the surface, the use of a cost-intensive, high-strength material for the component can be avoided and the affected area can be cladded with a harder material, e.g. 100Cr6 or X45CrSi9-3, with a thickness of up to several millimeters instead. Various processes are available for applying these high-strength claddings. In order to achieve the final geometry, the cladding is subsequently machined. A disadvantage of cladding is the weld microstructure which is present in the cladding and in the heat-affected zone of the base material after the material is deposited. This can reduce the load capacity in the component area. Mildebrath et al. have demonstrated that the microstructure can be transformed into a fine-grained forming structure by subsequent hot forming [1].\nThe combination of a joining process or Additive Manufacturing with a hot forming process, subsequent machining and, if necessary, heat treatment is known as a Tailored Forming process chain. The process chain is used for the production of hybrid components like axial bearing washers, bushings, shafts and bevel gears. Various material combinations were investigated for the production of Tailored Forming components. The research results on the materials and process steps used in this investigation are presented in this section and the research question is derived.2.1Deposition weldingVarious processes can be used for the production of claddings. The claddings can serve different purposes, such as corrosion or wear resistance. They are also used to improve surface hardness and strength. This is known as hardfacing. The layers can range from 0.5\u00a0mm to several millimeters in thickness [2]. Various processes are known for the welding of protective claddings. These processes are laser metal deposition with powder (LMD-P), laser hot-wire cladding (LHWC), laser induction cladding (LIC), gas metal arc welding (GMAW) or plasma transferred arc welding (PTA) [2,3,4,5].The LHWC process is ",
        "dimensions": [
            [
                "Material composition of bearings",
                "Lifespan of bearings",
                "Friction and wear characteristics of materials",
                "Operating conditions (temperature, load, speed)",
                "Manufacturing processes for bearings",
                "Corrosion resistance of materials",
                "Testing methods for bearing lifespan"
            ],
            [
                "bearing material",
                "lubrication type",
                "operating temperature",
                "load type",
                "bearing design",
                "environmental conditions",
                "maintenance schedule"
            ],
            [
                "Material Combinations",
                "Load and Speed Conditions",
                "Lubrication Methods",
                "Environmental Factors",
                "Failure Modes and Mechanisms",
                "Maintenance and Reliability Strategies",
                "Experimental Methods and Testing"
            ],
            [
                "Material combination",
                "Operating conditions",
                "Lubrication method",
                "Load type and magnitude",
                "Environmental factors",
                "Maintenance practices",
                "Bearing design"
            ],
            [
                "Material combinations",
                "Lifespan measurement method",
                "Environmental conditions",
                "Testing equipment",
                "Data analysis method"
            ]
        ],
        "impr+abs": [
            "Material combination",
            "Manufacturing process",
            "Cooling strategy",
            "Microstructure",
            "Component stress",
            "Cladding process",
            "Forming process",
            "Research results"
        ],
        "impr+120b": [
            "research problem",
            "Study object",
            "Material type",
            "Performance metric",
            "Experimental method",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Material combination",
            "Manufacturing process",
            "Process parameters",
            "Cooling method",
            "Microstructure",
            "Hardness measurement",
            "Component type"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R108199",
        "research_problem": "Focus of Crowd-based Requirements Engineering",
        "orkg_properties": "['research problem', 'RE activities with crowd involvement', 'Utilities in CrowdRE']",
        "nechakhin_result": "['Requirements engineering', 'Crowdsourcing', 'Crowd-based', 'Collective intelligence', 'Crowd wisdom', 'User requirements', 'Collaborative requirements', 'Crowd collaboration', 'Crowd participation', 'Crowd evaluation']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution",
        "abstract": ":Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number---in the range of thousands per day for popular applications---a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements.",
        "dimensions": [
            [
                "Requirements Engineering",
                "Crowdsourcing",
                "Collaborative Software Development",
                "User Involvement",
                "Software Development",
                "Crowd-based Systems",
                "User Participation",
                "Collaborative Requirements Elicitation"
            ],
            [
                "crowd-based",
                "requirements engineering",
                "stakeholders",
                "collaboration",
                "communication",
                "tools",
                "challenges",
                "best practices"
            ],
            [
                "Domain",
                "Methodology",
                "Challenges and Limitations",
                "Benefits and Advantages",
                "Tools and Platforms",
                "Collaboration and Communication",
                "Quality Assurance",
                "User Engagement"
            ],
            [
                "Type of Crowd-based Requirements Engineering",
                "Application Domain",
                "Collaboration Platform",
                "Stakeholder Involvement",
                "Success Metrics",
                "Quality Assurance Mechanisms",
                "Cultural and Geographical Factors",
                "Ethical and Legal Considerations"
            ],
            [
                "research problem",
                "Crowd-based approach",
                "Requirements elicitation method",
                "Evaluation criteria"
            ]
        ],
        "impr+abs": [
            "Social network platform",
            "Research problem",
            "Methodology",
            "Evaluation metric",
            "Data used",
            "Application domain",
            "Resolution methods"
        ],
        "impr+120b": [
            "Research problem",
            "Methodology",
            "Data source",
            "Stakeholder involvement",
            "Evaluation metric"
        ],
        "impr+120b+abs": [
            "Data source",
            "Research problem",
            "Methodology",
            "Machine learning technique",
            "Evaluation metric",
            "Study size",
            "Application domain"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R138661",
        "research_problem": "Biomedical research in  Psychiatric Disorders",
        "orkg_properties": "['Data', 'research problem', 'Used models', 'Study cohort', 'Outcome assessment']",
        "nechakhin_result": "['Biomedical approaches', 'Psychiatric disorders', 'Research methodology', 'Clinical trials', 'Neurology', 'Genetics', 'Pharmacology', 'Psychology', 'Mental health', 'Neuroimaging', 'Treatment options', 'Epidemiology', 'Biological markers']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "Clinical data Neuroimage data",
        "abstract": ":Effective discrimination of attention deficit hyperactivity disorder (ADHD) using imaging and functional biomarkers would have fundamental influence on public health. In usual, the discrimination is based on the standards of American Psychiatric Association. In this paper, we modified one of the deep learning method on structure and parameters according to the properties of ADHD data, to discriminate ADHD on the unique public dataset of ADHD-200. We predicted the subjects as control, combined, inattentive or hyperactive through their frequency features. The results achieved improvement greatly compared to the performance released by the competition. Besides, the imbalance in datasets of deep learning model influenced the results of classification. As far as we know, it is the first time that the deep learning method has been used for the discrimination of ADHD with fMRI data.\n",
        "dimensions": [
            [
                "Biomedical",
                "Psychiatric Disorders",
                "Mental Health",
                "Neuroscience",
                "Genetics",
                "Brain Imaging",
                "Pharmacology",
                "Neurobiology",
                "Behavioral Science",
                "Clinical Trials"
            ],
            [
                "disorder type",
                "biomarkers",
                "genetics",
                "neuroimaging",
                "treatment methods",
                "clinical trials",
                "epidemiology",
                "comorbidity",
                "risk factors"
            ],
            [
                "Specific Psychiatric Disorders",
                "Biomedical Techniques and Methods",
                "Neurobiological Mechanisms",
                "Clinical Interventions and Treatments",
                "Epidemiological Studies",
                "Comorbidity and Co-occurring Conditions",
                "Animal Models and Preclinical Research",
                "Psychosocial Factors",
                "Translational Research",
                "Cross-disciplinary Approaches"
            ],
            [
                "Type of psychiatric disorder",
                "Research methodology",
                "Treatment approach",
                "Biomarkers",
                "Patient population",
                "Neurobiological mechanisms",
                "Co-morbidities",
                "Intervention outcomes"
            ],
            [
                "Biomedical research",
                "Psychiatric Disorders",
                "Research methodology",
                "Data sources",
                "Analysis techniques",
                "Findings",
                "Implications",
                "Future directions"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Data used",
            "Methodology",
            "Evaluation metric",
            "Dataset characteristics"
        ],
        "impr+120b": [
            "Research domain",
            "Target condition",
            "Study type",
            "Data modality",
            "Methodology"
        ],
        "impr+120b+abs": [
            "Data used",
            "Research problem",
            "Methodology",
            "Dataset",
            "Target classes",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R44731",
        "research_problem": "Determination of the COVID-19 basic reproduction number",
        "orkg_properties": "['Time period', 'Basic reproduction number', 'research problem', 'location']",
        "nechakhin_result": "['COVID-19', 'basic reproduction number', 'determination']",
        "nechakhin_mappings": 1,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Transmission interval estimates suggest pre-symptomatic spread of COVID-19",
        "abstract": "BackgroundAs the COVID-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. We determine the incubation period and serial interval distribution for transmission clusters in Singapore and in Tianjin. We infer the basic reproduction number and identify the extent of pre-symptomatic transmission.MethodsWe collected outbreak information from Singapore and Tianjin, China, reported from Jan.19-Feb.26 and Jan.21-Feb.27, respectively. We estimated incubation periods and serial intervals in both populations.ResultsThe mean incubation period was 7.1 (6.13, 8.25) days for Singapore and 9 (7.92, 10.2) days for Tianjin. Both datasets had shorter incubation periods for earlier-occurring cases. The mean serial interval was 4.56 (2.69, 6.42) days for Singapore and 4.22 (3.43, 5.01) for Tianjin. We inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (Singapore, Tianjin). The estimated basic reproduction number for Singapore was 1.97 (1.45, 2.48) secondary cases per infective; for Tianjin it was 1.87 (1.65, 2.09) secondary cases per infective.ConclusionsEstimated serial intervals are shorter than incubation periods in both Singapore and Tianjin, suggesting that pre-symptomatic transmission is occurring. Shorter serial intervals lead to lower estimates of R0, which suggest that half of all secondary infections should be prevented to control spread.",
        "dimensions": [
            [
                "COVID-19",
                "basic reproduction number",
                "epidemiology",
                "infectious diseases",
                "viral transmission",
                "public health",
                "mathematical modeling",
                "epidemic dynamics"
            ],
            [
                "disease",
                "virus",
                "transmission",
                "epidemiology",
                "mathematical model",
                "infectious period",
                "population",
                "interventions",
                "data source"
            ],
            [
                "Publication Date",
                "Keywords",
                "Author Affiliations",
                "Study Methodology",
                "Geographic Location",
                "Journal Impact Factor",
                "Data Sources"
            ],
            [
                "Geographic location",
                "Time period",
                "Epidemiological data source",
                "Mathematical model",
                "Public health interventions",
                "Demographic characteristics",
                "Data analysis methods",
                "Transmission dynamics"
            ],
            [
                "Disease studied",
                "Calculation method",
                "Data source",
                "Epidemiological parameter"
            ]
        ],
        "impr+abs": [
            "Transmission interval",
            "Incubation period",
            "Serial interval distribution",
            "Basic reproduction number",
            "Pre-symptomatic transmission",
            "Outbreak information",
            "Estimated values",
            "Control measures"
        ],
        "impr+120b": [
            "research problem",
            "target metric",
            "disease studied",
            "methodology",
            "data source",
            "geographic scope",
            "modeling approach"
        ],
        "impr+120b+abs": [
            "Study location",
            "Study period",
            "Data source",
            "Epidemiological parameters",
            "Methodology",
            "Key findings"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189433",
        "research_problem": "Key Information Extraction",
        "orkg_properties": "['research problem', 'operates on', 'targets']",
        "nechakhin_result": "['Research problem', 'Information extraction techniques', 'Similar papers', 'Dimensions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 2,
        "nechakhin_deviation": 4,
        "title": "An end-to-end joint model for evidence information extraction from court record document",
        "abstract": "Information extraction is one of the important tasks in the field of Natural Language Processing (NLP). Most of the existing methods focus on general texts and little attention is paid to information extraction in specialized domains such as legal texts. This paper explores the task of information extraction in the legal field, which aims to extract evidence information from court record documents (CRDs). In the general domain, entities and relations are mostly words and phrases, indicating that they do not span multiple sentences. In contrast, evidence information in CRDs may span multiple sentences, while existing models cannot handle this situation. To address this issue, we first add a classification task in addition to the extraction task. We then formulate the two tasks as a multi-task learning problem and present a novel end-to-end model to jointly address the two tasks. The joint model adopts a shared encoder followed by separate decoders for the two tasks. The experimental results on the dataset show the effectiveness of the proposed model, which can obtain 72.36% F1 score, outperforming previous methods and strong baselines by a large margin.\nAutomated legal text classification is a prominent research topic in the legal field. It lays the foundation for building an intelligent legal system. Current literature focuses on international legal texts, such as Chinese cases, European cases, and Australian cases. Little attention is paid to text classification for U.S. legal texts. Deep learning has been applied to improving text classification performance. Its effectiveness needs further exploration in domains such as the legal field. This paper investigates legal text classification with a large collection of labeled U.S. case documents through comparing the effectiveness of different text classification techniques. We propose a machine learning algorithm using domain concepts as features and random forests as the classifier. Our experiment results on 30,000 full U.S. case documents in 50 categories demonstrated that our approach significantly outperforms a deep learning system built on multiple pre-trained word embeddings and deep neural networks. In addition, applying only the top 400 domain concepts as features for building the random forests could achieve the best performance. This study provides a reference to select machine learning techniques for building high-performance text classification systems in the legal domain or other fields.\nNatural language processing (NLP) based approaches have recently received attention for legal systems of several countries. It is of interest to study the wide variety of legal systems that have so far not received any attention. In particular, for the legal system of the Republic of Turkey, codified in Turkish, no works have been published. We first review the state-of-the-art of NLP in law, and then study the problem of predicting verdicts for several different courts, using several different algorithms. This study is much broader than earlier studies in the number of different courts and the variety of algorithms it includes. Therefore it provides a reference point and baseline for further studies in this area. We further hope the scope and systematic nature of this study can set a framework that can be applied to the study of other legal systems. We present novel results on predicting the rulings of the Turkish Constitutional Court and Courts of Appeal, using only fact descriptions, and wi",
        "dimensions": [
            [
                "Natural Language Processing",
                "Information Retrieval",
                "Text Mining",
                "Machine Learning",
                "Data Mining",
                "Information Extraction",
                "Named Entity Recognition",
                "Text Processing",
                "Semantic Analysis",
                "Knowledge Graphs"
            ],
            [
                "text data",
                "natural language processing",
                "information retrieval",
                "machine learning",
                "data sources",
                "entity recognition",
                "text summarization",
                "named entity recognition",
                "text classification"
            ],
            [
                "Specific aspect of key information extraction",
                "Techniques and algorithms",
                "Application domain",
                "Data sources",
                "Evaluation metrics",
                "Language and text type"
            ],
            [
                "Information type",
                "Extraction method",
                "Domain or application",
                "Data source",
                "Evaluation metrics",
                "Language",
                "Annotation or labeling process",
                "Integration with downstream tasks"
            ],
            [
                "research problem",
                "Data source",
                "Methodology",
                "Key findings",
                "Evaluation metric"
            ]
        ],
        "impr+abs": [
            "Information extraction",
            "Specialized domain",
            "Multi-task learning",
            "Model performance",
            "Text classification",
            "Legal system",
            "NLP application",
            "Algorithm comparison"
        ],
        "impr+120b": [
            "research problem",
            "methodology",
            "data source",
            "evaluation metric",
            "application domain",
            "output format"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Domain",
            "Data source",
            "Methodology",
            "Model architecture",
            "Evaluation metric",
            "Feature set"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189439",
        "research_problem": "Document-Level Information Extraction",
        "orkg_properties": "['implementation', 'research problem', 'targets']",
        "nechakhin_result": "['Document type', 'Keyword extraction', 'Named entity recognition', 'Sentence-level information extraction', 'Information extraction methods', 'Machine learning techniques', 'Natural language processing', 'Data source', 'Domain specific information extraction', 'Text classification', 'Data preprocessing']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 4,
        "title": "DeepCPCFG: Deep Learning and\u00a0Context Free Grammars for\u00a0End-to-End Information Extraction",
        "abstract": "We address the challenge of extracting structured information from business documents without detailed annotations. We propose Deep Conditional Probabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional complex documents and use Recursive Neural Networks to create an end-to-end system for finding the most probable parse that represents the structured information to be extracted. This system is trained end-to-end with scanned documents as input and only relational-records as labels. The relational-records are extracted from existing databases avoiding the cost of annotating documents by hand. We apply this approach to extract information from scanned invoices achieving state-of-the-art results despite using no hand-annotations.\n",
        "dimensions": [
            [
                "Information Extraction",
                "Natural Language Processing",
                "Document Analysis",
                "Text Mining",
                "Machine Learning",
                "Data Mining",
                "Information Retrieval",
                "Named Entity Recognition",
                "Text Classification",
                "Semantic Analysis"
            ],
            [
                "document type",
                "extraction method",
                "information type",
                "language",
                "corpus size",
                "evaluation metric"
            ],
            [
                "Research Problem",
                "Information Extraction Techniques",
                "Document Types",
                "Performance Metrics",
                "Domain or Industry",
                "Data Sources",
                "Evaluation Datasets",
                "Language",
                "Temporal Aspects"
            ],
            [
                "Document type",
                "Information extraction technique",
                "Language",
                "Domain or subject area",
                "Evaluation metrics",
                "Dataset or corpus",
                "Information types",
                "Benchmark tasks"
            ],
            [
                "Research problem",
                "Data source",
                "Extraction method",
                "Document type",
                "Information category"
            ]
        ],
        "impr+abs": [
            "research problem",
            "Methodology",
            "Data used",
            "Evaluation metric"
        ],
        "impr+120b": [
            "Research problem",
            "Task definition",
            "Input data type",
            "Output format",
            "Methodology",
            "Evaluation metric",
            "Application domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Methodology",
            "Model architecture",
            "Training data",
            "Application domain",
            "Evaluation metric"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189444",
        "research_problem": "Knowledge Graph Completion",
        "orkg_properties": "['implementation', 'research problem', 'operates on', 'targets']",
        "nechakhin_result": "['Graph structure', 'Semantic relationships', 'Entities', 'Attributes', 'Ontologies', 'Linked data', 'Triplets', 'Embeddings', 'Graph algorithms', 'Graph mining', 'Graph neural networks']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 4,
        "title": "Triple Classification for Scholarly Knowledge Graph Completion",
        "abstract": "structured information representing knowledge encoded in scientific publications. With the sheer volume of published scientific literature comprising a plethora of inhomogeneous entities and relations to describe scientific concepts, these KGs are inherently incomplete. We present exBERT, a method for leveraging pre-trained transformer language models to perform scholarly knowledge graph completion. We model triples of a knowledge graph as text and perform triple classification (i.e., belongs to KG or not). The evaluation shows that exBERT outperforms other baselines on three scholarly KG completion datasets in the tasks of triple classification, link prediction, and relation prediction. Furthermore, we present two scholarly datasets as resources for the research community, collected from public KGs and online resources.",
        "dimensions": [
            [
                "Graph structure",
                "Knowledge representation",
                "Link prediction",
                "Semantic similarity",
                "Entity embeddings",
                "Machine learning",
                "Data mining",
                "Information retrieval"
            ],
            [
                "knowledge domain",
                "entity",
                "relationship",
                "knowledge graph structure",
                "inference method",
                "embedding model",
                "link prediction",
                "knowledge graph size",
                "knowledge graph quality"
            ],
            [
                "Research Problem",
                "Techniques and Algorithms",
                "Evaluation Metrics",
                "Datasets",
                "Application Domains",
                "Publication Venue and Authors"
            ],
            [
                "Entity type",
                "Knowledge graph structure",
                "Embedding methods",
                "Evaluation metrics",
                "Incorporation of external knowledge",
                "Application domains",
                "Scalability",
                "Incorporation of uncertainty"
            ],
            [
                "research problem",
                "Entity type",
                "Methodology",
                "Evaluation metric",
                "Data source",
                "Application domain",
                "Reference"
            ]
        ],
        "impr+abs": [
            "Knowledge Graph type",
            "Methodology",
            "Evaluation metric",
            "Dataset"
        ],
        "impr+120b": [
            "research problem",
            "Task definition",
            "Methodology",
            "Evaluation metric",
            "Dataset",
            "Baseline approaches"
        ],
        "impr+120b+abs": [
            "research problem",
            "methodology",
            "model",
            "evaluation tasks",
            "datasets",
            "resources"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189447",
        "research_problem": "Question Answering",
        "orkg_properties": "['implementation', 'research problem', 'operates on']",
        "nechakhin_result": "['Natural Language Processing',\n 'Machine Learning',\n 'Information Retrieval',\n 'Text Mining',\n 'Semantic Analysis']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 4,
        "nechakhin_deviation": 5,
        "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
        "abstract": "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.",
        "dimensions": [
            [
                "Natural Language Processing",
                "Information Retrieval",
                "Machine Learning",
                "Text Mining",
                "Semantic Analysis",
                "Knowledge Graphs",
                "Deep Learning",
                "Information Extraction",
                "Text Summarization",
                "Question Generation"
            ],
            [
                "question type",
                "answer type",
                "dataset",
                "language",
                "model architecture",
                "training method",
                "evaluation metric",
                "domain"
            ],
            [
                "Topic",
                "Methodology",
                "Application Domain",
                "Publication Date",
                "Data Source",
                "Evaluation Metrics",
                "Dataset"
            ],
            [
                "Task",
                "Model architecture",
                "Dataset",
                "Evaluation metric",
                "Domain",
                "Technique",
                "Publication venue"
            ],
            [
                "research problem",
                "Solution approach",
                "Evaluation metric",
                "Domain",
                "Data source"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Dataset description",
            "Data collection method",
            "Evaluation metric"
        ],
        "impr+120b": [
            "research problem",
            "task type",
            "evaluation metric",
            "dataset",
            "methodology",
            "application domain"
        ],
        "impr+120b+abs": [
            "Dataset name",
            "Task type",
            "Domain",
            "Data source",
            "Annotation methodology",
            "Evaluation metric",
            "Baseline performance"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R189455",
        "research_problem": "Information Extraction",
        "orkg_properties": "['implementation', 'research problem']",
        "nechakhin_result": "['Topic/Subject', 'Keywords', 'Abstract', 'Methods/Approach', 'Results', 'Conclusions', 'Authors', 'Publication Venue', 'Citations', 'Date of Publication', 'Researcher/Author Affiliation', 'Funding Source', 'Dataset Used', 'Language']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 3,
        "title": "TableSeer: automatic table metadata extraction and searching in digital libraries",
        "abstract": "Tables are ubiquitous in digital libraries. In scientific documents, tables are widely used to present experimental results or statistical data in a condensed fashion. However, current search engines do not support table search. The difficulty of automatic extracting tables from un-tagged documents, the lack of a universal table metadata specification, and the limitation of the existing ranking schemes make table search problem challenging. In this paper, we describeTableSeer, a search engine for tables.TableSeercrawls digital libraries, detects tables from documents, extracts tables metadata, indexes and ranks tables, and provides a user-friendly search interface. We propose an extensive set of medium-independent metadata for tables that scientists and other users can adopt for representing table information. In addition, we devise a novel pagebox-cuttingmethod to improve the performance of the table detection. Given a query,TableSeerranks the matched tables using an innovative ranking algorithm -TableRank.TableRankrates each \u20edquery, table\u2102 pair with a tailored vector space model and a specific term weighting scheme. Overall,TableSeereliminates the burden of manually extract table data from digital libraries and enables users to automatically examine tables. We demonstrate the value ofTableSeerwith empirical studies on scientific documents.",
        "dimensions": [
            [
                "Natural Language Processing",
                "Text Mining",
                "Machine Learning",
                "Information Retrieval",
                "Named Entity Recognition",
                "Text Classification",
                "Knowledge Graphs",
                "Information Extraction Techniques",
                "Semantic Analysis",
                "Entity Linking"
            ],
            [
                "text",
                "source",
                "date",
                "entity",
                "relation",
                "context",
                "language",
                "accuracy",
                "application"
            ],
            [
                "Research Problem",
                "Techniques and Algorithms",
                "Datasets",
                "Evaluation Metrics",
                "Application Domains",
                "Challenges and Limitations",
                "Publication Venue",
                "Year of Publication"
            ],
            [
                "Type of Information",
                "Information Source",
                "Extraction Method",
                "Domain or Topic",
                "Data Format",
                "Information Quality",
                "Application Area"
            ],
            [
                "research problem",
                "Data source",
                "Methodology",
                "Evaluation metric",
                "Results",
                "Conclusion"
            ]
        ],
        "impr+abs": [
            "Research problem",
            "Methodology",
            "Tool/Algorithm",
            "Data source"
        ],
        "impr+120b": [
            "research problem",
            "task definition",
            "methodology",
            "dataset",
            "evaluation metric",
            "application domain"
        ],
        "impr+120b+abs": [
            "Research problem",
            "Proposed system",
            "Data source",
            "Table detection method",
            "Metadata specification",
            "Ranking algorithm",
            "Evaluation method"
        ]
    },
    {
        "paper_id": "http://orkg.org/orkg/resource/R74055",
        "research_problem": "COVID-19 case fatality rate",
        "orkg_properties": "['Time period', 'research problem', 'hazard ratio', 'is about', 'location']",
        "nechakhin_result": "['Disease',\n 'Epidemiology',\n 'Public health',\n 'Virus',\n 'Mortality',\n 'Statistics',\n 'Healthcare',\n 'Pandemic',\n 'Risk factors',\n 'Comorbidities',\n 'Testing',\n 'Treatment',\n 'Interventions']",
        "nechakhin_mappings": 0,
        "nechakhin_alignment": 3,
        "nechakhin_deviation": 5,
        "title": "Case fatality risk of the SARS-CoV-2 variant of concern B.1.1.7 in England",
        "abstract": "The B.1.1.7 variant of concern (VOC) is increasing in prevalence across Europe. Accurate estimation of disease severity associated with this VOC is critical for pandemic planning. We found increased risk of death for VOC compared with non-VOC cases in England (HR: 1.67 (95% CI: 1.34 - 2.09; P<.0001)). Absolute risk of death by 28-days increased with age and comorbidities. VOC has potential to spread faster with higher mortality than the pandemic to date.",
        "dimensions": [
            [
                "Disease",
                "Epidemiology",
                "Public Health",
                "Mortality Rate",
                "Pandemic",
                "Virus",
                "Infection",
                "Healthcare",
                "Treatment",
                "Risk Factors",
                "Demographics",
                "Geographic Location",
                "Time Period",
                "Data Source",
                "Statistical Analysis"
            ],
            [
                "disease",
                "location",
                "time period",
                "population demographics",
                "healthcare system capacity",
                "testing capacity",
                "interventions",
                "data source"
            ],
            [
                "timeframe",
                "geographic location",
                "demographics",
                "research methodology",
                "interventions and treatments",
                "data sources",
                "public health measures",
                "variants of the virus"
            ],
            [
                "Time period",
                "Geographic location",
                "Demographics",
                "Public health interventions",
                "Variants",
                "Data source"
            ],
            [
                "Disease",
                "Mortality rate",
                "Epidemiological factor",
                "Geographical location"
            ]
        ],
        "impr+abs": [
            "Variant name",
            "Geographical prevalence",
            "Disease severity",
            "Risk factors",
            "Transmission potential"
        ],
        "impr+120b": [
            "research problem",
            "evaluation metric",
            "disease",
            "outcome",
            "population",
            "data source"
        ],
        "impr+120b+abs": [
            "Study location",
            "Research problem",
            "Variant studied",
            "Outcome measure",
            "Statistical method",
            "Population characteristics",
            "Time frame"
        ]
    }
]